[
["index.html", "Visualisation of biomolecular data Chapter 1 Introduction 1.1 Installation 1.2 Questions 1.3 License 1.4 Useful references", " Visualisation of biomolecular data Laurent Gatto 2018-04-30 Chapter 1 Introduction This Visualisation of biomolecular data course was set up as part of the 2018 edition of the May Institute Computation and statistics for mass spectrometry and proteomics at Northeastern University, Boston MA. It is aimed at people who are already familiar with the R language and syntax, and who would like to get a hands-on introduction to visualisation, with a focus on biomolecular data in general, and proteomics in particular. This course is meant to be mostly hands-on, with an intuitive understanding of the underlying techniques. Let’s use the famous Anscombe’s quartet data as a motivating example. This data is composed of 4 pairs of values, \\((x_1, y_1)\\) to \\((x_4, y_4)\\): x1 x2 x3 x4 y1 y2 y3 y4 10 10 10 8 8.04 9.14 7.46 6.58 8 8 8 8 6.95 8.14 6.77 5.76 13 13 13 8 7.58 8.74 12.74 7.71 9 9 9 8 8.81 8.77 7.11 8.84 11 11 11 8 8.33 9.26 7.81 8.47 14 14 14 8 9.96 8.10 8.84 7.04 6 6 6 8 7.24 6.13 6.08 5.25 4 4 4 19 4.26 3.10 5.39 12.50 12 12 12 8 10.84 9.13 8.15 5.56 7 7 7 8 4.82 7.26 6.42 7.91 5 5 5 8 5.68 4.74 5.73 6.89 Each of these \\(x\\) and \\(y\\) sets have the same variance, mean and correlation: 1 2 3 4 var(x) 11.0000000 11.0000000 11.0000000 11.0000000 mean(x) 9.0000000 9.0000000 9.0000000 9.0000000 var(y) 4.1272691 4.1276291 4.1226200 4.1232491 mean(y) 7.5009091 7.5009091 7.5000000 7.5009091 cor(x,y) 0.8164205 0.8162365 0.8162867 0.8165214 But… While the residuals of the linear regression clearly indicate fundamental differences in these data, the most simple and straightforward approach is visualisation to highlight the fundamental differences in the datasets. See also another, more recent example: The Datasaurus Dozen dataset. The Datasaurus Dozen dataset We will focus on producing visualisation that will enable understanding important features of biomolecular data or the effect of their transformation and analyses. With this in mind, the beauty of the visualisations we will produce won’t be assessed by how visually attractive the figures are, but how then help us understand and assess the data. In the process of data exploration and data analysis, we want to be able to quickly generate and interpret figures, to progress in our understanding of data. It is of course important to fine tune the graphics to make them visually appealing and improve communication with the audience, but we won’t focus on these aspects here. An important aspect of data visualisation is data manipulation, transformation and the format of the data. These topics will be introduced and documented throughout the course. A short URL for this book is http://bit.ly/biomolvis 1.1 Installation A set of packages that are used, either directly or indirectly are provided below. A complete session information with all packages used to compile this document is available at the end. The source code for this document is available on GitHub at https://github.com/lgatto/VisualisingBiomolecularData library(&quot;affy&quot;) library(&quot;ALL&quot;) library(&quot;ggplot2&quot;) library(&quot;knitr&quot;) library(&quot;limma&quot;) library(&quot;magrittr&quot;) library(&quot;MSnbase&quot;) library(&quot;msdata&quot;) library(&quot;MSstats&quot;) library(&quot;MSstatsBioData&quot;) library(&quot;plyr&quot;) library(&quot;pRoloc&quot;) library(&quot;pRolocdata&quot;) library(&quot;readr&quot;) library(&quot;Rtsne&quot;) library(&quot;tidyr&quot;) library(&quot;tidyverse&quot;) library(&quot;UpSetR&quot;) library(&quot;Vennerable&quot;) ## on GitHub at js229/Vennerable 1.2 Questions For any questions or suggestions, please open an issue. Please do add the output of your session information and, of possible, a reproducible example describing your question or suggestion. 1.3 License This material is licensed under the Creative Commons Attribution-ShareAlike 4.0 License. Some content is inspired by other sources, see the Credit section in the material. 1.4 Useful references Gatto L, Breckels LM, Naake T, Gibb S. Visualization of proteomics data using R and bioconductor. Proteomics. 2015 Apr;15(8):1375-89. doi: 10.1002/pmic.201400392. PMID: 25690415; PMCID: PMC4510819. Key M. A tutorial in displaying mass spectrometry-based proteomic data using heat maps. BMC Bioinformatics. 2012;13 Suppl 16:S10. doi: 10.1186/1471-2105-13-S16-S10. Epub 2012 Nov 5. PMID: 23176119; PMCID: PMC3489527. Gatto L, Christoforou A. Using R and Bioconductor for proteomics data analysis. Biochim Biophys Acta. 2014 Jan;1844(1 Pt A):42-51. doi: 10.1016/j.bbapap.2013.04.032. Epub 2013 May 18. PMID: 23692960. Conway JR, Lex A, Gehlenborg N. UpSetR: an R package for the visualization of intersecting sets and their properties. Bioinformatics. 2017 Sep 15;33(18):2938-2940. doi: 10.1093/bioinformatics/btx364. PMID: 28645171; PMCID: PMC5870712. "],
["example-datasets-and-data-manipulation.html", "Chapter 2 Example datasets and data manipulation 2.1 Raw MS data 2.2 Mulvey 2015 time course 2.3 The iPRG data 2.4 CRC training data 2.5 ALL data 2.6 Using dplyr for data manipulation", " Chapter 2 Example datasets and data manipulation We will used various datasets throughout the course. These data are briefly described below, and we will explore them through various visualisations later. This section also introduces some techniques to manipulate data and transorm them into different formats. 2.1 Raw MS data Section Using R and Bioconductor for MS-based proteomics shows how to visualise raw mass spectrometry data. The raw data that will be using come from the msdata MSnbase packages. These data will be introduced later. 2.2 Mulvey 2015 time course This data comes from Mulvey CM, Schröter C, Gatto L, Dikicioglu D, Fidaner IB, Christoforou A, Deery MJ, Cho LT, Niakan KK, Martinez-Arias A, Lilley KS. Dynamic Proteomic Profiling of Extra-Embryonic Endoderm Differentiation in Mouse Embryonic Stem Cells. Stem Cells. 2015 Sep;33(9):2712-25. doi: 10.1002/stem.2067. Epub 2015 Jun 23. PMID: 26059426. library(&quot;pRolocdata&quot;) data(mulvey2015norm) This MSnSet, available from the pRolocdata package, measured the expression profiles of 2337 proteins along 6 time points in triplicate. exprs(mulvey2015norm)[1:5, 1:3] ## rep1_0hr rep1_16hr rep1_24hr ## P48432 2.479592 1.698630 1.0350877 ## Q62315-2 1.979592 1.342466 0.9181287 ## P55821 1.780612 1.767123 1.1578947 ## P17809 1.637755 1.157534 0.9941520 ## Q8K3F7 1.852041 1.623288 1.3040936 pData(mulvey2015norm) ## rep times cond ## rep1_0hr 1 1 1 ## rep1_16hr 1 2 1 ## rep1_24hr 1 3 1 ## rep1_48hr 1 4 1 ## rep1_72hr 1 5 1 ## rep1_XEN 1 6 1 ## rep2_0hr 2 1 1 ## rep2_16hr 2 2 1 ## rep2_24hr 2 3 1 ## rep2_48hr 2 4 1 ## rep2_72hr 2 5 1 ## rep2_XEN 2 6 1 ## rep3_0hr 3 1 1 ## rep3_16hr 3 2 1 ## rep3_24hr 3 3 1 ## rep3_48hr 3 4 1 ## rep3_72hr 3 5 1 ## rep3_XEN 3 6 1 2.3 The iPRG data iprg &lt;- read.csv(&quot;http://bit.ly/VisBiomedDataIprgCsv&quot;) head(iprg) ## Protein Log2Intensity Run Condition ## 1 sp|D6VTK4|STE2_YEAST 26.81232 JD_06232014_sample1_B.raw Condition1 ## 2 sp|D6VTK4|STE2_YEAST 26.60786 JD_06232014_sample1_C.raw Condition1 ## 3 sp|D6VTK4|STE2_YEAST 26.58301 JD_06232014_sample1-A.raw Condition1 ## 4 sp|D6VTK4|STE2_YEAST 26.83563 JD_06232014_sample2_A.raw Condition2 ## 5 sp|D6VTK4|STE2_YEAST 26.79430 JD_06232014_sample2_B.raw Condition2 ## 6 sp|D6VTK4|STE2_YEAST 26.60863 JD_06232014_sample2_C.raw Condition2 ## BioReplicate Intensity TechReplicate ## 1 1 117845016 B ## 2 1 102273602 C ## 3 1 100526837 A ## 4 2 119765106 A ## 5 2 116382798 B ## 6 2 102328260 C dim(iprg) ## [1] 36321 7 table(iprg$Condition, iprg$TechReplicate) ## ## A B C ## Condition1 3026 3026 3027 ## Condition2 3027 3027 3027 ## Condition3 3027 3027 3027 ## Condition4 3027 3026 3027 Reference: Choi M, Eren-Dogu ZF, Colangelo C, Cottrell J, Hoopmann MR, Kapp EA, Kim S, Lam H, Neubert TA, Palmblad M, Phinney BS, Weintraub ST, MacLean B, Vitek O. ABRF Proteome Informatics Research Group (iPRG) 2015 Study: Detection of Differentially Abundant Proteins in Label-Free Quantitative LC-MS/MS Experiments. J Proteome Res. 2017 Feb 3;16(2):945-957. doi: 10.1021/acs.jproteome.6b00881 Epub 2017 Jan 3. PMID: 27990823. This data is in the so-called long format. In some applications, it is more convenient to have the data in wide format, where rows contain the protein expression data for all samples. Let’s start by simplifying the data to keep only the relevant columns: head(iprg2 &lt;- iprg[, c(1, 3, 6)]) ## Protein Run Intensity ## 1 sp|D6VTK4|STE2_YEAST JD_06232014_sample1_B.raw 117845016 ## 2 sp|D6VTK4|STE2_YEAST JD_06232014_sample1_C.raw 102273602 ## 3 sp|D6VTK4|STE2_YEAST JD_06232014_sample1-A.raw 100526837 ## 4 sp|D6VTK4|STE2_YEAST JD_06232014_sample2_A.raw 119765106 ## 5 sp|D6VTK4|STE2_YEAST JD_06232014_sample2_B.raw 116382798 ## 6 sp|D6VTK4|STE2_YEAST JD_06232014_sample2_C.raw 102328260 We can convert the iPRG into a wide format with tidyr::spread: library(&quot;tidyr&quot;) iprg3 &lt;- spread(iprg2, key = Run, value = Intensity) head(iprg3) ## Protein JD_06232014_sample1-A.raw ## 1 sp|D6VTK4|STE2_YEAST 100526837 ## 2 sp|O13297|CET1_YEAST 27598550 ## 3 sp|O13329|FOB1_YEAST 11625198 ## 4 sp|O13539|THP2_YEAST 20606703 ## 5 sp|O13547|CCW14_YEAST 145493943 ## 6 sp|O13563|RPN13_YEAST 75530595 ## JD_06232014_sample1_B.raw JD_06232014_sample1_C.raw ## 1 117845016 102273602 ## 2 27618234 26774670 ## 3 10892143 16948335 ## 4 192490784 175282010 ## 5 156581624 117211277 ## 6 71664672 82193735 ## JD_06232014_sample2_A.raw JD_06232014_sample2_B.raw ## 1 119765106 116382798 ## 2 24114625 26803677 ## 3 9448487 11334921 ## 4 61781078 72052121 ## 5 151407246 126364703 ## 6 62998676 73431260 ## JD_06232014_sample2_C.raw JD_06232014_sample3_A.raw ## 1 102328260 103830944 ## 2 25055912 21977898 ## 3 14304554 19285356 ## 4 62895992 52653164 ## 5 123774524 99222081 ## 6 67612042 75916420 ## JD_06232014_sample3_B.raw JD_06232014_sample3_C.raw ## 1 94660680 96919972 ## 2 25901361 26545544 ## 3 10230925 13985282 ## 4 84856844 63400641 ## 5 113287367 129669642 ## 6 68877388 72632994 ## JD_06232014_sample4-A.raw JD_06232014_sample4_B.raw ## 1 105724288 102150172 ## 2 23860882 26305424 ## 3 8592981 9414560 ## 4 35363603 109094905 ## 5 141404829 132444723 ## 6 57600421 70708409 ## JD_06232014_sample4_C.raw ## 1 87702341 ## 2 26612349 ## 3 12783705 ## 4 63392610 ## 5 117065261 ## 6 69675572 nrow(iprg3) ## [1] 3027 Indeed, we started with length(unique(iprg$Protein)) ## [1] 3027 unique proteins, which corresponds to the number of rows in the new wide dataset. The long format is ideal when using ggplot2, as we will see in a later chapter. The wide format has also advantages. For example, it becomes straighforward to verify if there are proteins that haven’t been quantified in some samples. (k &lt;- which(is.na(iprg3), arr.ind = dim(iprg3))) ## row col ## [1,] 2721 2 ## [2,] 2721 3 ## [3,] 652 12 iprg3[unique(k[, &quot;row&quot;]), ] ## Protein JD_06232014_sample1-A.raw ## 2721 sp|Q08236|AVO1_YEAST NA ## 652 sp|P28320|CWC16_YEAST 1108670 ## JD_06232014_sample1_B.raw JD_06232014_sample1_C.raw ## 2721 NA 10989286 ## 652 961273.8 1229085 ## JD_06232014_sample2_A.raw JD_06232014_sample2_B.raw ## 2721 5407419.7 2041424 ## 652 449839.1 1334009 ## JD_06232014_sample2_C.raw JD_06232014_sample3_A.raw ## 2721 13108081 4677052 ## 652 1633894 1493440 ## JD_06232014_sample3_B.raw JD_06232014_sample3_C.raw ## 2721 10308957 13639794 ## 652 2205688 2120321 ## JD_06232014_sample4-A.raw JD_06232014_sample4_B.raw ## 2721 10377088.4 2592511 ## 652 757082.7 NA ## JD_06232014_sample4_C.raw ## 2721 10420338 ## 652 1264712 The opposite operation to spread is gather, also from the tidyr package: head(iprg4 &lt;- gather(iprg3, key = Run, value = Intensity, -Protein)) ## Protein Run Intensity ## 1 sp|D6VTK4|STE2_YEAST JD_06232014_sample1-A.raw 100526837 ## 2 sp|O13297|CET1_YEAST JD_06232014_sample1-A.raw 27598550 ## 3 sp|O13329|FOB1_YEAST JD_06232014_sample1-A.raw 11625198 ## 4 sp|O13539|THP2_YEAST JD_06232014_sample1-A.raw 20606703 ## 5 sp|O13547|CCW14_YEAST JD_06232014_sample1-A.raw 145493943 ## 6 sp|O13563|RPN13_YEAST JD_06232014_sample1-A.raw 75530595 The two lond datasets, iprg2 and iprg4 are different due to the missing values shown above. nrow(iprg2) ## [1] 36321 nrow(iprg4) ## [1] 36324 nrow(na.omit(iprg4)) ## [1] 36321 which can be accounted for by removing rows with missing values by setting na.rm = TRUE. head(iprg5 &lt;- gather(iprg3, key = Run, value = Intensity, -Protein, na.rm = TRUE)) ## Protein Run Intensity ## 1 sp|D6VTK4|STE2_YEAST JD_06232014_sample1-A.raw 100526837 ## 2 sp|O13297|CET1_YEAST JD_06232014_sample1-A.raw 27598550 ## 3 sp|O13329|FOB1_YEAST JD_06232014_sample1-A.raw 11625198 ## 4 sp|O13539|THP2_YEAST JD_06232014_sample1-A.raw 20606703 ## 5 sp|O13547|CCW14_YEAST JD_06232014_sample1-A.raw 145493943 ## 6 sp|O13563|RPN13_YEAST JD_06232014_sample1-A.raw 75530595 2.4 CRC training data This dataset comes from the MSstatsBioData package and was generated as follows: library(&quot;MSstats&quot;) library(&quot;MSstatsBioData&quot;) data(SRM_crc_training) Quant &lt;- dataProcess(SRM_crc_training) subjectQuant &lt;- quantification(Quant) It provides quantitative information for 72 proteins, including two standard proteins, AIAG-Bovine and FETUA-Bovine. These proteins were targeted for plasma samples with SRM with isotope labeled reference peptides in order to identify candidate protein biomarker for non-invasive detection of CRC. The training cohort included 100 subjects in control group and 100 subjects with CRC. Each sample for subject was measured in a single injection without technical replicate. The training cohort was analyzed with Skyline. The dataset was already normalized as described in manuscript. User do not need extra normalization. NAs should be considered as censored missing. Two standard proteins can be removed for statistical analysis. Clinical information where added manually thereafter. To load this dataset: crcdf &lt;- read.csv(&quot;http://bit.ly/VisBiomedDataCrcCsv&quot;) crcdf[1:10, 1:3] ## A1AG2 AFM AHSG ## 1 14.23816 16.10302 19.95179 ## 2 15.02411 16.02071 19.71592 ## 3 15.63136 16.14380 19.71085 ## 4 15.40137 16.27642 19.70438 ## 5 16.00316 16.95821 20.42033 ## 6 13.93242 16.52772 19.88985 ## 7 14.34155 16.43029 20.10060 ## 8 13.57086 17.11052 19.93833 ## 9 15.83348 15.88189 18.64270 ## 10 15.37996 16.22621 20.09992 This dataset is in the wide format. It contains the intensity of the proteins in columns 1 to 72 for each of the 200 samples along the rows. Generally, omics datasets contain the features (proteins, transcripts, …) along the rows and the samples along the columns. In columns 73 to 79, we sample metadata. crcdf[1:10, 73:79] ## Sample Group Age Gender Cancer_stage Tumour_location Sub_group ## 1 P1A10 CRC 60 female 1 colon CRC ## 2 P1A2 CRC 70 male 1 rectum CRC ## 3 P1A4 CRC 65 male 1 rectum CRC ## 4 P1A6 CRC 65 female 4 colon CRC ## 5 P1B12 CRC 62 female 3 colon CRC ## 6 P1B2 CRC 55 male 2 colon CRC ## 7 P1B3 CRC 61 male 2 rectum CRC ## 8 P1B6 CRC 52 male 4 rectum CRC ## 9 P1B9 CRC 89 female 4 colon CRC ## 10 P1C11 CRC 81 male 1 rectum CRC A widely used data structure for omics data follows the convention described in the figure below: An eSet-type of expression data container This typical omics data structure, as defined by the eSet class in the Bioconductor Biobase package, is represented below. It’s main features are An assay data slot containing the quantitative omics data (expression data), stored as a matrix and accessible with exprs. Features defined along the rows and samples along the columns. A sample metadata slot containing sample co-variates, stored as an annotated data.frame and accessible with pData. This data frame is stored with rows representing samples and sample covariate along the columns, and its rows match the expression data columns exactly. A feature metadata slot containing feature co-variates, stored as an annotated data.frame and accessible with fData. This dataframe’s rows match the expression data rows exactly. The coordinated nature of the high throughput data guarantees that the dimensions of the different slots will always match (i.e the columns in the expression data and then rows in the sample metadata, as well as the rows in the expression data and feature metadata) during data manipulation. The metadata slots can grow additional co-variates (columns) without affecting the other structures. Below, we show how to transform the crc dataset into an MSnSet (implementing the data structure above for quantitative proteomics data) using the readMSnSet2 function. library(&quot;MSnbase&quot;) i &lt;- 1:72 ## expression columns e &lt;- t(crcdf[, i]) ## expression data colnames(e) &lt;- 1:200 crc &lt;- readMSnSet2(data.frame(e), e = 1:200) pd &lt;- crcdf[, -i] rownames(pd) &lt;- paste0(&quot;X&quot;, rownames(pd)) pData(crc) &lt;- pd crc ## MSnSet (storageMode: lockedEnvironment) ## assayData: 72 features, 200 samples ## element names: exprs ## protocolData: none ## phenoData ## sampleNames: X1 X2 ... X200 (200 total) ## varLabels: Sample Group ... Sub_group (7 total) ## varMetadata: labelDescription ## featureData: none ## experimentData: use &#39;experimentData(object)&#39; ## Annotation: ## - - - Processing information - - - ## MSnbase version: 2.5.14 Or load it direcly: download.file(&quot;http://bit.ly/VisBiomedDataCrcMSnSet&quot;, &quot;./data/crc.rda&quot;) load(&quot;./data/crc.rda&quot;) crc ## MSnSet (storageMode: lockedEnvironment) ## assayData: 72 features, 200 samples ## element names: exprs ## protocolData: none ## phenoData ## sampleNames: X1 X2 ... X200 (200 total) ## varLabels: Sample Group ... Sub_group (7 total) ## varMetadata: labelDescription ## featureData: none ## experimentData: use &#39;experimentData(object)&#39; ## Annotation: ## - - - Processing information - - - ## MSnbase version: 2.5.14 Reference: See Surinova, S. et al. (2015) Prediction of colorectal cancer diagnosis based on circulating plasma proteins. EMBO Mol. Med., 7, 1166–1178 for details. 2.5 ALL data library(&quot;ALL&quot;) data(ALL) ALL ## ExpressionSet (storageMode: lockedEnvironment) ## assayData: 12625 features, 128 samples ## element names: exprs ## protocolData: none ## phenoData ## sampleNames: 01005 01010 ... LAL4 (128 total) ## varLabels: cod diagnosis ... date last seen (21 total) ## varMetadata: labelDescription ## featureData: none ## experimentData: use &#39;experimentData(object)&#39; ## pubMedIds: 14684422 16243790 ## Annotation: hgu95av2 From the documentation page: The Acute Lymphoblastic Leukemia Data from the Ritz Laboratory consist of microarrays from 128 different individuals with acute lymphoblastic leukemia (ALL). A number of additional covariates are available. The data have been normalized (using rma) and it is the jointly normalized data that are available here. The ALL data is of class ExpressionSet, which implements the data structure above for microarray expression data, and contains normalised and summarised transcript intensities. Below, we select will patients with B-cell lymphomas and BCR/ABL abnormality and negative controls. table(ALL$BT) ## ## B B1 B2 B3 B4 T T1 T2 T3 T4 ## 5 19 36 23 12 5 1 15 10 2 table(ALL$mol.biol) ## ## ALL1/AF4 BCR/ABL E2A/PBX1 NEG NUP-98 p15/p16 ## 10 37 5 74 1 1 ALL_bcrneg &lt;- ALL[, ALL$mol.biol %in% c(&quot;NEG&quot;, &quot;BCR/ABL&quot;) &amp; grepl(&quot;B&quot;, ALL$BT)] ALL_bcrneg$mol.biol &lt;- factor(ALL_bcrneg$mol.biol) We then use the limma package to library(&quot;limma&quot;) design &lt;- model.matrix(~0+ALL_bcrneg$mol.biol) colnames(design) &lt;- c(&quot;BCR.ABL&quot;, &quot;NEG&quot;) ## Step1: linear model. lmFit is a wrapper around lm in R fit1 &lt;- lmFit(ALL_bcrneg, design) ## Step 2: fit contrasts: find genes that respond to estrogen contrast.matrix &lt;- makeContrasts(BCR.ABL-NEG, levels = design) fit2 &lt;- contrasts.fit(fit1, contrast.matrix) ## Step3: add empirical Bayes moderation fit3 &lt;- eBayes(fit2) ## Extract results and set them to the feature data res &lt;- topTable(fit3, n = Inf) fData(ALL_bcrneg) &lt;- res[featureNames(ALL_bcrneg), ] This annotated ExpressionSet can be reproduced as shown above or downloaded and loaded using download.file(&quot;http://bit.ly/VisBiomedDataALL_bcrneg&quot;, &quot;./data/ALL_bcrneg.rda&quot;) load(&quot;./data/ALL_bcrneg.rda&quot;) Reference: Sabina Chiaretti, Xiaochun Li, Robert Gentleman, Antonella Vitale, Marco Vignetti, Franco Mandelli, Jerome Ritz, and Robin Foa Gene expression profile of adult T-cell acute lymphocytic leukemia identifies distinct subsets of patients with different response to therapy and survival. Blood, 1 April 2004, Vol. 103, No. 7. 2.6 Using dplyr for data manipulation The following material is based on Data Carpentry’s the Data analisis and visualisation lessons. Bracket subsetting is handy, but it can be cumbersome and difficult to read, especially for complicated operations. Enter dplyr. dplyr is a package for making tabular data manipulation easier. It pairs nicely with tidyr which enables you to swiftly convert between different data formats for plotting and analysis. We will need the tidyverse package. This is an “umbrella-package” that installs several packages useful for data analysis which work together well such as dplyr, ggplot2 (for visualisation), tibble, etc. library(&quot;tidyverse&quot;) ## ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ── ## ✔ tibble 1.4.2 ✔ dplyr 0.7.4.9003 ## ✔ purrr 0.2.4 ✔ stringr 1.3.0 ## ✔ tibble 1.4.2 ✔ forcats 0.3.0 ## ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::collapse() masks IRanges::collapse() ## ✖ dplyr::combine() masks MSnbase::combine(), Biobase::combine(), BiocGenerics::combine() ## ✖ dplyr::desc() masks IRanges::desc() ## ✖ tidyr::expand() masks S4Vectors::expand() ## ✖ dplyr::exprs() masks affy::exprs(), MSnbase::exprs(), Biobase::exprs() ## ✖ magrittr::extract() masks tidyr::extract() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::first() masks S4Vectors::first() ## ✖ dplyr::ident() masks msdata::ident() ## ✖ dplyr::lag() masks stats::lag() ## ✖ BiocGenerics::Position() masks ggplot2::Position(), base::Position() ## ✖ purrr::reduce() masks IRanges::reduce(), MSnbase::reduce() ## ✖ dplyr::rename() masks S4Vectors::rename() ## ✖ dplyr::select() masks AnnotationDbi::select() ## ✖ purrr::set_names() masks magrittr::set_names() ## ✖ dplyr::slice() masks IRanges::slice() The tidyverse package tries to address 3 major problems with some of base R functions: The results from a base R function sometimes depends on the type of data. Using R expressions in a non standard way, which can be confusing for new learners. Hidden arguments, having default operations that new learners are not aware of. To learn more about dplyr and tidyr, you may want to check out this handy data transformation with dplyr cheatsheet and this one about tidyr. Let’s start by reading the data using readr::read_csv that will produce a tibble. library(&quot;readr&quot;) iprg &lt;- read_csv(&quot;http://bit.ly/VisBiomedDataIprgCsv&quot;) ## Parsed with column specification: ## cols( ## Protein = col_character(), ## Log2Intensity = col_double(), ## Run = col_character(), ## Condition = col_character(), ## BioReplicate = col_integer(), ## Intensity = col_double(), ## TechReplicate = col_character() ## ) Tibbles are data frames, but they tweak some of the old behaviors of data frames. The data structure is very similar to a data frame. For our purposes the only differences are that: In addition to displaying the data type of each column under its name, it only prints the first few rows of data and only as many columns as fit on one screen. Columns of class character are never converted into factors. 2.6.1 Selecting columns and filtering rows We’re going to learn some of the most common dplyr functions: select(), filter(), mutate(), group_by(), and summarize(). To select columns of a data frame, use select(). The first argument to this function is the data frame, and the subsequent arguments are the columns to keep. select(iprg, Protein, Run, Condition) To choose rows based on a specific criteria, use filter(): filter(iprg, BioReplicate == 1) ## # A tibble: 9,079 x 7 ## Protein Log2Intensity Run Condition BioReplicate Intensity ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 1 1.18e8 ## 2 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 1 1.02e8 ## 3 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 1 1.01e8 ## 4 sp|O13297|C… 24.7 JD_0623201… Conditio… 1 2.76e7 ## 5 sp|O13297|C… 24.7 JD_0623201… Conditio… 1 2.68e7 ## 6 sp|O13297|C… 24.7 JD_0623201… Conditio… 1 2.76e7 ## 7 sp|O13329|F… 23.4 JD_0623201… Conditio… 1 1.09e7 ## 8 sp|O13329|F… 24.0 JD_0623201… Conditio… 1 1.69e7 ## 9 sp|O13329|F… 23.5 JD_0623201… Conditio… 1 1.16e7 ## 10 sp|O13539|T… 27.5 JD_0623201… Conditio… 1 1.92e8 ## # ... with 9,069 more rows, and 1 more variable: TechReplicate &lt;chr&gt; filter(iprg, Condition == &#39;Condition2&#39;) ## # A tibble: 9,081 x 7 ## Protein Log2Intensity Run Condition BioReplicate Intensity ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 2 1.20e8 ## 2 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 2 1.16e8 ## 3 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 2 1.02e8 ## 4 sp|O13297|C… 24.5 JD_0623201… Conditio… 2 2.41e7 ## 5 sp|O13297|C… 24.7 JD_0623201… Conditio… 2 2.68e7 ## 6 sp|O13297|C… 24.6 JD_0623201… Conditio… 2 2.51e7 ## 7 sp|O13329|F… 23.2 JD_0623201… Conditio… 2 9.45e6 ## 8 sp|O13329|F… 23.4 JD_0623201… Conditio… 2 1.13e7 ## 9 sp|O13329|F… 23.8 JD_0623201… Conditio… 2 1.43e7 ## 10 sp|O13539|T… 25.9 JD_0623201… Conditio… 2 6.18e7 ## # ... with 9,071 more rows, and 1 more variable: TechReplicate &lt;chr&gt; 2.6.2 Pipes But what if you wanted to select and filter at the same time? There are three ways to do this: use intermediate steps, nested functions, or pipes. With intermediate steps, you essentially create a temporary data frame and use that as input to the next function. This can clutter up your workspace with lots of objects. You can also nest functions (i.e. one function inside of another). This is handy, but can be difficult to read if too many functions are nested as things are evaluated from the inside out. The last option, pipes, are a fairly recent addition to R. Pipes let you take the output of one function and send it directly to the next, which is useful when you need to do many things to the same dataset. Pipes in R look like %&gt;% and are made available via the magrittr package, installed automatically with dplyr. If you use RStudio, you can type the pipe with Ctrl + Shift + M if you have a PC or Cmd + Shift + M if you have a Mac. iprg %&gt;% filter(Intensity &gt; 1e8) %&gt;% select(Protein, Condition, Intensity) ## # A tibble: 4,729 x 3 ## Protein Condition Intensity ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 sp|D6VTK4|STE2_YEAST Condition1 117845016. ## 2 sp|D6VTK4|STE2_YEAST Condition1 102273602. ## 3 sp|D6VTK4|STE2_YEAST Condition1 100526837. ## 4 sp|D6VTK4|STE2_YEAST Condition2 119765106. ## 5 sp|D6VTK4|STE2_YEAST Condition2 116382798. ## 6 sp|D6VTK4|STE2_YEAST Condition2 102328260. ## 7 sp|D6VTK4|STE2_YEAST Condition3 103830944. ## 8 sp|D6VTK4|STE2_YEAST Condition4 102150172. ## 9 sp|D6VTK4|STE2_YEAST Condition4 105724288. ## 10 sp|O13539|THP2_YEAST Condition1 192490784. ## # ... with 4,719 more rows In the above, we use the pipe to send the iprg dataset first through filter() to keep rows where Intensity is greater than 1e8, then through select() to keep only the Protein, Condition, and Intensity columns. Since %&gt;% takes the object on its left and passes it as the first argument to the function on its right, we don’t need to explicitly include it as an argument to the filter() and select() functions anymore. If we wanted to create a new object with this smaller version of the data, we could do so by assigning it a new name: iprg_sml &lt;- iprg %&gt;% filter(Intensity &gt; 1e8) %&gt;% select(Protein, Condition, Intensity) iprg_sml ## # A tibble: 4,729 x 3 ## Protein Condition Intensity ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 sp|D6VTK4|STE2_YEAST Condition1 117845016. ## 2 sp|D6VTK4|STE2_YEAST Condition1 102273602. ## 3 sp|D6VTK4|STE2_YEAST Condition1 100526837. ## 4 sp|D6VTK4|STE2_YEAST Condition2 119765106. ## 5 sp|D6VTK4|STE2_YEAST Condition2 116382798. ## 6 sp|D6VTK4|STE2_YEAST Condition2 102328260. ## 7 sp|D6VTK4|STE2_YEAST Condition3 103830944. ## 8 sp|D6VTK4|STE2_YEAST Condition4 102150172. ## 9 sp|D6VTK4|STE2_YEAST Condition4 105724288. ## 10 sp|O13539|THP2_YEAST Condition1 192490784. ## # ... with 4,719 more rows Note that the final data frame is the leftmost part of this expression. Challenge Using pipes, subset the iprg data to include Proteins with a log2 intensity greater than 20 and retain only the columns Proteins, and Condition. ## Answer iprg %&gt;% filter(Log2Intensity &gt; 20) %&gt;% select(Protein, Condition) 2.6.3 Mutate Frequently you’ll want to create new columns based on the values in existing columns, for example to do unit conversions, or find the ratio of values in two columns. For this we’ll use mutate(). To create a new column of weight in kg: iprg %&gt;% mutate(Log10Intensity = log10(Intensity)) ## # A tibble: 36,321 x 8 ## Protein Log2Intensity Run Condition BioReplicate Intensity ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 1 1.18e8 ## 2 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 1 1.02e8 ## 3 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 1 1.01e8 ## 4 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 2 1.20e8 ## 5 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 2 1.16e8 ## 6 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 2 1.02e8 ## 7 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 3 1.04e8 ## 8 sp|D6VTK4|S… 26.5 JD_0623201… Conditio… 3 9.47e7 ## 9 sp|D6VTK4|S… 26.5 JD_0623201… Conditio… 3 9.69e7 ## 10 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 4 1.02e8 ## # ... with 36,311 more rows, and 2 more variables: TechReplicate &lt;chr&gt;, ## # Log10Intensity &lt;dbl&gt; You can also create a second new column based on the first new column within the same call of mutate(): iprg %&gt;% mutate(Log10Intensity = log10(Intensity), Log10Intensity2 = Log10Intensity * 2) ## # A tibble: 36,321 x 9 ## Protein Log2Intensity Run Condition BioReplicate Intensity ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 1 1.18e8 ## 2 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 1 1.02e8 ## 3 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 1 1.01e8 ## 4 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 2 1.20e8 ## 5 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 2 1.16e8 ## 6 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 2 1.02e8 ## 7 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 3 1.04e8 ## 8 sp|D6VTK4|S… 26.5 JD_0623201… Conditio… 3 9.47e7 ## 9 sp|D6VTK4|S… 26.5 JD_0623201… Conditio… 3 9.69e7 ## 10 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 4 1.02e8 ## # ... with 36,311 more rows, and 3 more variables: TechReplicate &lt;chr&gt;, ## # Log10Intensity &lt;dbl&gt;, Log10Intensity2 &lt;dbl&gt; If this runs off your screen and you just want to see the first few rows, you can use a pipe to view the head() of the data. (Pipes work with non-dplyr functions, too, as long as the dplyr or magrittr package is loaded). iprg %&gt;% mutate(Log10Intensity = log10(Intensity)) %&gt;% head ## # A tibble: 6 x 8 ## Protein Log2Intensity Run Condition BioReplicate Intensity ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 sp|D6VTK4|S… 26.8 JD_06232014… Conditio… 1 1.18e8 ## 2 sp|D6VTK4|S… 26.6 JD_06232014… Conditio… 1 1.02e8 ## 3 sp|D6VTK4|S… 26.6 JD_06232014… Conditio… 1 1.01e8 ## 4 sp|D6VTK4|S… 26.8 JD_06232014… Conditio… 2 1.20e8 ## 5 sp|D6VTK4|S… 26.8 JD_06232014… Conditio… 2 1.16e8 ## 6 sp|D6VTK4|S… 26.6 JD_06232014… Conditio… 2 1.02e8 ## # ... with 2 more variables: TechReplicate &lt;chr&gt;, Log10Intensity &lt;dbl&gt; Note that we don’t include parentheses at the end of our call to head() above. When piping into a function with no additional arguments, you can call the function with or without parentheses (e.g. head or head()). If you want to display more data, you can use the print() function at the end of your chain with the argument n specifying the number of rows to display: iprg %&gt;% mutate(Log10Intensity = log10(Intensity), Log10Intensity2 = Log10Intensity * 2) %&gt;% print(n = 20) ## # A tibble: 36,321 x 9 ## Protein Log2Intensity Run Condition BioReplicate Intensity ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 1 1.18e8 ## 2 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 1 1.02e8 ## 3 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 1 1.01e8 ## 4 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 2 1.20e8 ## 5 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 2 1.16e8 ## 6 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 2 1.02e8 ## 7 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 3 1.04e8 ## 8 sp|D6VTK4|S… 26.5 JD_0623201… Conditio… 3 9.47e7 ## 9 sp|D6VTK4|S… 26.5 JD_0623201… Conditio… 3 9.69e7 ## 10 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 4 1.02e8 ## 11 sp|D6VTK4|S… 26.4 JD_0623201… Conditio… 4 8.77e7 ## 12 sp|D6VTK4|S… 26.7 JD_0623201… Conditio… 4 1.06e8 ## 13 sp|O13297|C… 24.7 JD_0623201… Conditio… 1 2.76e7 ## 14 sp|O13297|C… 24.7 JD_0623201… Conditio… 1 2.68e7 ## 15 sp|O13297|C… 24.7 JD_0623201… Conditio… 1 2.76e7 ## 16 sp|O13297|C… 24.5 JD_0623201… Conditio… 2 2.41e7 ## 17 sp|O13297|C… 24.7 JD_0623201… Conditio… 2 2.68e7 ## 18 sp|O13297|C… 24.6 JD_0623201… Conditio… 2 2.51e7 ## 19 sp|O13297|C… 24.4 JD_0623201… Conditio… 3 2.20e7 ## 20 sp|O13297|C… 24.6 JD_0623201… Conditio… 3 2.59e7 ## # ... with 3.63e+04 more rows, and 3 more variables: TechReplicate &lt;chr&gt;, ## # Log10Intensity &lt;dbl&gt;, Log10Intensity2 &lt;dbl&gt; Let’s use a modified iprg data that contains missing values for the next example. It can be loaded with download.file(&quot;http://bit.ly/VisBiomedDataIprgNA&quot;, &quot;./data/iprgna.rda&quot;) load(&quot;./data/iprgna.rda&quot;) Challenge Using the iprgna data repeat the creation of a new Log10Intensisty column. iprgna %&gt;% mutate(Log10Intensity = log10(Intensity)) ## # A tibble: 36,321 x 7 ## Protein Run Condition BioReplicate Intensity TechReplicate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 sp|D6VTK4|S… JD_0623201… Conditio… 1 NA B ## 2 sp|D6VTK4|S… JD_0623201… Conditio… 1 1.02e8 C ## 3 sp|D6VTK4|S… JD_0623201… Conditio… 1 NA A ## 4 sp|D6VTK4|S… JD_0623201… Conditio… 2 1.20e8 A ## 5 sp|D6VTK4|S… JD_0623201… Conditio… 2 NA B ## 6 sp|D6VTK4|S… JD_0623201… Conditio… 2 1.02e8 C ## 7 sp|D6VTK4|S… JD_0623201… Conditio… 3 1.04e8 A ## 8 sp|D6VTK4|S… JD_0623201… Conditio… 3 9.47e7 B ## 9 sp|D6VTK4|S… JD_0623201… Conditio… 3 9.69e7 C ## 10 sp|D6VTK4|S… JD_0623201… Conditio… 4 1.02e8 B ## # ... with 36,311 more rows, and 1 more variable: Log10Intensity &lt;dbl&gt; The first few rows of the output are full of NAs, so if we wanted to remove those we could insert a filter() in the chain: iprgna %&gt;% filter(!is.na(Intensity)) %&gt;% mutate(Log10Intensity = log10(Intensity)) ## # A tibble: 35,318 x 7 ## Protein Run Condition BioReplicate Intensity TechReplicate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 sp|D6VTK4|S… JD_0623201… Conditio… 1 1.02e8 C ## 2 sp|D6VTK4|S… JD_0623201… Conditio… 2 1.20e8 A ## 3 sp|D6VTK4|S… JD_0623201… Conditio… 2 1.02e8 C ## 4 sp|D6VTK4|S… JD_0623201… Conditio… 3 1.04e8 A ## 5 sp|D6VTK4|S… JD_0623201… Conditio… 3 9.47e7 B ## 6 sp|D6VTK4|S… JD_0623201… Conditio… 3 9.69e7 C ## 7 sp|D6VTK4|S… JD_0623201… Conditio… 4 1.02e8 B ## 8 sp|D6VTK4|S… JD_0623201… Conditio… 4 8.77e7 C ## 9 sp|D6VTK4|S… JD_0623201… Conditio… 4 1.06e8 A ## 10 sp|O13297|C… JD_0623201… Conditio… 1 2.76e7 B ## # ... with 35,308 more rows, and 1 more variable: Log10Intensity &lt;dbl&gt; is.na() is a function that determines whether something is an NA. The ! symbol negates the result, so we’re asking for everything that is not an NA. 2.6.4 Split-apply-combine data analysis and the summarize() function Many data analysis tasks can be approached using the split-apply-combine paradigm: split the data into groups, apply some analysis to each group, and then combine the results. dplyr makes this very easy through the use of the group_by() function. 2.6.4.1 The summarize() function group_by() is often used together with summarize(), which collapses each group into a single-row summary of that group. group_by() takes as arguments the column names that contain the categorical variables for which you want to calculate the summary statistics. So to view the mean weight by sex: iprgna %&gt;% group_by(Condition) %&gt;% summarize(mean_Intensity = mean(Intensity)) ## # A tibble: 4 x 2 ## Condition mean_Intensity ## &lt;chr&gt; &lt;dbl&gt; ## 1 Condition1 NA ## 2 Condition2 NA ## 3 Condition3 NA ## 4 Condition4 NA Unfortunately, the mean of any vector that contains even a single missing value is NA. We need to remove missing values before calculating the mean, which is done easily with the na.rm argument. iprgna %&gt;% group_by(Condition) %&gt;% summarize(mean_Intensity = mean(Intensity, na.rm = TRUE)) ## # A tibble: 4 x 2 ## Condition mean_Intensity ## &lt;chr&gt; &lt;dbl&gt; ## 1 Condition1 65144912. ## 2 Condition2 64439756. ## 3 Condition3 62475797. ## 4 Condition4 63616488. You can also group by multiple columns: iprgna %&gt;% group_by(TechReplicate, BioReplicate) %&gt;% summarize(mean_Intensity = mean(Intensity, na.rm = TRUE)) ## # A tibble: 12 x 3 ## # Groups: TechReplicate [?] ## TechReplicate BioReplicate mean_Intensity ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 A 1 64891444. ## 2 A 2 63870255. ## 3 A 3 61648150. ## 4 A 4 63662564. ## 5 B 1 65563938. ## 6 B 2 65164270. ## 7 B 3 62758494. ## 8 B 4 64196979. ## 9 C 1 64978764. ## 10 C 2 64283727. ## 11 C 3 63020774. ## 12 C 4 62984686. 2.6.4.2 Tallying When working with data, it is also common to want to know the number of observations found for each factor or combination of factors. For this, dplyr provides tally(). iprgna %&gt;% group_by(Condition) %&gt;% tally ## # A tibble: 4 x 2 ## Condition n ## &lt;chr&gt; &lt;int&gt; ## 1 Condition1 9079 ## 2 Condition2 9081 ## 3 Condition3 9081 ## 4 Condition4 9080 Here, tally() is the action applied to the groups created by group_by() and counts the total number of records for each category. Challenge How many proteins of each technical replicate are there? Use group_by() and summarize() to find the mean, min, and max intensity for each condition. What are the proteins with the highest intensity in each condition? ## Answer 1 iprgna %&gt;% group_by(TechReplicate) %&gt;% tally ## # A tibble: 3 x 2 ## TechReplicate n ## &lt;chr&gt; &lt;int&gt; ## 1 A 12107 ## 2 B 12106 ## 3 C 12108 ## Answer 2 iprgna %&gt;% filter(!is.na(Intensity)) %&gt;% group_by(Condition) %&gt;% summarize(mean_int = mean(Intensity), min_int = min(Intensity), max_int = max(Intensity)) ## # A tibble: 4 x 4 ## Condition mean_int min_int max_int ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Condition1 65144912. 254608. 2841953257. ## 2 Condition2 64439756. 259513. 2757471311. ## 3 Condition3 62475797. 88409. 2659018724. ## 4 Condition4 63616488. 84850. 2881057105. ## Answer 3 iprgna %&gt;% filter(!is.na(Intensity)) %&gt;% group_by(Condition) %&gt;% filter(Intensity == max(Intensity)) %&gt;% arrange(Intensity) ## # A tibble: 4 x 6 ## # Groups: Condition [4] ## Protein Run Condition BioReplicate Intensity TechReplicate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 sp|P48589|R… JD_06232014… Conditio… 3 2.66e9 B ## 2 sp|P48589|R… JD_06232014… Conditio… 2 2.76e9 B ## 3 sp|P48589|R… JD_06232014… Conditio… 1 2.84e9 A ## 4 sp|P48589|R… JD_06232014… Conditio… 4 2.88e9 A "],
["exploring-and-visualising-biomolecular-data.html", "Chapter 3 Exploring and visualising biomolecular data", " Chapter 3 Exploring and visualising biomolecular data This chapter provides an overview of a typical exploratory data analysis, statistical analysis and their associated visualisations. We will look into the respective figures and how to produce them in the later chapters. "],
["plotting-in-r.html", "Chapter 4 Plotting in R 4.1 Base graphics 4.2 Plotting with ggplot2 4.3 Customising plots 4.4 Saving your figures 4.5 References", " Chapter 4 Plotting in R 4.1 Base graphics We won’t go through all base graphics plotting functions one by one here. We will encounter and learn several of these functions throughout the course and, if necessary, discuss them when questions arise. 4.2 Plotting with ggplot2 Base graphics uses a canvas model a series of instructions that sequentially fill the plotting canvas. While this model is very useful to build plots bits by bits bottom up, which is useful in some cases, it has some clear drawback: Layout choices have to be made without global overview over what may still be coming. Different functions for different plot types with different interfaces. No standard data input. Many routine tasks require a lot of boilerplate code. No concept of facets/lattices/viewports. Poor default colours. The ggplot2 package implements a grammar of graphics. Users describe what and how to visualise data and the package then generates the figure. The components of ggplot2’s of graphics are A tidy dataset A choice of geometric objects that servers as the visual representation of the data - for instance, points, lines, rectangles, contours. A description of how the variables in the data are mapped to visual properties (aesthetics) or the geometric objects, and an associated scale (e.g. linear, logarithmic, polar) A statistical summarisation rule A coordinate system. A facet specification, i.e. the use of several plots to look at the same data. Fist of all, we need to load the ggplot2 package and load the iprg data. library(&quot;ggplot2&quot;) iprg &lt;- read.csv(&quot;http://bit.ly/VisBiomedDataIprgCsv&quot;) ggplot graphics are built step by step by adding new elements. To build a ggplot we need to: bind the plot to a specific data frame using the data argument ggplot(data = iprg) define aesthetics (aes), by selecting the variables to be plotted and the variables to define the presentation such as plotting size, shape color, etc. ggplot(data = iprg, aes(x = Run, y = Log2Intensity)) add geoms – graphical representation of the data in the plot (points, lines, bars). To add a geom to the plot use + operator ggplot(data = iprg, aes(x = Run, y = Log2Intensity)) + geom_boxplot() See the documentation page to explore the many available geoms. The + in the ggplot2 package is particularly useful because it allows you to modify existing ggplot objects. This means you can easily set up plot “templates” and conveniently explore different types of plots, so the above plot can also be generated with code like this: ## Assign plot to a variable ints_plot &lt;- ggplot(data = iprg, aes(x = Run, y = Log2Intensity)) ## Draw the plot ints_plot + geom_boxplot() Notes: Anything you put in the ggplot() function can be seen by any geom layers that you add (i.e., these are universal plot settings). This includes the x and y axis you set up in aes(). You can also specify aesthetics for a given geom independently of the aesthetics defined globally in the ggplot() function. The + sign used to add layers must be placed at the end of each line containing a layer. If, instead, the + sign is added in the line before the other layer, ggplot2 will not add the new layer and will return an error message. Challenge Repeat the plot above but displaying the raw intensities. Log-10 transform the raw intensities on the flight when plotting. ggplot(data = iprg, aes(x = Run, y = Intensity)) + geom_boxplot() ggplot(data = iprg, aes(x = Run, y = log10(Intensity))) + geom_boxplot() 4.3 Customising plots First, let’s colour the boxplot based on the condition: ggplot(data = iprg, aes(x = Run, y = Log2Intensity, fill = Condition)) + geom_boxplot() Now let’s rename all axis labels and title, and rotate the x-axis labels 90 degrees. We can add those specifications using the labs and theme functions of the ggplot2 package. ggplot(aes(x = Run, y = Log2Intensity, fill = Condition), data = iprg) + geom_boxplot() + labs(title = &#39;Log2 transformed intensity distribution per MS run&#39;, y = &#39;Log2(Intensity)&#39;, x = &#39;MS run&#39;) + theme(axis.text.x = element_text(angle = 90)) And easily switch from a boxplot to a violin plot representation by changing the geom type. ggplot(aes(x = Run, y = Log2Intensity, fill = Condition), data = iprg) + geom_violin() + labs(title = &#39;Log2 transformed intensity distribution per Subject&#39;, y = &#39;Log2(Intensity)&#39;, x = &#39;MS run&#39;) + theme(axis.text.x = element_text(angle = 90)) Finally, we can also overlay multiple geoms by simply adding them one after the other. p &lt;- ggplot(aes(x = Run, y = Log2Intensity, fill = Condition), data = iprg) p + geom_boxplot() p + geom_boxplot() + geom_jitter() ## not very usefull p + geom_jitter() + geom_boxplot() p + geom_jitter(alpha = 0.1) + geom_boxplot() Challenge Overlay a boxplot goem on top of a jitter geom for the raw or log-10 transformed intensities. Customise the plot as suggested above. ## Note how the log10 transformation is applied to both geoms ggplot(data = iprg, aes(x = Run, y = log10(Intensity))) + geom_jitter(alpha = 0.1) + geom_boxplot() Finally, a very useful feature of ggplot2 is facetting, that defines how to subset the data into different panels (facets). names(iprg) ## [1] &quot;Protein&quot; &quot;Log2Intensity&quot; &quot;Run&quot; &quot;Condition&quot; ## [5] &quot;BioReplicate&quot; &quot;Intensity&quot; &quot;TechReplicate&quot; ggplot(data = iprg, aes(x = TechReplicate, y = Log2Intensity, fill = Condition)) + geom_boxplot() + facet_grid(~ Condition) 4.4 Saving your figures You can save plots to a number of different file formats. PDF is by far the most common format because it’s lightweight, cross-platform and scales up well but jpegs, pngs and a number of other file formats are also supported. Let’s redo the last barplot but save it to the file system this time. Let’s save the boxplot as pdf file. pdf() p + geom_jitter(alpha = 0.1) + geom_boxplot() dev.off() The default file name is Rplots.pdf. We can customise that file name specifying it by passing the file name, as a character, to the pdf() function. Challenge Save a figure of your choice to a pdf file. Read the manual for the png function and save that same image to a png file. Tip: save your figures in a dedicated directory. 4.5 References ggplot2 extensions - gallery ggplot2 webpage and documentation ggplot2: Elegant Graphics for Data Analysis book (source of the book available for free here). "],
["tools-and-plots.html", "Chapter 5 Tools and plots 5.1 Transformations 5.2 Comparing samples and linear models 5.3 Plots for statistical analyses 5.4 Visualising intersections 5.5 Unsupervised learning 5.6 k-means clustering 5.7 Hierarchical clustering 5.8 Pre-processing 5.9 Principal component analysis (PCA) 5.10 t-Distributed Stochastic Neighbour Embedding 5.11 PCA loadings vs differential expression 5.12 Heatmaps", " Chapter 5 Tools and plots 5.1 Transformations 5.1.1 Expression data Let’s start with the comparison of two vectors of matching expression intensities such as those from two samples in the iprg3 dataset. Let’s extract the intensities of samples JD_06232014_sample1-A.raw (second column) and JD_06232014_sample1_B.raw (third column) and produce a scatter plot of one against the other. x &lt;- iprg3[[2]] y &lt;- iprg3[[3]] plot(x, y) Due to the distribution of the raw intensities, where most of the intensities are low with very few high intensities (see density plots below), the majority of points are squeezed close to the origin of the scatter plot. plot(density(na.omit(x)), col = &quot;blue&quot;) lines(density(na.omit(y)), col = &quot;red&quot;) This has negative effects as it (1) leads to overplotting in the low intensity range and (2) gives too much confidence in the correlation of the two vectors. A simple way to avoid this effect is to directly log-tranform the data or set the graph axes to log scales: par(mfrow = c(1, 2)) plot(log10(x), log10(y)) plot(x, y, log = &quot;xy&quot;) We will see better visualisations to detect correlation between sample replicates below. It is possible to generalise to production of scatter plots to more samples using the pairs function: pairs(iprg3[2:6], log = &quot;xy&quot;) A lot of space is wasted by repeating the same sets of plots in the upper right and lower left triangles of the matrix. See the pairs documentation page. A general technique to overcome overplotting is to set the alpha scale (transparency), of to use graphics::smoothScatter: par(mfrow = c(1, 2)) plot(x, y, pch = 19, col = &quot;#00000010&quot;, log = &quot;xy&quot;) smoothScatter(log10(x), log10(y)) 5.1.2 Fold changes Log-transformation also comes handy when computing fold-changes. Below we calculate the fold-changes and log2 fold-changes (omitting missing values) fc &lt;- na.omit(iprg3[[2]] / iprg3[[3]]) lfc &lt;- log2(fc) Below, we see how the log2 fold-changes become symmetrical around zero (the absence of change), with positive values corresponding to up-regulation and negative values to down-regulation. plot(density(lfc), ylim = c(0, 5)) abline(v = median(lfc)) lines(density(fc), col = &quot;red&quot;) abline(v = median(fc), col = &quot;red&quot;) Note: when the data is already log-transformed, log fold-changes are computed by subtracting values. 5.2 Comparing samples and linear models Let’s return to the scatter plot example above and focus on three replicates from consitions 1 and 4, remove missing values and log-tranform the intensites. x &lt;- log2(na.omit(iprg3[, c(2, 3, 11)])) Below, we use the pairs function and print the pairwise correlations in the upper right traingle. ## put (absolute) correlations on the upper panels, ## with size proportional to the correlations. ## From ?pairs panel.cor &lt;- function(x, y, digits = 2, prefix = &quot;&quot;, cex.cor, ...) { usr &lt;- par(&quot;usr&quot;); on.exit(par(usr)) par(usr = c(0, 1, 0, 1)) r &lt;- abs(cor(x, y)) txt &lt;- format(c(r, 0.123456789), digits = digits)[1] txt &lt;- paste0(prefix, txt) if (missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt) text(0.5, 0.5, txt, cex = cex.cor * r) } pairs(x, lower.panel = panel.smooth, upper.panel = panel.cor) It is often assumed that high correlation is a halmark of good replication. Rather than focus on the correlation of the data, a better measurement would be to look a the log2 fold-changes, i.e. the distance between repeated measurements. The ideal way to visualise this is on an MA-plot: par(mfrow = c(1, 2)) r1 &lt;- x[[1]] r2 &lt;- x[[2]] M &lt;- r1 - r2 A &lt;- (r1 + r2)/2 plot(A, M); grid() library(&quot;affy&quot;) affy::ma.plot(A, M) See also this post on the Simply Statistics blog. abline(0, 1) can be used to add a line with intercept 0 and slop 1. It we want to add the line that models the data linearly, we can calculate the parameters using the lm function: lmod &lt;- lm(r2 ~ r1) summary(lmod) ## ## Call: ## lm(formula = r2 ~ r1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4939 -0.0721 0.0126 0.0881 3.4595 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.348190 0.091842 3.791 0.000153 *** ## r1 0.985878 0.003688 267.357 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3263 on 3024 degrees of freedom ## Multiple R-squared: 0.9594, Adjusted R-squared: 0.9594 ## F-statistic: 7.148e+04 on 1 and 3024 DF, p-value: &lt; 2.2e-16 which can be used to add the adequate line that reflects the (linear) relationship between the two data plot(r1, r2) abline(lmod, col = &quot;red&quot;) As we have seen in the beginning of this section, it is essential not to rely solely on the correlation value, but look at the data. This also holds true for linear (or any) modelling, which can be done by plotting the model: par(mfrow = c(2, 2)) plot(lmod) Cook’s distance is a commonly used estimate of the influence of a data point when performing a least-squares regression analysis and can be used to highlight points that particularly influence the regression. Leverage quantifies the influence of a given observation on the regression due to its location in the space of the inputs. See also ?influence.measures. Challenge Take any of the iprg3 replicates, model and plot their linear relationship. The Anscombe quartet is available as anscombe. Load it, create a linear model for one \\((x_i, y_i)\\) pair of your choice and visualise/check the model. x3 &lt;- anscombe[, 3] y3 &lt;- anscombe[, 7] lmod &lt;- lm(y3 ~ x3) summary(lmod) ## ## Call: ## lm(formula = y3 ~ x3) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1586 -0.6146 -0.2303 0.1540 3.2411 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0025 1.1245 2.670 0.02562 * ## x3 0.4997 0.1179 4.239 0.00218 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.236 on 9 degrees of freedom ## Multiple R-squared: 0.6663, Adjusted R-squared: 0.6292 ## F-statistic: 17.97 on 1 and 9 DF, p-value: 0.002176 par(mfrow = c(2, 2)) plot(lmod) 5.3 Plots for statistical analyses Let’s use the ALL_bclneg dataset, that we already have analysed with limma in the Data chapter. Whenever performing a statistical test, it is important to quality check the distribution of non-adjusted p-values. Below, we see an enrichment of small p-values, as opposed to a uniform distribution to be expected under the null hypothesis of absence of changes between groups. fvarLabels(ALL_bcrneg) ## [1] &quot;logFC&quot; &quot;AveExpr&quot; &quot;t&quot; &quot;P.Value&quot; &quot;adj.P.Val&quot; &quot;B&quot; hist(fData(ALL_bcrneg)$P.Value) The histograms below illustrate other distributions to beware of. Another important visualisation for statistical results are the Volcano plots, that show the relationship between the significance of the individual tests (adjusted p-values) and their magnitude of the effect (log2 fold-changes). lfc &lt;- fData(ALL_bcrneg)$logFC bh &lt;- fData(ALL_bcrneg)$adj.P.Val plot(lfc, -log10(bh), main = &quot;Volcano plot&quot;, xlab = expression(log[2]~fold-change), ylab = expression(-log[10]~adjusted~p-value)) grid() The volcano plot can further be annotated using vertical and horizontal lines depicting thresholds of interest or points can be colour-coded based on their interest. lfc &lt;- fData(ALL_bcrneg)$logFC bh &lt;- fData(ALL_bcrneg)$adj.P.Val sign &lt;- abs(lfc) &gt; 1 &amp; bh &lt; 0.01 plot(lfc, -log10(bh), main = &quot;Volcano plot&quot;, col = ifelse(sign, &quot;red&quot;, &quot;black&quot;), pch = ifelse(sign, 19, 1), xlab = expression(log[2]~fold-change), ylab = expression(-log[10]~adjusted~p-value)) grid() abline(v = c(-1, 1), lty = &quot;dotted&quot;) abline(h = -log10(0.05), lty = &quot;dotted&quot;) It is also possible to identify and label individual points on the plot using the identify function i &lt;- identify(lfc, -log10(bh), featureNames(ALL_bcrneg)) 5.4 Visualising intersections Venn and Euler diagrams are popular representation when comparing sets and their intersection. Two useful R packages to generate such plots are venneuler and Vennerable. We will use the crc feature names to generate a test data: set.seed(123) x &lt;- replicate(3, sample(featureNames(crc), 35), simplify = FALSE) names(x) &lt;- LETTERS[1:3] (v &lt;- Venn(x)) ## A Venn object on 3 sets named ## A,B,C ## 000 100 010 110 001 101 011 111 ## 0 11 9 6 5 10 12 8 plot(v) The UpSetR visualises intersections of sets as a matrix in which the rows represent the sets and the columns represent their intersection sizes. For each set that is part of a given intersection, a black filled circle is placed in the corresponding matrix cell. If a set is not part of the intersection, a light gray circle is shown. A vertical black line connects the topmost black circle with the bottom most black circle in each column to emphasise the column-based relationships. The size of the intersections is shown as a bar chart placed on top of the matrix so that each column lines up with exactly one bar. A second bar chart showing the size of the each set is shown to the left of the matrix. We will first make use of the fromList function to convert our list to a UpSetR compatible input and then generate the figure: library(&quot;UpSetR&quot;) x2 &lt;- fromList(x) upset(x2) The following tweet by the author of the package illustrates how Venn and upset diagrams relate to each other. upset(x2, order.by = &quot;freq&quot;) upset(x2, order.by = &quot;degree&quot;) upset(x2, order.by = c(&quot;freq&quot;, &quot;degree&quot;)) upset(x2, order.by = c(&quot;degree&quot;, &quot;freq&quot;)) upset(x2, sets = c(&quot;A&quot;, &quot;B&quot;)) upset(x2, sets = c(&quot;B&quot;, &quot;C&quot;, &quot;A&quot;), keep.order = TRUE) upset(x2, group.by = &quot;sets&quot;) ## Add set D with a single intersection x3 &lt;- x2 x3$D &lt;- 0 x3[1, &quot;D&quot;] &lt;- 1 head(x3) ## A B C D ## 1 1 0 1 1 ## 2 1 0 1 0 ## 3 1 0 0 0 ## 4 1 0 0 0 ## 5 1 0 0 0 ## 6 1 0 0 0 upset(x3) upset(x3, empty.intersections = &quot;on&quot;) Visualising intersections with UpSetR shines with more that 4 sets, as Venn diagrams become practically useless. Challenge Generate a bigger dataset containing 10 sets. Try to generate Venn and upset diagrams as shown above. set.seed(123) x &lt;- replicate(10, sample(featureNames(crc), 35), simplify = FALSE) names(x) &lt;- LETTERS[1:10] When the number of sets become larger, the options above, as well as nsets, the number of sets (default is 5) and nintersects, the number of intersectios (default is 40) becomes useful. 5.5 Unsupervised learning In unsupervised learning (UML), no labels are provided, and the learning algorithm focuses solely on detecting structure in unlabelled input data. One generally differentiates between Clustering, where the goal is to find homogeneous subgroups within the data; the grouping is based on distance between observations. Dimensionality reduction, where the goal is to identify patterns in the features of the data. Dimensionality reduction is often used to facilitate visualisation of the data, as well as a pre-processing method before supervised learning. UML presents specific challenges and benefits: there is no single goal in UML there is generally much more unlabelled data available than labelled data. Unsupervised learning techniques are paramount for exploratory data analysis and visualisation. 5.6 k-means clustering The k-means clustering algorithms aims at partitioning n observations into a fixed number of k clusters. The algorithm will find homogeneous clusters. In R, we use stats::kmeans(x, centers = 3, nstart = 10) where x is a numeric data matrix centers is the pre-defined number of clusters the k-means algorithm has a random component and can be repeated nstart times to improve the returned model Challenge: To learn about k-means, let’s use the iris dataset with the sepal and petal length variables only (to facilitate visualisation). Create such a data matrix and name it x Run the k-means algorithm on the newly generated data x, save the results in a new variable cl, and explore its output when printed. The actual results of the algorithms, i.e. the cluster membership can be accessed in the clusters element of the clustering result output. Use it to colour the inferred clusters to generate a figure like that shown below. Figure 5.1: k-means algorithm on sepal and petal lengths i &lt;- grep(&quot;Length&quot;, names(iris)) x &lt;- iris[, i] cl &lt;- kmeans(x, 3, nstart = 10) plot(x, col = cl$cluster) How does k-means work Initialisation: randomly assign class membership set.seed(12) init &lt;- sample(3, nrow(x), replace = TRUE) plot(x, col = init) Figure 5.2: k-means random intialisation Iteration: Calculate the centre of each subgroup as the average position of all observations is that subgroup. Each observation is then assigned to the group of its nearest centre. It’s also possible to stop the algorithm after a certain number of iterations, or once the centres move less than a certain distance. par(mfrow = c(1, 2)) plot(x, col = init) centres &lt;- sapply(1:3, function(i) colMeans(x[init == i, ], )) centres &lt;- t(centres) points(centres[, 1], centres[, 2], pch = 19, col = 1:3) tmp &lt;- dist(rbind(centres, x)) tmp &lt;- as.matrix(tmp)[, 1:3] ki &lt;- apply(tmp, 1, which.min) ki &lt;- ki[-(1:3)] plot(x, col = ki) points(centres[, 1], centres[, 2], pch = 19, col = 1:3) Figure 5.3: k-means iteration: calculate centers (left) and assign new cluster membership (right) Termination: Repeat iteration until no point changes its cluster membership. k-means convergence (credit Wikipedia) Model selection Due to the random initialisation, one can obtain different clustering results. When k-means is run multiple times, the best outcome, i.e. the one that generates the smallest total within cluster sum of squares (SS), is selected. The total within SS is calculated as: For each cluster results: for each observation, determine the squared euclidean distance from observation to centre of cluster sum all distances Note that this is a local minimum; there is no guarantee to obtain a global minimum. Challenge: Repeat k-means on our x data multiple times, setting the number of iterations to 1 or greater and check whether you repeatedly obtain the same results. Try the same with random data of identical dimensions. cl1 &lt;- kmeans(x, centers = 3, nstart = 10) cl2 &lt;- kmeans(x, centers = 3, nstart = 10) table(cl1$cluster, cl2$cluster) ## ## 1 2 3 ## 1 58 0 0 ## 2 0 41 0 ## 3 0 0 51 cl1 &lt;- kmeans(x, centers = 3, nstart = 1) cl2 &lt;- kmeans(x, centers = 3, nstart = 1) table(cl1$cluster, cl2$cluster) ## ## 1 2 3 ## 1 0 0 41 ## 2 0 51 0 ## 3 58 0 0 set.seed(42) xr &lt;- matrix(rnorm(prod(dim(x))), ncol = ncol(x)) cl1 &lt;- kmeans(xr, centers = 3, nstart = 1) cl2 &lt;- kmeans(xr, centers = 3, nstart = 1) table(cl1$cluster, cl2$cluster) ## ## 1 2 3 ## 1 46 0 6 ## 2 1 51 0 ## 3 0 1 45 diffres &lt;- cl1$cluster != cl2$cluster par(mfrow = c(1, 2)) plot(xr, col = cl1$cluster, pch = ifelse(diffres, 19, 1)) plot(xr, col = cl2$cluster, pch = ifelse(diffres, 19, 1)) Figure 5.4: Different k-means results on the same (random) data How to determine the number of clusters Run k-means with k=1, k=2, …, k=n Record total within SS for each value of k. Choose k at the elbow position, as illustrated below. Challenge Calculate the total within sum of squares for k from 1 to 5 for our x test data, and reproduce the figure above. ks &lt;- 1:5 tot_within_ss &lt;- sapply(ks, function(k) { cl &lt;- kmeans(x, k, nstart = 10) cl$tot.withinss }) plot(ks, tot_within_ss, type = &quot;b&quot;) There exists other metrics, other than the total within cluster sum of squares that can be applied, such as the gap statistic (see cluster::clusGap), or the Akaike (AIC) and Bayesian (BIC) information criteria. Challenge Let’s use what we have learned to cluster the 2337 proteins from the mulvey2015norm data in 20 clusters. Use k-means to cluster the mulvey2015norm data, setting centers = 20. Take care in repeating the clustering more than once. To plot the expression profiles for the 20 clusters, I suggest to use gplot2. Do do so, create a 2337 proteins by 18 sample dataframe (or tibble), appending the protein accession numbers (from the feature data - you can use the MSnbase::ms2df helper function.) and cluster numbers as 2 additional columns. Use gather to transform the data in a long format. Use ggplot2 to reproduce the figure below. Optional: use stat_summary to add a mean profile for each cluster of proteins. kmeans clustering on mulvey2015norm library(&quot;pRolocdata&quot;) data(mulvey2015norm) cl &lt;- kmeans(MSnbase::exprs(mulvey2015norm), centers = 16, nstart = 10, iter.max = 50) x &lt;- ms2df(mulvey2015norm, fcol = &quot;Accession&quot;) x[[&quot;cluster&quot;]] &lt;- cl$cluster tb &lt;- gather(x, key = sample, value = expression, -cluster, -Accession) %&gt;% as_tibble ## Check dimensions stopifnot(nrow(tb) == prod(dim(mulvey2015norm))) pd &lt;- pData(mulvey2015norm) tb$time &lt;- pd[tb[[&quot;sample&quot;]], &quot;times&quot;] tb$rep &lt;- pd[tb[[&quot;sample&quot;]], &quot;rep&quot;] ## Plotting kmp &lt;- ggplot(data = tb, aes(x = paste(time, rep), y = expression, group = Accession, colour = as.factor(cluster))) + geom_line() + facet_wrap(~ cluster) + theme(legend.position = &quot;none&quot;) + scale_x_discrete(&quot;Time course&quot;) kmp2 &lt;- kmp + stat_summary(aes(group = cluster), fun.y = mean, geom = &quot;line&quot;, colour = &quot;black&quot;) 5.7 Hierarchical clustering How does hierarchical clustering work Initialisation: Starts by assigning each of the n points its own cluster Iteration Find the two nearest clusters, and join them together, leading to n-1 clusters Continue the cluster merging process until all are grouped into a single cluster Termination: All observations are grouped within a single cluster. Figure 5.5: Hierarchical clustering: initialisation (left) and colour-coded results after iteration (right). The results of hierarchical clustering are typically visualised along a dendrogram, where the distance between the clusters is proportional to the branch lengths. Figure 5.6: Visualisation of the hierarchical clustering results on a dendrogram In R: Calculate the distance using dist, typically the Euclidean distance. Hierarchical clustering on this distance matrix using hclust Challenge Apply hierarchical clustering on the iris data and generate a dendrogram using the dedicated plot method. d &lt;- dist(iris[, 1:4]) hcl &lt;- hclust(d) hcl ## ## Call: ## hclust(d = d) ## ## Cluster method : complete ## Distance : euclidean ## Number of objects: 150 plot(hcl) Defining clusters After producing the hierarchical clustering result, we need to cut the tree (dendrogram) at a specific height to defined the clusters. For example, on our test dataset above, we could decide to cut it at a distance around 1.5, with would produce 2 clusters. Figure 5.7: Cutting the dendrogram at height 1.5. In R we can us the cutree function to cut the tree at a specific height: cutree(hcl, h = 1.5) cut the tree to get a certain number of clusters: cutree(hcl, k = 2) Challenge Cut the iris hierarchical clustering result at a height to obtain 3 clusters by setting h. Cut the iris hierarchical clustering result at a height to obtain 3 clusters by setting directly k, and verify that both provide the same results. plot(hcl) abline(h = 3.9, col = &quot;red&quot;) cutree(hcl, k = 3) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 2 3 2 3 2 3 3 3 3 2 3 2 3 3 2 3 ## [71] 2 3 2 2 2 2 2 2 2 3 3 3 3 2 3 2 2 2 3 3 3 2 3 3 3 3 3 2 3 3 ## [ reached getOption(&quot;max.print&quot;) -- omitted 50 entries ] cutree(hcl, h = 3.9) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 2 3 2 3 2 3 3 3 3 2 3 2 3 3 2 3 ## [71] 2 3 2 2 2 2 2 2 2 3 3 3 3 2 3 2 2 2 3 3 3 2 3 3 3 3 3 2 3 3 ## [ reached getOption(&quot;max.print&quot;) -- omitted 50 entries ] identical(cutree(hcl, k = 3), cutree(hcl, h = 3.9)) ## [1] TRUE Challenge Using the same mulvey2015norm dataset, generate a hierarchical cluster of samples (the dendrogram will have 18 leafs), as shown below. The cut it to obtain early, late and fully developed groups. ## rep1_0hr rep1_16hr rep1_24hr rep1_48hr rep1_72hr rep1_XEN rep2_0hr ## 1 1 1 2 2 3 1 ## rep2_16hr rep2_24hr rep2_48hr rep2_72hr rep2_XEN rep3_0hr rep3_16hr ## 1 1 2 2 3 1 1 ## rep3_24hr rep3_48hr rep3_72hr rep3_XEN ## 1 2 2 3 5.8 Pre-processing Many of the machine learning methods that are regularly used are sensitive to difference scales. This applies to unsupervised methods as well as supervised methods, as we will see in the next chapter. A typical way to pre-process the data prior to learning is to scale the data, or apply principal component analysis (next section). Scaling assures that all data columns have a mean of 0 and standard deviation of 1. In R, scaling is done with the scale function. Example: Using the mtcars data as an example, verify that the variables are of different scales, then scale the data. To observe the effect different scales, compare the hierarchical clusters obtained on the original and scaled data. colMeans(mtcars) ## mpg cyl disp hp drat wt ## 20.090625 6.187500 230.721875 146.687500 3.596563 3.217250 ## qsec vs am gear carb ## 17.848750 0.437500 0.406250 3.687500 2.812500 hcl1 &lt;- hclust(dist(mtcars)) hcl2 &lt;- hclust(dist(scale(mtcars))) par(mfrow = c(1, 2)) plot(hcl1, main = &quot;original data&quot;) plot(hcl2, main = &quot;scaled data&quot;) 5.9 Principal component analysis (PCA) Dimensionality reduction techniques are widely used and versatile techniques that can be used to: find structure in features pre-processing for other ML algorithms, and aid in visualisation. The basic principle of dimensionality reduction techniques is to transform the data into a new space that summarise properties of the whole data set along a reduced number of dimensions. These are then ideal candidates used to visualise the data along these reduced number of informative dimensions. How does it work Principal Component Analysis (PCA) is a technique that transforms the original n-dimensional data into a new n-dimensional space. These new dimensions are linear combinations of the original data, i.e. they are composed of proportions of the original variables. Along these new dimensions, called principal components, the data expresses most of its variability along the first PC, then second, … Principal components are orthogonal to each other, i.e. non-correlated. Figure 5.8: Original data (left). PC1 will maximise the variability while minimising the residuals (centre). PC2 is orthogonal to PC1 (right). In R, we can use the prcomp function. Let’s explore PCA on the iris data. While it contains only 4 variables, is already becomes difficult to visualise the 3 groups along all these dimensions. pairs(iris[, -5], col = iris[, 5], pch = 19) Let’s use PCA to reduce the dimension. irispca &lt;- prcomp(iris[, -5]) summary(irispca) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 2.0563 0.49262 0.2797 0.15439 ## Proportion of Variance 0.9246 0.05307 0.0171 0.00521 ## Cumulative Proportion 0.9246 0.97769 0.9948 1.00000 A summary of the prcomp output shows that along PC1 along, we are able to retain over 92% of the total variability in the data. Figure 5.9: Iris data along PC1. Visualisation A biplot features all original points re-mapped (rotated) along the first two PCs as well as the original features as vectors along the same PCs. Feature vectors that are in the same direction in PC space are also correlated in the original data space. biplot(irispca) One important piece of information when using PCA is the proportion of variance explained along the PCs, in particular when dealing with high dimensional data, as PC1 and PC2 (that are generally used for visualisation), might only account for an insufficient proportion of variance to be relevant on their own. In the code chunk below, I extract the standard deviations from the PCA result to calculate the variances, then obtain the percentage of and cumulative variance along the PCs. var &lt;- irispca$sdev^2 (pve &lt;- var/sum(var)) ## [1] 0.924618723 0.053066483 0.017102610 0.005212184 cumsum(pve) ## [1] 0.9246187 0.9776852 0.9947878 1.0000000 Challenge Repeat the PCA analysis on the iris dataset above, reproducing the biplot and preparing a barplot of the percentage of variance explained by each PC. It is often useful to produce custom figures using the data coordinates in PCA space, which can be accessed as x in the prcomp object. Reproduce the PCA plots below, along PC1 and PC2 and PC3 and PC4 respectively. par(mfrow = c(1, 2)) plot(irispca$x[, 1:2], col = iris$Species) plot(irispca$x[, 3:4], col = iris$Species) Data pre-processing We haven’t looked at other prcomp parameters, other that the first one, x. There are two other ones that are or importance, in particular in the light of the section on pre-processing above, which are center and scale.. The former is set to TRUE by default, while the second one is set the FALSE. Example Repeat the analysis comparing the need for scaling on the mtcars dataset, but using PCA instead of hierarchical clustering. When comparing the two. par(mfrow = c(1, 2)) biplot(prcomp(mtcars, scale = FALSE), main = &quot;No scaling&quot;) ## 1 biplot(prcomp(mtcars, scale = TRUE), main = &quot;With scaling&quot;) ## 2 Without scaling, disp and hp are the features with the highest loadings along PC1 and 2 (all others are negligible), which are also those with the highest units of measurement. Scaling removes this effect. Final comments on PCA Real datasets often come with missing values. In R, these should be encoded using NA. Unfortunately, standard PCA cannot deal with missing values, and observations containing NA values will be dropped automatically. This is a viable solution only when the proportion of missing values is low. Alternatively, the NIPALS (non-linear iterative partial least squares) implementation does support missing values (see nipals::nipals). Finally, we should be careful when using categorical data in any of the unsupervised methods described above. Categories are generally represented as factors, which are encoded as integer levels, and might give the impression that a distance between levels is a relevant measure (which it is not, unless the factors are ordered). In such situations, categorical data can be dropped, or it is possible to encode categories as binary dummy variables. For example, if we have 3 categories, say A, B and C, we would create two dummy variables to encode the categories as: ## x y ## A 1 0 ## B 0 1 ## C 0 0 so that the distance between each category are approximately equal to 1. Challenge Produce the PCA plot for the ALL_bcrneg samples, and annotating the NEG and BCR/ABL samples on the plot. Do you think that the two first components offer enough resolution? pca &lt;- prcomp(t(MSnbase::exprs(ALL_bcrneg)), scale = TRUE, center = TRUE) plot(pca$x[, 1:2], col = ALL_bcrneg$mol.bio, cex = 2) legend(&quot;bottomright&quot;, legend = unique(ALL_bcrneg$mol.bio), pch = 1, col = c(&quot;black&quot;, &quot;red&quot;), bty = &quot;n&quot;) text(pca$x[, 1], pca$x[, 2], sampleNames(ALL_bcrneg), cex = 0.8) par(mfrow = c(1, 2)) barplot(pca$sdev^2/sum(pca$sdev^2), xlab=&quot;Principle component&quot;, ylab=&quot;% of variance&quot;) barplot(cumsum(pca$sdev^2/sum(pca$sdev^2) ), xlab=&quot;Principle component&quot;, ylab=&quot;Pumulative % of variance&quot;) ## Conclusion: the two first principle components are insufficient 5.10 t-Distributed Stochastic Neighbour Embedding t-Distributed Stochastic Neighbour Embedding (t-SNE) is a non-linear dimensionality reduction technique, i.e. that different regions of the data space will be subjected to different transformations. t-SNE will compress small distances, thus bringing close neighbours together, and will ignore large distances. It is particularly well suited for very high dimensional data. In R, we can use the Rtsne function from the Rtsne. Before, we however need to remove any duplicated entries in the dataset. library(&quot;Rtsne&quot;) uiris &lt;- unique(iris[, 1:5]) iristsne &lt;- Rtsne(uiris[, 1:4]) plot(iristsne$Y, col = uiris$Species) As with PCA, the data can be scaled and centred prior the running t-SNE (see the pca_center and pca_scale arguments). The algorithm is stochastic, and will produce different results at each repetition. 5.10.1 Parameter tuning t-SNE has two important parameters that can substantially influence the clustering of the data Perplexity: balances global and local aspects of the data. Iterations: number of iterations before the clustering is stopped. It is important to adapt these for different data. The figure below shows a 5032 by 20 dataset that represent protein sub-cellular localisation. Effect of different perplexity and iterations when running t-SNE As a comparison, below are the same data with PCA (left) and t-SNE (right). ![PCA and t-SNE on hyperLOPIT(https://raw.githubusercontent.com/lgatto/visualisation/master/figure/tsneex-1.png) 5.11 PCA loadings vs differential expression Let’s now compare the PCA loadings as calculated above and the p-values. As we can see below, large loadings may or may not correspond to small p-values. pca &lt;- prcomp(t(MSnbase::exprs(ALL_bcrneg)), scale = TRUE, center = TRUE) plot(pca$rotation[, 1], fData(ALL_bcrneg)$adj.P.Val, xlab = &quot;PCA Loading&quot;, ylab = &quot;Adjusted p-values&quot;) Let’s now repeat the same camparison, focusing on differentially expressed genes. table(sign &lt;- fData(ALL_bcrneg)$adj.P.Val &lt; 0.05) ## ## FALSE TRUE ## 12442 183 ALL_sign &lt;- ALL_bcrneg[sign, ] pca_sign &lt;- prcomp(t(MSnbase::exprs(ALL_sign)), center = TRUE, scale = TRUE) Below, we see that there is better separation when we focus on differentially expressed genes and better consistency between p-values and loadings. However we can’t do this in practice! plot(pca_sign$x[,1:2], col = ALL_sign$mol.biol, pch = 19) legend(&quot;bottomleft&quot;, legend = unique(ALL_sign$mol.biol), col = 1:2, pch = 19, bty = &quot;n&quot;) plot(pca_sign$rotation[, 1], fData(ALL_sign)$adj.P.Val, xlab = &quot;PCA Loading&quot;, ylab = &quot;Adjusted p-values&quot;) 5.12 Heatmaps See Key M. A tutorial in displaying mass spectrometry-based proteomic data using heat maps. BMC Bioinformatics. 2012;13 Suppl 16:S10. doi: 10.1186/1471-2105-13-S16-S10. Epub 2012 Nov 5. Review. PMID: 23176119; PMCID: PMC3489527. "],
["using-r-and-bioconductor-for-ms-based-proteomics.html", "Chapter 6 Using R and Bioconductor for MS-based proteomics", " Chapter 6 Using R and Bioconductor for MS-based proteomics To be based on second part of RforProteomics visualisation vignette and the MSnbase vignettes. "],
["interactive-visualisation.html", "Chapter 7 Interactive visualisation", " Chapter 7 Interactive visualisation Based on this material and the RforProteomics::shinyMA() app. "],
["session-information.html", "Chapter 8 Session information", " Chapter 8 Session information The session information below documents the packages and versions used to generate this material. sessionInfo() ## R Under development (unstable) (2018-04-02 r74505) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 14.04.5 LTS ## ## Matrix products: default ## BLAS: /usr/lib/atlas-base/atlas/libblas.so.3.0 ## LAPACK: /usr/lib/lapack/liblapack.so.3.0 ## ## locale: ## [1] LC_CTYPE=en_GB.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_GB.UTF-8 LC_COLLATE=en_GB.UTF-8 ## [5] LC_MONETARY=en_GB.UTF-8 LC_MESSAGES=en_GB.UTF-8 ## [7] LC_PAPER=en_GB.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats4 parallel stats graphics grDevices utils datasets ## [8] methods base ## ## other attached packages: ## [1] Rtsne_0.13 bindrcpp_0.2.2.9000 forcats_0.3.0 ## [4] stringr_1.3.0 dplyr_0.7.4.9003 purrr_0.2.4 ## [7] tibble_1.4.2 tidyverse_1.2.1 knitr_1.20 ## [10] UpSetR_1.3.3 Vennerable_3.1.0.9000 magrittr_1.5 ## [13] tidyr_0.8.0 readr_1.1.1 MSstatsBioData_1.1.0 ## [16] ALL_1.21.0 pRoloc_1.19.4 MLInterfaces_1.59.1 ## [19] cluster_2.0.7 annotate_1.57.3 XML_3.98-1.10 ## [22] AnnotationDbi_1.41.4 IRanges_2.13.28 S4Vectors_0.17.41 ## [25] pRolocdata_1.17.3 affy_1.57.0 MSnbase_2.5.14 ## [28] ProtGenerics_1.11.0 BiocParallel_1.13.3 mzR_2.13.6 ## [31] Rcpp_0.12.16 Biobase_2.39.2 BiocGenerics_0.25.3 ## [34] msdata_0.19.4 ggplot2_2.2.1 BiocStyle_2.7.8 ## [37] limma_3.35.14 ## ## loaded via a namespace (and not attached): ## [1] utf8_1.1.3 tidyselect_0.2.4 RSQLite_2.1.0 ## [4] htmlwidgets_1.0 grid_3.6.0 trimcluster_0.1-2 ## [7] lpSolve_5.6.13 rda_1.0.2-2 munsell_0.4.3 ## [10] codetools_0.2-15 preprocessCore_1.41.0 withr_2.1.2 ## [13] colorspace_1.3-2 BiocInstaller_1.29.6 highr_0.6 ## [16] rstudioapi_0.7 robustbase_0.92-8 dimRed_0.1.0 ## [19] mzID_1.17.0 labeling_0.3 mnormt_1.5-5 ## [22] hwriter_1.3.2 bit64_0.9-7 ggvis_0.4.3 ## [25] rprojroot_1.3-2 ipred_0.9-6 xfun_0.1 ## [28] randomForest_4.6-14 diptest_0.75-7 R6_2.2.2 ## [31] doParallel_1.0.11 flexmix_2.3-14 DRR_0.0.3 ## [34] bitops_1.0-6 assertthat_0.2.0 scales_0.5.0 ## [37] nnet_7.3-12 gtable_0.2.0 ddalpha_1.3.1.1 ## [40] timeDate_3043.102 rlang_0.2.0 CVST_0.2-1 ## [43] genefilter_1.61.1 RcppRoll_0.2.2 splines_3.6.0 ## [46] lazyeval_0.2.1 ModelMetrics_1.1.0 impute_1.53.0 ## [49] hexbin_1.27.2 broom_0.4.4 yaml_2.1.18 ## [52] reshape2_1.4.3 modelr_0.1.1 threejs_0.3.1 ## [55] crosstalk_1.0.0 backports_1.1.2 httpuv_1.3.6.2 ## [58] RBGL_1.55.1 caret_6.0-78 tools_3.6.0 ## [61] lava_1.6.1 bookdown_0.7 psych_1.8.3.3 ## [64] affyio_1.49.2 RColorBrewer_1.1-2 proxy_0.4-22 ## [67] plyr_1.8.4 base64enc_0.1-3 progress_1.1.2 ## [70] zlibbioc_1.25.0 RCurl_1.95-4.10 prettyunits_1.0.2 ## [73] rpart_4.1-13 viridis_0.5.1 sampling_2.8 ## [76] sfsmisc_1.1-2 haven_1.1.1 pcaMethods_1.71.0 ## [79] mvtnorm_1.0-7 whisker_0.3-2 hms_0.4.2 ## [82] mime_0.5 evaluate_0.10.1 xtable_1.8-2 ## [85] readxl_1.0.0 mclust_5.4 gridExtra_2.3 ## [88] compiler_3.6.0 biomaRt_2.35.13 KernSmooth_2.23-15 ## [91] crayon_1.3.4 htmltools_0.3.6 lubridate_1.7.4 ## [94] DBI_0.8 MASS_7.3-49 fpc_2.1-11 ## [97] Matrix_1.2-13 cli_1.0.0 vsn_3.47.1 ## [100] gdata_2.18.0 ## [ reached getOption(&quot;max.print&quot;) -- omitted 42 entries ] "]
]
