[
["index.html", "Visualisation of biomolecular data Chapter 1 Introduction 1.1 Installation 1.2 Questions 1.3 License 1.4 Useful references", " Visualisation of biomolecular data Laurent Gatto Last update: Mon May 7 02:37:36 2018 Chapter 1 Introduction This Visualisation of biomolecular data course was set up as part of the 2018 edition of the May Institute Computation and statistics for mass spectrometry and proteomics at Northeastern University, Boston MA. It is aimed at people who are already familiar with the R language and syntax, and who would like to get a hands-on introduction to visualisation, with a focus on biomolecular data in general, and proteomics in particular. This course is meant to be mostly hands-on, with an intuitive understanding of the underlying techniques. Let’s use the famous Anscombe’s quartet data as a motivating example. This data is composed of 4 pairs of values, \\((x_1, y_1)\\) to \\((x_4, y_4)\\): x1 x2 x3 x4 y1 y2 y3 y4 10 10 10 8 8.04 9.14 7.46 6.58 8 8 8 8 6.95 8.14 6.77 5.76 13 13 13 8 7.58 8.74 12.74 7.71 9 9 9 8 8.81 8.77 7.11 8.84 11 11 11 8 8.33 9.26 7.81 8.47 14 14 14 8 9.96 8.10 8.84 7.04 6 6 6 8 7.24 6.13 6.08 5.25 4 4 4 19 4.26 3.10 5.39 12.50 12 12 12 8 10.84 9.13 8.15 5.56 7 7 7 8 4.82 7.26 6.42 7.91 5 5 5 8 5.68 4.74 5.73 6.89 Each of these \\(x\\) and \\(y\\) sets have the same variance, mean and correlation: 1 2 3 4 var(x) 11.0000000 11.0000000 11.0000000 11.0000000 mean(x) 9.0000000 9.0000000 9.0000000 9.0000000 var(y) 4.1272691 4.1276291 4.1226200 4.1232491 mean(y) 7.5009091 7.5009091 7.5000000 7.5009091 cor(x,y) 0.8164205 0.8162365 0.8162867 0.8165214 But… While the residuals of the linear regression clearly indicate fundamental differences in these data, the most simple and straightforward approach is visualisation to highlight the fundamental differences in the datasets. See also another, more recent example: The Datasaurus Dozen dataset. We will focus on producing visualisation that will enable understanding important features of biomolecular data or the effect of their transformation and analyses. With this in mind, the beauty of the visualisations we will produce won’t be assessed by how visually attractive the figures are, but how they help us understand the data. In the process of data exploration and data analysis, we want to be able to quickly generate and interpret figures, to progress in our understanding. It is of course important to fine tune the graphics to make them visually appealing and improve communication with the audience, but we won’t focus on these aspects here. An important aspect of data visualisation is data manipulation, transformation and the format of the data. These topics will be introduced and documented throughout the course. A last feature of data that is intimately associated with its manipulation and visualisation is its structure. Some visualisations will be much easier if the data is formatted in a certain way. As such, we will describe different formats (long, wide) and data structures (dataframes and more specialised data structure). The source code for this document is available on GitHub at https://github.com/lgatto/VisualisingBiomolecularData A short URL for this book is http://bit.ly/biomolvis 1.1 Installation To install all dependencies, you can run the installation script by running the following line in R: source(&quot;http://bit.ly/biomolvis_install&quot;) A complete session information with all packages used to compile this document is available at the end. 1.2 Questions For any questions or suggestions, please open an issue. Please do add the output of your session information and, of possible, a reproducible example describing your question or suggestion. 1.3 License This material is licensed under the Creative Commons Attribution-ShareAlike 4.0 License. Some content is inspired by other sources, see the Credit section in the material. 1.4 Useful references Gatto L, Breckels LM, Naake T, Gibb S. Visualization of proteomics data using R and bioconductor. Proteomics. 2015 Apr;15(8):1375-89. doi: 10.1002/pmic.201400392. PMID: 25690415; PMCID: PMC4510819. Key M. A tutorial in displaying mass spectrometry-based proteomic data using heat maps. BMC Bioinformatics. 2012;13 Suppl 16:S10. doi: 10.1186/1471-2105-13-S16-S10. Epub 2012 Nov 5. PMID: 23176119; PMCID: PMC3489527. Gatto L, Christoforou A. Using R and Bioconductor for proteomics data analysis. Biochim Biophys Acta. 2014 Jan;1844(1 Pt A):42-51. doi: 10.1016/j.bbapap.2013.04.032. Epub 2013 May 18. PMID: 23692960. Conway JR, Lex A, Gehlenborg N. UpSetR: an R package for the visualization of intersecting sets and their properties. Bioinformatics. 2017 Sep 15;33(18):2938-2940. doi: 10.1093/bioinformatics/btx364. PMID: 28645171; PMCID: PMC5870712. "],
["example-datasets.html", "Chapter 2 Example datasets 2.1 Raw MS data 2.2 The iPRG data 2.3 CRC training data 2.4 Time course from Mulvey et al. 2015 2.5 ALL data 2.6 Spatial proteomics data", " Chapter 2 Example datasets We will used various datasets throughout the course. These data are briefly described below, and we will explore them through various visualisations later. 2.1 Raw MS data Section Using R and Bioconductor for MS-based proteomics shows how to visualise raw mass spectrometry data. The raw data that will be using come from the msdata MSnbase packages. These data will be introduced later. library(&quot;msdata&quot;) 2.2 The iPRG data This iPRG data is a spiked-in exeriment, where 6 proteins where spiked at different ratios in a Yeast proteome background. Each run was repeated in triplicates and order was randomised. Participants in the study were asked to identify the differentially abundant spiked-in proteins. Choi M, Eren-Dogu ZF, Colangelo C, Cottrell J, Hoopmann MR, Kapp EA, Kim S, Lam H, Neubert TA, Palmblad M, Phinney BS, Weintraub ST, MacLean B, Vitek O. ABRF Proteome Informatics Research Group (iPRG) 2015 Study: Detection of Differentially Abundant Proteins in Label-Free Quantitative LC-MS/MS Experiments. J Proteome Res. 2017 Feb 3;16(2):945-957. doi: 10.1021/acs.jproteome.6b00881 Epub 2017 Jan 3. PMID: 27990823. iprg &lt;- read.csv(&quot;http://bit.ly/VisBiomedDataIprgCsv&quot;) head(iprg) ## Protein Log2Intensity Run Condition ## 1 sp|D6VTK4|STE2_YEAST 26.81232 JD_06232014_sample1_B.raw Condition1 ## 2 sp|D6VTK4|STE2_YEAST 26.60786 JD_06232014_sample1_C.raw Condition1 ## 3 sp|D6VTK4|STE2_YEAST 26.58301 JD_06232014_sample1-A.raw Condition1 ## 4 sp|D6VTK4|STE2_YEAST 26.83563 JD_06232014_sample2_A.raw Condition2 ## 5 sp|D6VTK4|STE2_YEAST 26.79430 JD_06232014_sample2_B.raw Condition2 ## 6 sp|D6VTK4|STE2_YEAST 26.60863 JD_06232014_sample2_C.raw Condition2 ## BioReplicate Intensity TechReplicate ## 1 1 117845016 B ## 2 1 102273602 C ## 3 1 100526837 A ## 4 2 119765106 A ## 5 2 116382798 B ## 6 2 102328260 C dim(iprg) ## [1] 36321 7 table(iprg$Condition, iprg$TechReplicate) ## ## A B C ## Condition1 3026 3026 3027 ## Condition2 3027 3027 3027 ## Condition3 3027 3027 3027 ## Condition4 3027 3026 3027 This data is in the so-called long format. In some applications, it is more convenient to have the data in wide format, where rows contain the protein expression data for all samples. Let’s start by simplifying the data to keep only the relevant columns: head(iprg2 &lt;- iprg[, c(1, 3, 6)]) ## Protein Run Intensity ## 1 sp|D6VTK4|STE2_YEAST JD_06232014_sample1_B.raw 117845016 ## 2 sp|D6VTK4|STE2_YEAST JD_06232014_sample1_C.raw 102273602 ## 3 sp|D6VTK4|STE2_YEAST JD_06232014_sample1-A.raw 100526837 ## 4 sp|D6VTK4|STE2_YEAST JD_06232014_sample2_A.raw 119765106 ## 5 sp|D6VTK4|STE2_YEAST JD_06232014_sample2_B.raw 116382798 ## 6 sp|D6VTK4|STE2_YEAST JD_06232014_sample2_C.raw 102328260 We can convert the iPRG into a wide format with tidyr::spread: library(&quot;tidyr&quot;) iprg3 &lt;- spread(iprg2, key = Run, value = Intensity) head(iprg3) ## Protein JD_06232014_sample1-A.raw ## 1 sp|D6VTK4|STE2_YEAST 100526837 ## 2 sp|O13297|CET1_YEAST 27598550 ## 3 sp|O13329|FOB1_YEAST 11625198 ## 4 sp|O13539|THP2_YEAST 20606703 ## 5 sp|O13547|CCW14_YEAST 145493943 ## 6 sp|O13563|RPN13_YEAST 75530595 ## JD_06232014_sample1_B.raw JD_06232014_sample1_C.raw ## 1 117845016 102273602 ## 2 27618234 26774670 ## 3 10892143 16948335 ## 4 192490784 175282010 ## 5 156581624 117211277 ## 6 71664672 82193735 ## JD_06232014_sample2_A.raw JD_06232014_sample2_B.raw ## 1 119765106 116382798 ## 2 24114625 26803677 ## 3 9448487 11334921 ## 4 61781078 72052121 ## 5 151407246 126364703 ## 6 62998676 73431260 ## JD_06232014_sample2_C.raw JD_06232014_sample3_A.raw ## 1 102328260 103830944 ## 2 25055912 21977898 ## 3 14304554 19285356 ## 4 62895992 52653164 ## 5 123774524 99222081 ## 6 67612042 75916420 ## JD_06232014_sample3_B.raw JD_06232014_sample3_C.raw ## 1 94660680 96919972 ## 2 25901361 26545544 ## 3 10230925 13985282 ## 4 84856844 63400641 ## 5 113287367 129669642 ## 6 68877388 72632994 ## JD_06232014_sample4-A.raw JD_06232014_sample4_B.raw ## 1 105724288 102150172 ## 2 23860882 26305424 ## 3 8592981 9414560 ## 4 35363603 109094905 ## 5 141404829 132444723 ## 6 57600421 70708409 ## JD_06232014_sample4_C.raw ## 1 87702341 ## 2 26612349 ## 3 12783705 ## 4 63392610 ## 5 117065261 ## 6 69675572 nrow(iprg3) ## [1] 3027 Spreading data from long to wide format Indeed, we started with length(unique(iprg$Protein)) ## [1] 3027 unique proteins, which corresponds to the number of rows in the new wide dataset. The long format is ideal when using ggplot2, as we will see in a later chapter. The wide format has also advantages. For example, it becomes straighforward to verify if there are proteins that haven’t been quantified in some samples. (k &lt;- which(is.na(iprg3), arr.ind = dim(iprg3))) ## row col ## [1,] 2721 2 ## [2,] 2721 3 ## [3,] 652 12 iprg3[unique(k[, &quot;row&quot;]), ] ## Protein JD_06232014_sample1-A.raw ## 2721 sp|Q08236|AVO1_YEAST NA ## 652 sp|P28320|CWC16_YEAST 1108670 ## JD_06232014_sample1_B.raw JD_06232014_sample1_C.raw ## 2721 NA 10989286 ## 652 961273.8 1229085 ## JD_06232014_sample2_A.raw JD_06232014_sample2_B.raw ## 2721 5407419.7 2041424 ## 652 449839.1 1334009 ## JD_06232014_sample2_C.raw JD_06232014_sample3_A.raw ## 2721 13108081 4677052 ## 652 1633894 1493440 ## JD_06232014_sample3_B.raw JD_06232014_sample3_C.raw ## 2721 10308957 13639794 ## 652 2205688 2120321 ## JD_06232014_sample4-A.raw JD_06232014_sample4_B.raw ## 2721 10377088.4 2592511 ## 652 757082.7 NA ## JD_06232014_sample4_C.raw ## 2721 10420338 ## 652 1264712 The opposite operation to spread is gather, also from the tidyr package: head(iprg4 &lt;- gather(iprg3, key = Run, value = Intensity, -Protein)) ## Protein Run Intensity ## 1 sp|D6VTK4|STE2_YEAST JD_06232014_sample1-A.raw 100526837 ## 2 sp|O13297|CET1_YEAST JD_06232014_sample1-A.raw 27598550 ## 3 sp|O13329|FOB1_YEAST JD_06232014_sample1-A.raw 11625198 ## 4 sp|O13539|THP2_YEAST JD_06232014_sample1-A.raw 20606703 ## 5 sp|O13547|CCW14_YEAST JD_06232014_sample1-A.raw 145493943 ## 6 sp|O13563|RPN13_YEAST JD_06232014_sample1-A.raw 75530595 Gathering data from wide to long format The two lond datasets, iprg2 and iprg4 are different due to the missing values shown above. nrow(iprg2) ## [1] 36321 nrow(iprg4) ## [1] 36324 nrow(na.omit(iprg4)) ## [1] 36321 which can be accounted for by removing rows with missing values by setting na.rm = TRUE. head(iprg5 &lt;- gather(iprg3, key = Run, value = Intensity, -Protein, na.rm = TRUE)) ## Protein Run Intensity ## 1 sp|D6VTK4|STE2_YEAST JD_06232014_sample1-A.raw 100526837 ## 2 sp|O13297|CET1_YEAST JD_06232014_sample1-A.raw 27598550 ## 3 sp|O13329|FOB1_YEAST JD_06232014_sample1-A.raw 11625198 ## 4 sp|O13539|THP2_YEAST JD_06232014_sample1-A.raw 20606703 ## 5 sp|O13547|CCW14_YEAST JD_06232014_sample1-A.raw 145493943 ## 6 sp|O13563|RPN13_YEAST JD_06232014_sample1-A.raw 75530595 2.3 CRC training data This dataset comes from the MSstatsBioData package and was generated as follows: library(&quot;MSstats&quot;) library(&quot;MSstatsBioData&quot;) data(SRM_crc_training) Quant &lt;- dataProcess(SRM_crc_training) subjectQuant &lt;- quantification(Quant) It provides quantitative information for 72 proteins, including two standard proteins, AIAG-Bovine and FETUA-Bovine. These proteins were targeted for plasma samples with SRM with isotope labeled reference peptides in order to identify candidate protein biomarker for non-invasive detection of CRC. The training cohort included 100 subjects in control group and 100 subjects with CRC. Each sample for subject was measured in a single injection without technical replicate. The training cohort was analyzed with Skyline. The dataset was already normalized as described in manuscript. User do not need extra normalization. NAs should be considered as censored missing. Two standard proteins can be removed for statistical analysis. Clinical information where added manually thereafter. To load this dataset: crcdf &lt;- read.csv(&quot;http://bit.ly/VisBiomedDataCrcCsv&quot;) crcdf[1:10, 1:3] ## A1AG2 AFM AHSG ## 1 14.23816 16.10302 19.95179 ## 2 15.02411 16.02071 19.71592 ## 3 15.63136 16.14380 19.71085 ## 4 15.40137 16.27642 19.70438 ## 5 16.00316 16.95821 20.42033 ## 6 13.93242 16.52772 19.88985 ## 7 14.34155 16.43029 20.10060 ## 8 13.57086 17.11052 19.93833 ## 9 15.83348 15.88189 18.64270 ## 10 15.37996 16.22621 20.09992 This dataset is in the wide format. It contains the intensity of the proteins in columns 1 to 72 for each of the 200 samples along the rows. Generally, omics datasets contain the features (proteins, transcripts, …) along the rows and the samples along the columns. In columns 73 to 79, we sample metadata. crcdf[1:10, 73:79] ## Sample Group Age Gender Cancer_stage Tumour_location Sub_group ## 1 P1A10 CRC 60 female 1 colon CRC ## 2 P1A2 CRC 70 male 1 rectum CRC ## 3 P1A4 CRC 65 male 1 rectum CRC ## 4 P1A6 CRC 65 female 4 colon CRC ## 5 P1B12 CRC 62 female 3 colon CRC ## 6 P1B2 CRC 55 male 2 colon CRC ## 7 P1B3 CRC 61 male 2 rectum CRC ## 8 P1B6 CRC 52 male 4 rectum CRC ## 9 P1B9 CRC 89 female 4 colon CRC ## 10 P1C11 CRC 81 male 1 rectum CRC A widely used data structure for omics data follows the convention described in the figure below: An eSet-type of expression data container This typical omics data structure, as defined by the eSet class in the Bioconductor Biobase package, is represented below. It’s main features are An assay data slot containing the quantitative omics data (expression data), stored as a matrix and accessible with exprs. Features defined along the rows and samples along the columns. A sample metadata slot containing sample co-variates, stored as an annotated data.frame and accessible with pData. This data frame is stored with rows representing samples and sample covariate along the columns, and its rows match the expression data columns exactly. A feature metadata slot containing feature co-variates, stored as an annotated data.frame and accessible with fData. This dataframe’s rows match the expression data rows exactly. The coordinated nature of the high throughput data guarantees that the dimensions of the different slots will always match (i.e the columns in the expression data and then rows in the sample metadata, as well as the rows in the expression data and feature metadata) during data manipulation. The metadata slots can grow additional co-variates (columns) without affecting the other structures. Below, we show how to transform the crc dataset into an MSnSet (implementing the data structure above for quantitative proteomics data) using the readMSnSet2 function. library(&quot;MSnbase&quot;) i &lt;- 1:72 ## expression columns e &lt;- t(crcdf[, i]) ## expression data colnames(e) &lt;- 1:200 crc &lt;- readMSnSet2(data.frame(e), e = 1:200) pd &lt;- crcdf[, -i] rownames(pd) &lt;- paste0(&quot;X&quot;, rownames(pd)) pData(crc) &lt;- pd crc ## MSnSet (storageMode: lockedEnvironment) ## assayData: 72 features, 200 samples ## element names: exprs ## protocolData: none ## phenoData ## sampleNames: X1 X2 ... X200 (200 total) ## varLabels: Sample Group ... Sub_group (7 total) ## varMetadata: labelDescription ## featureData: none ## experimentData: use &#39;experimentData(object)&#39; ## Annotation: ## - - - Processing information - - - ## MSnbase version: 2.6.0 Let’s also set the sample names. sampleNames(crc) &lt;- crc$Sample To download and load the MSnSet direcly: download.file(&quot;http://bit.ly/VisBiomedDataCrcMSnSet&quot;, &quot;./data/crc.rda&quot;) load(&quot;./data/crc.rda&quot;) crc ## MSnSet (storageMode: lockedEnvironment) ## assayData: 72 features, 200 samples ## element names: exprs ## protocolData: none ## phenoData ## sampleNames: P1A10 P1A2 ... P3H6 (200 total) ## varLabels: Sample Group ... Sub_group (7 total) ## varMetadata: labelDescription ## featureData: none ## experimentData: use &#39;experimentData(object)&#39; ## Annotation: ## - - - Processing information - - - ## MSnbase version: 2.6.0 Reference: See Surinova, S. et al. (2015) Prediction of colorectal cancer diagnosis based on circulating plasma proteins. EMBO Mol. Med., 7, 1166–1178 for details. 2.4 Time course from Mulvey et al. 2015 This data comes from Mulvey CM, Schröter C, Gatto L, Dikicioglu D, Fidaner IB, Christoforou A, Deery MJ, Cho LT, Niakan KK, Martinez-Arias A, Lilley KS. Dynamic Proteomic Profiling of Extra-Embryonic Endoderm Differentiation in Mouse Embryonic Stem Cells. Stem Cells. 2015 Sep;33(9):2712-25. doi: 10.1002/stem.2067. Epub 2015 Jun 23. PMID: 26059426. library(&quot;pRolocdata&quot;) data(mulvey2015norm) This MSnSet, available from the pRolocdata package, measured the expression profiles of 2337 proteins along 6 time points in triplicate. MSnbase::exprs(mulvey2015norm)[1:5, 1:3] ## rep1_0hr rep1_16hr rep1_24hr ## P48432 2.479592 1.698630 1.0350877 ## Q62315-2 1.979592 1.342466 0.9181287 ## P55821 1.780612 1.767123 1.1578947 ## P17809 1.637755 1.157534 0.9941520 ## Q8K3F7 1.852041 1.623288 1.3040936 pData(mulvey2015norm) ## rep times cond ## rep1_0hr 1 1 1 ## rep1_16hr 1 2 1 ## rep1_24hr 1 3 1 ## rep1_48hr 1 4 1 ## rep1_72hr 1 5 1 ## rep1_XEN 1 6 1 ## rep2_0hr 2 1 1 ## rep2_16hr 2 2 1 ## rep2_24hr 2 3 1 ## rep2_48hr 2 4 1 ## rep2_72hr 2 5 1 ## rep2_XEN 2 6 1 ## rep3_0hr 3 1 1 ## rep3_16hr 3 2 1 ## rep3_24hr 3 3 1 ## rep3_48hr 3 4 1 ## rep3_72hr 3 5 1 ## rep3_XEN 3 6 1 2.5 ALL data library(&quot;ALL&quot;) data(ALL) ALL ## ExpressionSet (storageMode: lockedEnvironment) ## assayData: 12625 features, 128 samples ## element names: exprs ## protocolData: none ## phenoData ## sampleNames: 01005 01010 ... LAL4 (128 total) ## varLabels: cod diagnosis ... date last seen (21 total) ## varMetadata: labelDescription ## featureData: none ## experimentData: use &#39;experimentData(object)&#39; ## pubMedIds: 14684422 16243790 ## Annotation: hgu95av2 From the documentation page: The Acute Lymphoblastic Leukemia Data from the Ritz Laboratory consist of microarrays from 128 different individuals with acute lymphoblastic leukemia (ALL). A number of additional covariates are available. The data have been normalized (using rma) and it is the jointly normalized data that are available here. The ALL data is of class ExpressionSet, which implements the data structure above for microarray expression data, and contains normalised and summarised transcript intensities. Below, we select will patients with B-cell lymphomas and BCR/ABL abnormality and negative controls. table(ALL$BT) ## ## B B1 B2 B3 B4 T T1 T2 T3 T4 ## 5 19 36 23 12 5 1 15 10 2 table(ALL$mol.biol) ## ## ALL1/AF4 BCR/ABL E2A/PBX1 NEG NUP-98 p15/p16 ## 10 37 5 74 1 1 ALL_bcrneg &lt;- ALL[, ALL$mol.biol %in% c(&quot;NEG&quot;, &quot;BCR/ABL&quot;) &amp; grepl(&quot;B&quot;, ALL$BT)] ALL_bcrneg$mol.biol &lt;- factor(ALL_bcrneg$mol.biol) We then use the limma package to library(&quot;limma&quot;) design &lt;- model.matrix(~0+ALL_bcrneg$mol.biol) colnames(design) &lt;- c(&quot;BCR.ABL&quot;, &quot;NEG&quot;) ## Step1: linear model. lmFit is a wrapper around lm in R fit1 &lt;- lmFit(ALL_bcrneg, design) ## Step 2: fit contrasts: find genes that respond to estrogen contrast.matrix &lt;- makeContrasts(BCR.ABL-NEG, levels = design) fit2 &lt;- contrasts.fit(fit1, contrast.matrix) ## Step3: add empirical Bayes moderation fit3 &lt;- eBayes(fit2) ## Extract results and set them to the feature data res &lt;- topTable(fit3, n = Inf) fData(ALL_bcrneg) &lt;- res[featureNames(ALL_bcrneg), ] This annotated ExpressionSet can be reproduced as shown above or downloaded and loaded using download.file(&quot;http://bit.ly/VisBiomedDataALL_bcrneg&quot;, &quot;./data/ALL_bcrneg.rda&quot;) load(&quot;./data/ALL_bcrneg.rda&quot;) Reference: Sabina Chiaretti, Xiaochun Li, Robert Gentleman, Antonella Vitale, Marco Vignetti, Franco Mandelli, Jerome Ritz, and Robin Foa Gene expression profile of adult T-cell acute lymphocytic leukemia identifies distinct subsets of patients with different response to therapy and survival. Blood, 1 April 2004, Vol. 103, No. 7. 2.6 Spatial proteomics data The goal of spatial proteomics is to study the sub-cellular localisation of proteins. The data below comes from Christoforou A, Mulvey CM, Breckels LM, Geladaki A, Hurrell T, Hayward PC, Naake T, Gatto L, Viner R, Martinez Arias A, Lilley KS. A draft map of the mouse pluripotent stem cell spatial proteome. Nat Commun. 2016 Jan 12;7:8992. doi: 10.1038/ncomms9992. PMID: 26754106; PMCID: PMC4729960. and investigated the sub-cellular location of over 5000 proteins in mouse pluripotent stem cells using a mass spectrometry-based technique called hyperLOPIT. library(&quot;pRolocdata&quot;) data(hyperLOPIT2015) dim(hyperLOPIT2015) ## [1] 5032 20 In addition to the quantitative data, another important piece of data here are the spatial markers, proteins for which we can confidently assign them a sub-cellular location apriori. These markers are defined in the markers feature variable: table(fData(hyperLOPIT2015)$markers) ## ## 40S Ribosome ## 27 ## 60S Ribosome ## 43 ## Actin cytoskeleton ## 13 ## Cytosol ## 43 ## Endoplasmic reticulum/Golgi apparatus ## 107 ## Endosome ## 13 ## Extracellular matrix ## 13 ## Lysosome ## 33 ## Mitochondrion ## 383 ## Nucleus - Chromatin ## 64 ## Nucleus - Non-chromatin ## 85 ## Peroxisome ## 17 ## Plasma membrane ## 51 ## Proteasome ## 34 ## unknown ## 4106 Here, the columns of the data represent fractions along a density gradient used to separate the sub-cellular proteome presented in this data. We will be using a simplified version of this data in the next chapters. Below, we retain the first replicate (hyperLOPIT2015$Replicate == 1]) and the marker proteins (using the markerMSnSet function): hlm &lt;- markerMSnSet(hyperLOPIT2015[, hyperLOPIT2015$Replicate == 1]) dim(hlm) ## [1] 926 10 getMarkerClasses(hlm) ## no unknowns anymore ## [1] &quot;40S Ribosome&quot; ## [2] &quot;60S Ribosome&quot; ## [3] &quot;Actin cytoskeleton&quot; ## [4] &quot;Cytosol&quot; ## [5] &quot;Endoplasmic reticulum/Golgi apparatus&quot; ## [6] &quot;Endosome&quot; ## [7] &quot;Extracellular matrix&quot; ## [8] &quot;Lysosome&quot; ## [9] &quot;Mitochondrion&quot; ## [10] &quot;Nucleus - Chromatin&quot; ## [11] &quot;Nucleus - Non-chromatin&quot; ## [12] &quot;Peroxisome&quot; ## [13] &quot;Plasma membrane&quot; ## [14] &quot;Proteasome&quot; "],
["exploring-and-visualising-biomolecular-data.html", "Chapter 3 Exploring and visualising biomolecular data Raw data Tracking an ion from the chromatogram to the MS2 spectra Raw data QC Exploring the experimental design Quantitative data Differential expression", " Chapter 3 Exploring and visualising biomolecular data This chapter provides an overview of a typical omics data analysis, exploration of the data, statistical analysis and their associated visualisations. We will look into the respective figures and how to produce them in the later chapters. A typical proteomics data processing and analysis workflow Raw data Tracking an ion from the chromatogram to the MS2 spectra See here for details and code. Raw data QC See ?plotMzDelta for details and code. Exploring the experimental design ## ## female male ## (32.9,44.6] 3 2 ## (44.6,56.2] 19 14 ## (56.2,67.8] 34 59 ## (67.8,79.4] 28 28 ## (79.4,91.1] 10 3 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Quantitative data Differential expression "],
["using-dplyr-for-data-manipulation.html", "Chapter 4 Using dplyr for data manipulation 4.1 Selecting columns and filtering rows 4.2 Pipes 4.3 Mutate 4.4 Split-apply-combine data analysis and the summarize() function", " Chapter 4 Using dplyr for data manipulation A useful dplyr cheet sheet is available here. The following material is based on Data Carpentry’s the Data analisis and visualisation lessons. Bracket subsetting is handy, but it can be cumbersome and difficult to read, especially for complicated operations. Enter dplyr. dplyr is a package for making tabular data manipulation easier. It pairs nicely with tidyr which enables you to swiftly convert between different data formats for plotting and analysis. We will need the tidyverse package. This is an “umbrella-package” that installs several packages useful for data analysis which work together well such as dplyr, ggplot2 (for visualisation), tibble, etc. library(&quot;tidyverse&quot;) ## ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ── ## ✔ tibble 1.4.2 ✔ dplyr 0.7.4.9003 ## ✔ purrr 0.2.4 ✔ stringr 1.3.0 ## ✔ tibble 1.4.2 ✔ forcats 0.3.0 ## ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::collapse() masks IRanges::collapse() ## ✖ dplyr::combine() masks MSnbase::combine(), Biobase::combine(), BiocGenerics::combine() ## ✖ dplyr::desc() masks IRanges::desc() ## ✖ purrr::detect() masks RforProteomics::detect() ## ✖ tidyr::expand() masks S4Vectors::expand() ## ✖ dplyr::exprs() masks affy::exprs(), MSnbase::exprs(), ggplot2::exprs(), Biobase::exprs() ## ✖ magrittr::extract() masks tidyr::extract() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::first() masks S4Vectors::first() ## ✖ dplyr::ident() masks msdata::ident() ## ✖ dplyr::lag() masks stats::lag() ## ✖ ggplot2::Position() masks BiocGenerics::Position(), base::Position() ## ✖ purrr::reduce() masks IRanges::reduce(), MSnbase::reduce() ## ✖ dplyr::rename() masks S4Vectors::rename() ## ✖ dplyr::select() masks AnnotationDbi::select() ## ✖ purrr::set_names() masks magrittr::set_names() ## ✖ dplyr::slice() masks IRanges::slice() ## ✖ dplyr::vars() masks ggplot2::vars() The tidyverse package tries to address 3 major problems with some of base R functions: The results from a base R function sometimes depends on the type of data. Using R expressions in a non standard way, which can be confusing for new learners. Hidden arguments, having default operations that new learners are not aware of. To learn more about dplyr and tidyr, you may want to check out this handy data transformation with dplyr cheatsheet and this one about tidyr. Let’s start by reading the data using readr::read_csv that will produce a tibble. library(&quot;readr&quot;) iprg &lt;- read_csv(&quot;http://bit.ly/VisBiomedDataIprgCsv&quot;) ## Parsed with column specification: ## cols( ## Protein = col_character(), ## Log2Intensity = col_double(), ## Run = col_character(), ## Condition = col_character(), ## BioReplicate = col_integer(), ## Intensity = col_double(), ## TechReplicate = col_character() ## ) Tibbles are data frames, but they tweak some of the old behaviors of data frames. The data structure is very similar to a data frame. For our purposes the only differences are that: In addition to displaying the data type of each column under its name, it only prints the first few rows of data and only as many columns as fit on one screen. Columns of class character are never converted into factors. 4.1 Selecting columns and filtering rows We’re going to learn some of the most common dplyr functions: select(), filter(), mutate(), group_by(), and summarize(). To select columns of a data frame, use select(). The first argument to this function is the data frame, and the subsequent arguments are the columns to keep. select(iprg, Protein, Run, Condition) To choose rows based on a specific criteria, use filter(): filter(iprg, BioReplicate == 1) ## # A tibble: 9,079 x 7 ## Protein Log2Intensity Run Condition BioReplicate Intensity ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 1 1.18e8 ## 2 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 1 1.02e8 ## 3 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 1 1.01e8 ## 4 sp|O13297|C… 24.7 JD_0623201… Conditio… 1 2.76e7 ## 5 sp|O13297|C… 24.7 JD_0623201… Conditio… 1 2.68e7 ## 6 sp|O13297|C… 24.7 JD_0623201… Conditio… 1 2.76e7 ## 7 sp|O13329|F… 23.4 JD_0623201… Conditio… 1 1.09e7 ## 8 sp|O13329|F… 24.0 JD_0623201… Conditio… 1 1.69e7 ## 9 sp|O13329|F… 23.5 JD_0623201… Conditio… 1 1.16e7 ## 10 sp|O13539|T… 27.5 JD_0623201… Conditio… 1 1.92e8 ## # ... with 9,069 more rows, and 1 more variable: TechReplicate &lt;chr&gt; filter(iprg, Condition == &#39;Condition2&#39;) ## # A tibble: 9,081 x 7 ## Protein Log2Intensity Run Condition BioReplicate Intensity ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 2 1.20e8 ## 2 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 2 1.16e8 ## 3 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 2 1.02e8 ## 4 sp|O13297|C… 24.5 JD_0623201… Conditio… 2 2.41e7 ## 5 sp|O13297|C… 24.7 JD_0623201… Conditio… 2 2.68e7 ## 6 sp|O13297|C… 24.6 JD_0623201… Conditio… 2 2.51e7 ## 7 sp|O13329|F… 23.2 JD_0623201… Conditio… 2 9.45e6 ## 8 sp|O13329|F… 23.4 JD_0623201… Conditio… 2 1.13e7 ## 9 sp|O13329|F… 23.8 JD_0623201… Conditio… 2 1.43e7 ## 10 sp|O13539|T… 25.9 JD_0623201… Conditio… 2 6.18e7 ## # ... with 9,071 more rows, and 1 more variable: TechReplicate &lt;chr&gt; 4.2 Pipes But what if you wanted to select and filter at the same time? There are three ways to do this: use intermediate steps, nested functions, or pipes. With intermediate steps, you essentially create a temporary data frame and use that as input to the next function. This can clutter up your workspace with lots of objects. You can also nest functions (i.e. one function inside of another). This is handy, but can be difficult to read if too many functions are nested as things are evaluated from the inside out. The last option, pipes, are a fairly recent addition to R. Pipes let you take the output of one function and send it directly to the next, which is useful when you need to do many things to the same dataset. Pipes in R look like %&gt;% and are made available via the magrittr package, installed automatically with dplyr. If you use RStudio, you can type the pipe with Ctrl + Shift + M if you have a PC or Cmd + Shift + M if you have a Mac. iprg %&gt;% filter(Intensity &gt; 1e8) %&gt;% select(Protein, Condition, Intensity) ## # A tibble: 4,729 x 3 ## Protein Condition Intensity ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 sp|D6VTK4|STE2_YEAST Condition1 117845016. ## 2 sp|D6VTK4|STE2_YEAST Condition1 102273602. ## 3 sp|D6VTK4|STE2_YEAST Condition1 100526837. ## 4 sp|D6VTK4|STE2_YEAST Condition2 119765106. ## 5 sp|D6VTK4|STE2_YEAST Condition2 116382798. ## 6 sp|D6VTK4|STE2_YEAST Condition2 102328260. ## 7 sp|D6VTK4|STE2_YEAST Condition3 103830944. ## 8 sp|D6VTK4|STE2_YEAST Condition4 102150172. ## 9 sp|D6VTK4|STE2_YEAST Condition4 105724288. ## 10 sp|O13539|THP2_YEAST Condition1 192490784. ## # ... with 4,719 more rows In the above, we use the pipe to send the iprg dataset first through filter() to keep rows where Intensity is greater than 1e8, then through select() to keep only the Protein, Condition, and Intensity columns. Since %&gt;% takes the object on its left and passes it as the first argument to the function on its right, we don’t need to explicitly include it as an argument to the filter() and select() functions anymore. If we wanted to create a new object with this smaller version of the data, we could do so by assigning it a new name: iprg_sml &lt;- iprg %&gt;% filter(Intensity &gt; 1e8) %&gt;% select(Protein, Condition, Intensity) iprg_sml ## # A tibble: 4,729 x 3 ## Protein Condition Intensity ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 sp|D6VTK4|STE2_YEAST Condition1 117845016. ## 2 sp|D6VTK4|STE2_YEAST Condition1 102273602. ## 3 sp|D6VTK4|STE2_YEAST Condition1 100526837. ## 4 sp|D6VTK4|STE2_YEAST Condition2 119765106. ## 5 sp|D6VTK4|STE2_YEAST Condition2 116382798. ## 6 sp|D6VTK4|STE2_YEAST Condition2 102328260. ## 7 sp|D6VTK4|STE2_YEAST Condition3 103830944. ## 8 sp|D6VTK4|STE2_YEAST Condition4 102150172. ## 9 sp|D6VTK4|STE2_YEAST Condition4 105724288. ## 10 sp|O13539|THP2_YEAST Condition1 192490784. ## # ... with 4,719 more rows Note that the final data frame is the leftmost part of this expression. Challenge Using pipes, subset the iprg data to include Proteins with a log2 intensity greater than 20 and retain only the columns Proteins, and Condition. ## Answer iprg %&gt;% filter(Log2Intensity &gt; 20) %&gt;% select(Protein, Condition) 4.3 Mutate Frequently you’ll want to create new columns based on the values in existing columns, for example to do unit conversions, or find the ratio of values in two columns. For this we’ll use mutate(). To create a new column of weight in kg: iprg %&gt;% mutate(Log10Intensity = log10(Intensity)) ## # A tibble: 36,321 x 8 ## Protein Log2Intensity Run Condition BioReplicate Intensity ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 1 1.18e8 ## 2 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 1 1.02e8 ## 3 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 1 1.01e8 ## 4 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 2 1.20e8 ## 5 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 2 1.16e8 ## 6 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 2 1.02e8 ## 7 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 3 1.04e8 ## 8 sp|D6VTK4|S… 26.5 JD_0623201… Conditio… 3 9.47e7 ## 9 sp|D6VTK4|S… 26.5 JD_0623201… Conditio… 3 9.69e7 ## 10 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 4 1.02e8 ## # ... with 36,311 more rows, and 2 more variables: TechReplicate &lt;chr&gt;, ## # Log10Intensity &lt;dbl&gt; You can also create a second new column based on the first new column within the same call of mutate(): iprg %&gt;% mutate(Log10Intensity = log10(Intensity), Log10Intensity2 = Log10Intensity * 2) ## # A tibble: 36,321 x 9 ## Protein Log2Intensity Run Condition BioReplicate Intensity ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 1 1.18e8 ## 2 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 1 1.02e8 ## 3 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 1 1.01e8 ## 4 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 2 1.20e8 ## 5 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 2 1.16e8 ## 6 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 2 1.02e8 ## 7 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 3 1.04e8 ## 8 sp|D6VTK4|S… 26.5 JD_0623201… Conditio… 3 9.47e7 ## 9 sp|D6VTK4|S… 26.5 JD_0623201… Conditio… 3 9.69e7 ## 10 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 4 1.02e8 ## # ... with 36,311 more rows, and 3 more variables: TechReplicate &lt;chr&gt;, ## # Log10Intensity &lt;dbl&gt;, Log10Intensity2 &lt;dbl&gt; If this runs off your screen and you just want to see the first few rows, you can use a pipe to view the head() of the data. (Pipes work with non-dplyr functions, too, as long as the dplyr or magrittr package is loaded). iprg %&gt;% mutate(Log10Intensity = log10(Intensity)) %&gt;% head ## # A tibble: 6 x 8 ## Protein Log2Intensity Run Condition BioReplicate Intensity ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 sp|D6VTK4|S… 26.8 JD_06232014… Conditio… 1 1.18e8 ## 2 sp|D6VTK4|S… 26.6 JD_06232014… Conditio… 1 1.02e8 ## 3 sp|D6VTK4|S… 26.6 JD_06232014… Conditio… 1 1.01e8 ## 4 sp|D6VTK4|S… 26.8 JD_06232014… Conditio… 2 1.20e8 ## 5 sp|D6VTK4|S… 26.8 JD_06232014… Conditio… 2 1.16e8 ## 6 sp|D6VTK4|S… 26.6 JD_06232014… Conditio… 2 1.02e8 ## # ... with 2 more variables: TechReplicate &lt;chr&gt;, Log10Intensity &lt;dbl&gt; Note that we don’t include parentheses at the end of our call to head() above. When piping into a function with no additional arguments, you can call the function with or without parentheses (e.g. head or head()). If you want to display more data, you can use the print() function at the end of your chain with the argument n specifying the number of rows to display: iprg %&gt;% mutate(Log10Intensity = log10(Intensity), Log10Intensity2 = Log10Intensity * 2) %&gt;% print(n = 20) ## # A tibble: 36,321 x 9 ## Protein Log2Intensity Run Condition BioReplicate Intensity ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 1 1.18e8 ## 2 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 1 1.02e8 ## 3 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 1 1.01e8 ## 4 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 2 1.20e8 ## 5 sp|D6VTK4|S… 26.8 JD_0623201… Conditio… 2 1.16e8 ## 6 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 2 1.02e8 ## 7 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 3 1.04e8 ## 8 sp|D6VTK4|S… 26.5 JD_0623201… Conditio… 3 9.47e7 ## 9 sp|D6VTK4|S… 26.5 JD_0623201… Conditio… 3 9.69e7 ## 10 sp|D6VTK4|S… 26.6 JD_0623201… Conditio… 4 1.02e8 ## 11 sp|D6VTK4|S… 26.4 JD_0623201… Conditio… 4 8.77e7 ## 12 sp|D6VTK4|S… 26.7 JD_0623201… Conditio… 4 1.06e8 ## 13 sp|O13297|C… 24.7 JD_0623201… Conditio… 1 2.76e7 ## 14 sp|O13297|C… 24.7 JD_0623201… Conditio… 1 2.68e7 ## 15 sp|O13297|C… 24.7 JD_0623201… Conditio… 1 2.76e7 ## 16 sp|O13297|C… 24.5 JD_0623201… Conditio… 2 2.41e7 ## 17 sp|O13297|C… 24.7 JD_0623201… Conditio… 2 2.68e7 ## 18 sp|O13297|C… 24.6 JD_0623201… Conditio… 2 2.51e7 ## 19 sp|O13297|C… 24.4 JD_0623201… Conditio… 3 2.20e7 ## 20 sp|O13297|C… 24.6 JD_0623201… Conditio… 3 2.59e7 ## # ... with 3.63e+04 more rows, and 3 more variables: TechReplicate &lt;chr&gt;, ## # Log10Intensity &lt;dbl&gt;, Log10Intensity2 &lt;dbl&gt; Let’s use a modified iprg data that contains missing values for the next example. It can be loaded with if (!file.exists(&quot;./data/iprgna.rda&quot;)) download.file(&quot;http://bit.ly/VisBiomedDataIprgNA&quot;, &quot;./data/iprgna.rda&quot;) load(&quot;./data/iprgna.rda&quot;) Challenge Using the iprgna data repeat the creation of a new Log10Intensisty column. iprgna %&gt;% mutate(Log10Intensity = log10(Intensity)) ## # A tibble: 36,321 x 7 ## Protein Run Condition BioReplicate Intensity TechReplicate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 sp|D6VTK4|S… JD_0623201… Conditio… 1 NA B ## 2 sp|D6VTK4|S… JD_0623201… Conditio… 1 1.02e8 C ## 3 sp|D6VTK4|S… JD_0623201… Conditio… 1 NA A ## 4 sp|D6VTK4|S… JD_0623201… Conditio… 2 1.20e8 A ## 5 sp|D6VTK4|S… JD_0623201… Conditio… 2 NA B ## 6 sp|D6VTK4|S… JD_0623201… Conditio… 2 1.02e8 C ## 7 sp|D6VTK4|S… JD_0623201… Conditio… 3 1.04e8 A ## 8 sp|D6VTK4|S… JD_0623201… Conditio… 3 9.47e7 B ## 9 sp|D6VTK4|S… JD_0623201… Conditio… 3 9.69e7 C ## 10 sp|D6VTK4|S… JD_0623201… Conditio… 4 1.02e8 B ## # ... with 36,311 more rows, and 1 more variable: Log10Intensity &lt;dbl&gt; The first few rows of the output are full of NAs, so if we wanted to remove those we could insert a filter() in the chain: iprgna %&gt;% filter(!is.na(Intensity)) %&gt;% mutate(Log10Intensity = log10(Intensity)) ## # A tibble: 35,318 x 7 ## Protein Run Condition BioReplicate Intensity TechReplicate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 sp|D6VTK4|S… JD_0623201… Conditio… 1 1.02e8 C ## 2 sp|D6VTK4|S… JD_0623201… Conditio… 2 1.20e8 A ## 3 sp|D6VTK4|S… JD_0623201… Conditio… 2 1.02e8 C ## 4 sp|D6VTK4|S… JD_0623201… Conditio… 3 1.04e8 A ## 5 sp|D6VTK4|S… JD_0623201… Conditio… 3 9.47e7 B ## 6 sp|D6VTK4|S… JD_0623201… Conditio… 3 9.69e7 C ## 7 sp|D6VTK4|S… JD_0623201… Conditio… 4 1.02e8 B ## 8 sp|D6VTK4|S… JD_0623201… Conditio… 4 8.77e7 C ## 9 sp|D6VTK4|S… JD_0623201… Conditio… 4 1.06e8 A ## 10 sp|O13297|C… JD_0623201… Conditio… 1 2.76e7 B ## # ... with 35,308 more rows, and 1 more variable: Log10Intensity &lt;dbl&gt; is.na() is a function that determines whether something is an NA. The ! symbol negates the result, so we’re asking for everything that is not an NA. 4.4 Split-apply-combine data analysis and the summarize() function Many data analysis tasks can be approached using the split-apply-combine paradigm: split the data into groups, apply some analysis to each group, and then combine the results. dplyr makes this very easy through the use of the group_by() function. The summarize() function group_by() is often used together with summarize(), which collapses each group into a single-row summary of that group. group_by() takes as arguments the column names that contain the categorical variables for which you want to calculate the summary statistics. So to view the mean weight by sex: iprgna %&gt;% group_by(Condition) %&gt;% summarize(mean_Intensity = mean(Intensity)) ## # A tibble: 4 x 2 ## Condition mean_Intensity ## &lt;chr&gt; &lt;dbl&gt; ## 1 Condition1 NA ## 2 Condition2 NA ## 3 Condition3 NA ## 4 Condition4 NA Unfortunately, the mean of any vector that contains even a single missing value is NA. We need to remove missing values before calculating the mean, which is done easily with the na.rm argument. iprgna %&gt;% group_by(Condition) %&gt;% summarize(mean_Intensity = mean(Intensity, na.rm = TRUE)) ## # A tibble: 4 x 2 ## Condition mean_Intensity ## &lt;chr&gt; &lt;dbl&gt; ## 1 Condition1 65144912. ## 2 Condition2 64439756. ## 3 Condition3 62475797. ## 4 Condition4 63616488. You can also group by multiple columns: iprgna %&gt;% group_by(TechReplicate, BioReplicate) %&gt;% summarize(mean_Intensity = mean(Intensity, na.rm = TRUE)) ## # A tibble: 12 x 3 ## # Groups: TechReplicate [?] ## TechReplicate BioReplicate mean_Intensity ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 A 1 64891444. ## 2 A 2 63870255. ## 3 A 3 61648150. ## 4 A 4 63662564. ## 5 B 1 65563938. ## 6 B 2 65164270. ## 7 B 3 62758494. ## 8 B 4 64196979. ## 9 C 1 64978764. ## 10 C 2 64283727. ## 11 C 3 63020774. ## 12 C 4 62984686. Tallying When working with data, it is also common to want to know the number of observations found for each factor or combination of factors. For this, dplyr provides tally(). iprgna %&gt;% group_by(Condition) %&gt;% tally ## # A tibble: 4 x 2 ## Condition n ## &lt;chr&gt; &lt;int&gt; ## 1 Condition1 9079 ## 2 Condition2 9081 ## 3 Condition3 9081 ## 4 Condition4 9080 Here, tally() is the action applied to the groups created by group_by() and counts the total number of records for each category. Challenge How many proteins of each technical replicate are there? Use group_by() and summarize() to find the mean, min, and max intensity for each condition. What are the proteins with the highest intensity in each condition? ## Answer 1 iprgna %&gt;% group_by(TechReplicate) %&gt;% tally ## # A tibble: 3 x 2 ## TechReplicate n ## &lt;chr&gt; &lt;int&gt; ## 1 A 12107 ## 2 B 12106 ## 3 C 12108 ## Answer 2 iprgna %&gt;% filter(!is.na(Intensity)) %&gt;% group_by(Condition) %&gt;% summarize(mean_int = mean(Intensity), min_int = min(Intensity), max_int = max(Intensity)) ## # A tibble: 4 x 4 ## Condition mean_int min_int max_int ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Condition1 65144912. 254608. 2841953257 ## 2 Condition2 64439756. 259513. 2757471311 ## 3 Condition3 62475797. 88409. 2659018724 ## 4 Condition4 63616488. 84850. 2881057105 ## Answer 3 iprgna %&gt;% filter(!is.na(Intensity)) %&gt;% group_by(Condition) %&gt;% filter(Intensity == max(Intensity)) %&gt;% arrange(Intensity) ## # A tibble: 4 x 6 ## # Groups: Condition [4] ## Protein Run Condition BioReplicate Intensity TechReplicate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 sp|P48589|R… JD_06232014… Conditio… 3 2.66e9 B ## 2 sp|P48589|R… JD_06232014… Conditio… 2 2.76e9 B ## 3 sp|P48589|R… JD_06232014… Conditio… 1 2.84e9 A ## 4 sp|P48589|R… JD_06232014… Conditio… 4 2.88e9 A "],
["plotting-in-r.html", "Chapter 5 Plotting in R 5.1 Base graphics 5.2 Plotting with ggplot2 5.3 Building complex plots 5.4 Colour scales 5.5 Customising plots 5.6 Combine ggplot figures 5.7 Saving your figures 5.8 Exercises 5.9 References", " Chapter 5 Plotting in R 5.1 Base graphics We won’t go through all base graphics plotting functions one by one here. We will encounter and learn several of these functions throughout the course and, if necessary, discuss them when questions arise. 5.2 Plotting with ggplot2 A useful ggplot2 cheet sheet is available here. More details are available on the documentation page. Base graphics uses a canvas model a series of instructions that sequentially fill the plotting canvas. While this model is very useful to build plots bits by bits bottom up, which is useful in some cases, it has some clear drawback: Layout choices have to be made without global overview over what may still be coming. Different functions for different plot types with different interfaces. No standard data input. Many routine tasks require a lot of boilerplate code. No concept of facets/lattices/viewports. Poor default colours. The ggplot2 package implements a grammar of graphics. Users describe what and how to visualise data and the package then generates the figure. The components of ggplot2’s of graphics are A tidy dataset A choice of geometric objects that servers as the visual representation of the data - for instance, points, lines, rectangles, contours. A description of how the variables in the data are mapped to visual properties (aesthetics) or the geometric objects, and an associated scale (e.g. linear, logarithmic, polar) A statistical summarisation rule A coordinate system. A facet specification, i.e. the use of several plots to look at the same data. Fist of all, we need to load the ggplot2 package and load the iprg data. library(&quot;ggplot2&quot;) iprg &lt;- read.csv(&quot;http://bit.ly/VisBiomedDataIprgCsv&quot;) ggplot graphics are built step by step by adding new elements. To build a ggplot we need to: bind the plot to a specific data frame using the data argument ggplot(data = iprg) define aesthetics (aes), by selecting the variables to be plotted and the variables to define the presentation such as plotting size, shape color, etc. ggplot(data = iprg, aes(x = Run, y = Log2Intensity)) add geoms – graphical representation of the data in the plot (points, lines, bars). To add a geom to the plot use + operator ggplot(data = iprg, aes(x = Run, y = Log2Intensity)) + geom_boxplot() See the documentation page to explore the many available geoms. The + in the ggplot2 package is particularly useful because it allows you to modify existing ggplot objects. This means you can easily set up plot “templates” and conveniently explore different types of plots, so the above plot can also be generated with code like this: ## Assign plot to a variable ints_plot &lt;- ggplot(data = iprg, aes(x = Run, y = Log2Intensity)) ## Draw the plot ints_plot + geom_boxplot() Notes: Anything you put in the ggplot() function can be seen by any geom layers that you add (i.e., these are universal plot settings). This includes the x and y axis you set up in aes(). You can also specify aesthetics for a given geom independently of the aesthetics defined globally in the ggplot() function. The + sign used to add layers must be placed at the end of each line containing a layer. If, instead, the + sign is added in the line before the other layer, ggplot2 will not add the new layer and will return an error message. Challenge Repeat the plot above but displaying the raw intensities. Log-10 transform the raw intensities on the flight when plotting. ggplot(data = iprg, aes(x = Run, y = Intensity)) + geom_boxplot() ggplot(data = iprg, aes(x = Run, y = log10(Intensity))) + geom_boxplot() 5.3 Building complex plots First, let’s colour the boxplot based on the condition: ggplot(data = iprg, aes(x = Run, y = Log2Intensity, fill = Condition)) + geom_boxplot() Now let’s rename all axis labels and title, and rotate the x-axis labels 90 degrees. We can add those specifications using the labs and theme functions of the ggplot2 package. ggplot(aes(x = Run, y = Log2Intensity, fill = Condition), data = iprg) + geom_boxplot() + labs(title = &#39;Log2 transformed intensity distribution per MS run&#39;, y = &#39;Log2(Intensity)&#39;, x = &#39;MS run&#39;) + theme(axis.text.x = element_text(angle = 90)) And easily switch from a boxplot to a violin plot representation by changing the geom type. ggplot(aes(x = Run, y = Log2Intensity, fill = Condition), data = iprg) + geom_violin() + labs(title = &#39;Log2 transformed intensity distribution per Subject&#39;, y = &#39;Log2(Intensity)&#39;, x = &#39;MS run&#39;) + theme(axis.text.x = element_text(angle = 90)) Finally, we can also overlay multiple geoms by simply adding them one after the other. p &lt;- ggplot(aes(x = Run, y = Log2Intensity, fill = Condition), data = iprg) p + geom_boxplot() p + geom_boxplot() + geom_jitter() ## not very usefull p + geom_jitter() + geom_boxplot() p + geom_jitter(alpha = 0.1) + geom_boxplot() Challenge Overlay a boxplot goem on top of a jitter geom for the raw or log-10 transformed intensities. Customise the plot as suggested above. ## Note how the log10 transformation is applied to both geoms ggplot(data = iprg, aes(x = Run, y = log10(Intensity))) + geom_jitter(alpha = 0.1) + geom_boxplot() Finally, a very useful feature of ggplot2 is facetting, that defines how to subset the data into different panels (facets). names(iprg) ## [1] &quot;Protein&quot; &quot;Log2Intensity&quot; &quot;Run&quot; &quot;Condition&quot; ## [5] &quot;BioReplicate&quot; &quot;Intensity&quot; &quot;TechReplicate&quot; ggplot(data = iprg, aes(x = TechReplicate, y = Log2Intensity, fill = Condition)) + geom_boxplot() + facet_grid(~ Condition) 5.4 Colour scales library(&quot;viridis&quot;) library(&quot;RColorBrewer&quot;) RColorBrewer::display.brewer.all() p &lt;- ggplot(data = crcdf, aes(x = A1AG2, y = AFM, colour = AHSG)) + geom_point() p + scale_color_viridis() p + scale_color_viridis(option = &quot;A&quot;) p + scale_color_distiller(palette = &quot;Spectral&quot;) p + scale_color_distiller(palette = &quot;Blues&quot;) p + scale_color_distiller(palette = &quot;Purples&quot;) p + scale_color_distiller(palette = &quot;Set1&quot;) p &lt;- ggplot(data = crcdf, aes(x = A1AG2, y = AFM, colour = Sub_group)) + geom_point() p ## uses scale_color_discrete() p + scale_color_grey() p + scale_color_viridis(discrete = TRUE) p + scale_color_viridis(discrete = TRUE, option = &quot;A&quot;) 5.5 Customising plots Using labs: p &lt;- ggplot(data = iprg, aes(x = Run, y = log10(Intensity))) + geom_jitter(alpha = 0.1) + geom_boxplot() p + labs(title = &quot;A title, at the top&quot;, subtitle = &quot;A subtitle, under the title&quot;, caption = &quot;Comes at the bottom of the plot&quot;, x = &quot;x axis label&quot;, y = &quot;y axis label&quot;) p p + theme(axis.text.x = element_text(angle = 90, hjust = 1)) Setting themes: p + theme_bw() p + theme_gray() p + theme_dark() p + theme_minimal() p + theme_light() p + theme_void() ## .... See also the ggthemes package. 5.6 Combine ggplot figures The goal of patchwork patchwork is to make it ridiculously simple to combine separate ggplots into the same graphic. As such it tries to solve the same problem as gridExtra::grid.arrange() and cowplot::plot_grid but using an API that incites exploration and iteration. Installation: ## install.packages(&quot;devtools&quot;) devtools::install_github(&quot;thomasp85/patchwork&quot;) p1 &lt;- ggplot(iprg, aes(x = Condition, y = Log2Intensity)) + geom_boxplot(aes(fill = Condition)) + theme(legend.position = &quot;none&quot;) p2 &lt;- ggplot(iprg, aes(x = Intensity, y = Log2Intensity)) + geom_point() p3 &lt;- iprg %&gt;% group_by(TechReplicate, BioReplicate) %&gt;% tally %&gt;% ggplot(aes(x = TechReplicate, y = n, fill = as.factor(BioReplicate))) + geom_col() + theme(legend.position = &quot;none&quot;) p4 &lt;- ggplot(iprg, aes(x = TechReplicate, y = Log2Intensity)) + geom_violin(aes(fill = Condition)) + theme(legend.position = &quot;none&quot;) + coord_flip() library(&quot;patchwork&quot;) p1 + p2 p1 + p2 + p3 + p4 + plot_layout(ncol = 2) p1 + p2 + p3 + plot_layout(ncol = 1) p1 + p2 - p3 + plot_layout(ncol = 1) (p1 | p2 | p3) / p4 (p1 + (p2 + p3) + p4 + plot_layout(ncol = 1)) 5.7 Saving your figures You can save plots to a number of different file formats. PDF is by far the most common format because it’s lightweight, cross-platform and scales up well but jpegs, pngs and a number of other file formats are also supported. Let’s redo the last barplot but save it to the file system this time. Let’s save the boxplot as pdf file. pdf() p + geom_jitter(alpha = 0.1) + geom_boxplot() dev.off() The default file name is Rplots.pdf. We can customise that file name specifying it by passing the file name, as a character, to the pdf() function. Challenge Save a figure of your choice to a pdf file. Read the manual for the png function and save that same image to a png file. Tip: save your figures in a dedicated directory. 5.8 Exercises Count the number of quantified proteins for each group, gender and age and visualise the results (suggestion below). Tips: For age, you can use cut to bin the ages. You’ll probably want to use dplyr::group_by and dplyr::tally to obtain the values after converting the crc data to a long format. tbl &lt;- gather(crcdf, key = Protein, value = expression, -Sample, -Group, -Age, -Gender, -Cancer_stage, -Tumour_location, -Sub_group) %&gt;% as_tibble tbl %&gt;% group_by(age = cut(tbl$Age, 5), Group, Gender) %&gt;% tally %&gt;% ggplot(aes(x = Gender, y = n, colour = age)) + geom_point(size = 5) + facet_grid( ~ Group) Experiment with goem_line to produce line plots showing the expression data for a subset of samples (below I used those starting with &quot;P1F&quot;). You will need to group proteins for the same protein data points in different samples to be linked. ## geom_path: Each group consists of only one observation. Do you need to ## adjust the group aesthetic? ggplot(data = filter(tbl, grepl(&quot;P1F&quot;, tbl$Sample)), aes(x = Sample, y = expression, colour = Protein)) + geom_line(aes(group = Protein)) + facet_grid(Gender ~ Group) + theme(legend.position = &quot;none&quot;, axis.text.x = element_text(angle = 90, hjust = 1)) Plot the expression of all proteins in one sample (for example P1A10) against another one (for example P1A2) and use dataset-wide features such as the log-fold change of all CRC vs. Healthy samples and the grand mean expression intensity. ## Warning: Removed 1 rows containing missing values (geom_point). x &lt;- MSnbase::ms2df(crc) %&gt;% as_tibble x$rm &lt;- rowMeans(MSnbase::exprs(crc), na.rm = TRUE) fc &lt;- rowSums(MSnbase::exprs(crc[, crc$Group == &quot;CRC&quot;]), na.rm = TRUE) / rowSums(MSnbase::exprs(crc[, crc$Group != &quot;CRC&quot;]), na.rm = TRUE) x$lfc &lt;- log2(fc) ggplot(x, aes(P1A10, P1A2, colour = lfc, size = rm)) + geom_point() + scale_color_distiller(palette = &quot;Spectral&quot;) 5.9 References ggplot2 extensions - gallery ggplot2 webpage and documentation ggplot2: Elegant Graphics for Data Analysis book (source of the book available for free here). "],
["tools-and-plots.html", "Chapter 6 Tools and plots 6.1 Transformations 6.2 Comparing samples and linear models 6.3 Plots for statistical analyses 6.4 Visualising intersections 6.5 Unsupervised learning 6.6 Hierarchical clustering 6.7 k-means clustering 6.8 Principal component analysis (PCA) 6.9 t-Distributed Stochastic Neighbour Embedding 6.10 PCA loadings vs differential expression 6.11 Heatmaps", " Chapter 6 Tools and plots 6.1 Transformations 6.1.1 Expression data Let’s start with the comparison of two vectors of matching expression intensities such as those from two samples in the iprg3 dataset. Let’s extract the intensities of samples JD_06232014_sample1-A.raw (second column) and JD_06232014_sample1_B.raw (third column) and produce a scatter plot of one against the other. x &lt;- iprg3[[2]] y &lt;- iprg3[[3]] plot(x, y) Due to the distribution of the raw intensities, where most of the intensities are low with very few high intensities (see density plots below), the majority of points are squeezed close to the origin of the scatter plot. plot(density(na.omit(x)), col = &quot;blue&quot;) lines(density(na.omit(y)), col = &quot;red&quot;) This has negative effects as it (1) leads to overplotting in the low intensity range and (2) gives too much confidence in the correlation of the two vectors. A simple way to avoid this effect is to directly log-tranform the data or set the graph axes to log scales: par(mfrow = c(1, 2)) plot(log10(x), log10(y)) plot(x, y, log = &quot;xy&quot;) We will see better visualisations to detect correlation between sample replicates below. It is possible to generalise to production of scatter plots to more samples using the pairs function: pairs(iprg3[2:6], log = &quot;xy&quot;) A lot of space is wasted by repeating the same sets of plots in the upper right and lower left triangles of the matrix. See the pairs documentation page. See also lattice::splom and GGally::ggairs for alternatives. A general technique to overcome overplotting is to set the alpha scale (transparency), of to use graphics::smoothScatter: par(mfrow = c(1, 2)) plot(x, y, pch = 19, col = &quot;#00000010&quot;, log = &quot;xy&quot;) smoothScatter(log10(x), log10(y)) 6.1.2 Fold changes Log-transformation also comes handy when computing fold-changes. Below we calculate the fold-changes and log2 fold-changes (omitting missing values) fc &lt;- na.omit(iprg3[[2]] / iprg3[[3]]) lfc &lt;- log2(fc) Below, we see how the log2 fold-changes become symmetrical around zero (the absence of change), with positive values corresponding to up-regulation and negative values to down-regulation. plot(density(lfc), ylim = c(0, 5)) abline(v = median(lfc)) lines(density(fc), col = &quot;red&quot;) abline(v = median(fc), col = &quot;red&quot;) Note: when the data is already log-transformed, log fold-changes are computed by subtracting values. 6.2 Comparing samples and linear models Let’s return to the scatter plot example above and focus on three replicates from consitions 1 and 4, remove missing values and log-tranform the intensites. x &lt;- log2(na.omit(iprg3[, c(2, 3, 11)])) Below, we use the pairs function and print the pairwise correlations in the upper right traingle. ## put (absolute) correlations on the upper panels, ## with size proportional to the correlations. ## From ?pairs panel.cor &lt;- function(x, y, digits = 2, prefix = &quot;&quot;, cex.cor, ...) { usr &lt;- par(&quot;usr&quot;); on.exit(par(usr)) par(usr = c(0, 1, 0, 1)) r &lt;- abs(cor(x, y)) txt &lt;- format(c(r, 0.123456789), digits = digits)[1] txt &lt;- paste0(prefix, txt) if (missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt) text(0.5, 0.5, txt, cex = cex.cor * r) } pairs(x, lower.panel = panel.smooth, upper.panel = panel.cor) It is often assumed that high correlation is a halmark of good replication. Rather than focus on the correlation of the data, a better measurement would be to look a the log2 fold-changes, i.e. the distance between repeated measurements. The ideal way to visualise this is on an MA-plot: par(mfrow = c(1, 2)) r1 &lt;- x[[1]] r2 &lt;- x[[2]] M &lt;- r1 - r2 A &lt;- (r1 + r2)/2 plot(A, M); grid() library(&quot;affy&quot;) affy::ma.plot(A, M) See also this post on the Simply Statistics blog. abline(0, 1) can be used to add a line with intercept 0 and slop 1. It we want to add the line that models the data linearly, we can calculate the parameters using the lm function: lmod &lt;- lm(r2 ~ r1) summary(lmod) ## ## Call: ## lm(formula = r2 ~ r1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4939 -0.0721 0.0126 0.0881 3.4595 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.348190 0.091842 3.791 0.000153 *** ## r1 0.985878 0.003688 267.357 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3263 on 3024 degrees of freedom ## Multiple R-squared: 0.9594, Adjusted R-squared: 0.9594 ## F-statistic: 7.148e+04 on 1 and 3024 DF, p-value: &lt; 2.2e-16 which can be used to add the adequate line that reflects the (linear) relationship between the two data plot(r1, r2) abline(lmod, col = &quot;red&quot;) As we have seen in the beginning of this section, it is essential not to rely solely on the correlation value, but look at the data. This also holds true for linear (or any) modelling, which can be done by plotting the model: par(mfrow = c(2, 2)) plot(lmod) Cook’s distance is a commonly used estimate of the influence of a data point when performing a least-squares regression analysis and can be used to highlight points that particularly influence the regression. Leverage quantifies the influence of a given observation on the regression due to its location in the space of the inputs. See also ?influence.measures. Challenge Take any of the iprg3 replicates, model and plot their linear relationship. The Anscombe quartet is available as anscombe. Load it, create a linear model for one \\((x_i, y_i)\\) pair of your choice and visualise/check the model. x3 &lt;- anscombe[, 3] y3 &lt;- anscombe[, 7] lmod &lt;- lm(y3 ~ x3) summary(lmod) ## ## Call: ## lm(formula = y3 ~ x3) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1586 -0.6146 -0.2303 0.1540 3.2411 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0025 1.1245 2.670 0.02562 * ## x3 0.4997 0.1179 4.239 0.00218 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.236 on 9 degrees of freedom ## Multiple R-squared: 0.6663, Adjusted R-squared: 0.6292 ## F-statistic: 17.97 on 1 and 9 DF, p-value: 0.002176 par(mfrow = c(2, 2)) plot(lmod) 6.3 Plots for statistical analyses Let’s use the ALL_bclneg dataset, that we already have analysed with limma in the Data chapter. Whenever performing a statistical test, it is important to quality check the distribution of non-adjusted p-values. Below, we see an enrichment of small p-values, as opposed to a uniform distribution to be expected under the null hypothesis of absence of changes between groups. fvarLabels(ALL_bcrneg) ## [1] &quot;logFC&quot; &quot;AveExpr&quot; &quot;t&quot; &quot;P.Value&quot; &quot;adj.P.Val&quot; &quot;B&quot; hist(fData(ALL_bcrneg)$P.Value) The histograms below illustrate other distributions to beware of. Another important visualisation for statistical results are the Volcano plots, that show the relationship between the significance of the individual tests (adjusted p-values) and their magnitude of the effect (log2 fold-changes). lfc &lt;- fData(ALL_bcrneg)$logFC bh &lt;- fData(ALL_bcrneg)$adj.P.Val plot(lfc, -log10(bh), main = &quot;Volcano plot&quot;, xlab = expression(log[2]~fold-change), ylab = expression(-log[10]~adjusted~p-value)) grid() The volcano plot can further be annotated using vertical and horizontal lines depicting thresholds of interest or points can be colour-coded based on their interest. lfc &lt;- fData(ALL_bcrneg)$logFC bh &lt;- fData(ALL_bcrneg)$adj.P.Val sign &lt;- abs(lfc) &gt; 1 &amp; bh &lt; 0.01 plot(lfc, -log10(bh), main = &quot;Volcano plot&quot;, col = ifelse(sign, &quot;red&quot;, &quot;black&quot;), pch = ifelse(sign, 19, 1), xlab = expression(log[2]~fold-change), ylab = expression(-log[10]~adjusted~p-value)) grid() abline(v = c(-1, 1), lty = &quot;dotted&quot;) abline(h = -log10(0.05), lty = &quot;dotted&quot;) It is also possible to identify and label individual points on the plot using the identify function i &lt;- identify(lfc, -log10(bh), featureNames(ALL_bcrneg)) 6.4 Visualising intersections Venn and Euler diagrams are popular representation when comparing sets and their intersection. Two useful R packages to generate such plots are venneuler and Vennerable. We will use the crc feature names to generate a test data: set.seed(123) x &lt;- replicate(3, sample(featureNames(crc), 35), simplify = FALSE) names(x) &lt;- LETTERS[1:3] (v &lt;- Venn(x)) ## A Venn object on 3 sets named ## A,B,C ## 000 100 010 110 001 101 011 111 ## 0 11 9 6 5 10 12 8 plot(v) The UpSetR visualises intersections of sets as a matrix in which the rows represent the sets and the columns represent their intersection sizes. For each set that is part of a given intersection, a black filled circle is placed in the corresponding matrix cell. If a set is not part of the intersection, a light gray circle is shown. A vertical black line connects the topmost black circle with the bottom most black circle in each column to emphasise the column-based relationships. The size of the intersections is shown as a bar chart placed on top of the matrix so that each column lines up with exactly one bar. A second bar chart showing the size of the each set is shown to the left of the matrix. We will first make use of the fromList function to convert our list to a UpSetR compatible input and then generate the figure: library(&quot;UpSetR&quot;) x2 &lt;- fromList(x) upset(x2) The following tweet by the author of the package illustrates how Venn and upset diagrams relate to each other. upset(x2, order.by = &quot;freq&quot;) upset(x2, order.by = &quot;degree&quot;) upset(x2, order.by = c(&quot;freq&quot;, &quot;degree&quot;)) upset(x2, order.by = c(&quot;degree&quot;, &quot;freq&quot;)) upset(x2, sets = c(&quot;A&quot;, &quot;B&quot;)) upset(x2, sets = c(&quot;B&quot;, &quot;C&quot;, &quot;A&quot;), keep.order = TRUE) upset(x2, group.by = &quot;sets&quot;) ## Add set D with a single intersection x3 &lt;- x2 x3$D &lt;- 0 x3[1, &quot;D&quot;] &lt;- 1 head(x3) ## A B C D ## 1 1 0 1 1 ## 2 1 0 1 0 ## 3 1 0 0 0 ## 4 1 0 0 0 ## 5 1 0 0 0 ## 6 1 0 0 0 upset(x3) upset(x3, empty.intersections = &quot;on&quot;) Visualising intersections with UpSetR shines with more that 4 sets, as Venn diagrams become practically useless. Challenge Generate a bigger dataset containing 10 sets. Try to generate Venn and upset diagrams as shown above. set.seed(123) x &lt;- replicate(10, sample(featureNames(crc), 35), simplify = FALSE) names(x) &lt;- LETTERS[1:10] When the number of sets become larger, the options above, as well as nsets, the number of sets (default is 5) and nintersects, the number of intersectios (default is 40) becomes useful. 6.5 Unsupervised learning In unsupervised learning (UML), no labels are provided, and the learning algorithm focuses solely on detecting structure in unlabelled input data. One generally differentiates between Clustering, where the goal is to find homogeneous subgroups within the data; the grouping is based on distance between observations. Dimensionality reduction, where the goal is to identify patterns in the features of the data. Dimensionality reduction is often used to facilitate visualisation of the data, as well as a pre-processing method before supervised learning. UML presents specific challenges and benefits: there is no single goal in UML there is generally much more unlabelled data available than labelled data. Unsupervised learning techniques are paramount for exploratory data analysis and visualisation. 6.6 Hierarchical clustering How does hierarchical clustering work Initialisation: Starts by assigning each of the n points its own cluster Iteration Find the two nearest clusters, and join them together, leading to n-1 clusters Continue the cluster merging process until all are grouped into a single cluster Termination: All observations are grouped within a single cluster. Figure 6.1: Hierarchical clustering: initialisation (left) and colour-coded results after iteration (right). The results of hierarchical clustering are typically visualised along a dendrogram, where the distance between the clusters is proportional to the branch lengths. Figure 6.2: Visualisation of the hierarchical clustering results on a dendrogram In R: Calculate the distance using dist, typically the Euclidean distance. Hierarchical clustering on this distance matrix using hclust Challenge Apply hierarchical clustering on the mulvey2015norm data and generate a sample dendrogram using the dedicated plot method as shown below. d &lt;- dist(t(MSnbase::exprs(mulvey2015norm))) hcl &lt;- hclust(d) hcl ## ## Call: ## hclust(d = d) ## ## Cluster method : complete ## Distance : euclidean ## Number of objects: 18 plot(hcl, main = &quot;Mulvey et al. 2016&quot;) Defining clusters After producing the hierarchical clustering result, we need to cut the tree (dendrogram) at a specific height to defined the clusters. For example, on our test dataset above, we could decide to cut it at a distance around 15, that would produce 3 clusters, namely the XEN samples, the late time points 72 and 48 hours, and the early time points. Figure 6.3: Cutting the dendrogram at height 15. In R we can us the cutree function to cut the tree at a specific height: cutree(hcl, h = 15) cut the tree to get a certain number of clusters: cutree(hcl, k = 3) Challenge Cut the hierarchical clustering result at a height to obtain 4 clusters by setting h. Cut the hierarchical clustering result at a height to obtain 4 clusters by setting directly k, and verify that both provide the same results. What happens if we wanted to obtain 5 clusters. plot(hcl) abline(h = 11, col = &quot;red&quot;) cutree(hcl, k = 4) cutree(hcl, h = 11) identical(cutree(hcl, k = 4), cutree(hcl, h = 11)) plot(hcl, labels = cutree(hcl, k = 4)) For more advanced dendrogram visualisations, see dendextend. Effect of distance and clustering algorithm It is important to highlight that the type of distance and the type of clustering algorithms will affect the result. Let’s observe how using the euclidean or Pearson correlation distances between expression profiles can fundamentally change the results. ## toy example: 3 genes, 5 samples from Olga Vitek gene1 &lt;- c(1, 6, 2, 4, 7) gene2 &lt;- gene1 + 4 gene3 &lt;- gene2/3 + c(0, 2, 0, 4, 0) e &lt;- rbind(gene1, gene2, gene3) dimnames(e) &lt;- list(paste0(&quot;gene&quot;, 1:3), paste0(&quot;sample&quot;, 1:5)) e ## sample1 sample2 sample3 sample4 sample5 ## gene1 1.000000 6.000000 2 4.000000 7.000000 ## gene2 5.000000 10.000000 6 8.000000 11.000000 ## gene3 1.666667 5.333333 2 6.666667 3.666667 matplot(t(e), type = &quot;b&quot;, xlab = &quot;Samples&quot;, ylab = &quot;Expression&quot;) library(&quot;bioDist&quot;) par(mfrow = c(1, 2)) plot(hclust(euc(e)), main = &quot;Euclidean distane&quot;) plot(hclust(cor.dist(e)), main = &quot;Pearson correlational distance&quot;) Many of the machine learning methods that are regularly used are sensitive to difference scales. This applies to unsupervised methods as well as supervised methods. Below, we show how centering and scaling the data (row-wise, hence the transposition) affects the euclidean distance but not the Pearson correlation distance. e2 &lt;- t(scale(t(e))) ## or manually ## e2 &lt;- (e - rowMeans(e)) / genefilter::rowSds(e) matplot(t(e2), type = &quot;b&quot;, xlab = &quot;Samples&quot;, ylab = &quot;Expression&quot;) par(mfrow = c(2, 2)) plot(hclust(euc(e)), main = &quot;Euclidean distance&quot;) plot(hclust(euc(e2)), main = &quot;Euclidean distance (scaled/centred)&quot;) plot(hclust(cor.dist(e)), main = &quot;Pearson correlational distance&quot;) plot(hclust(cor.dist(e2)), main = &quot;Pearson correlational distance (scaled/centred)&quot;) Finally, the clustering method itself (the method argument in hclust) will also influence the results: ALL_sign &lt;- ALL_bcrneg[fData(ALL_bcrneg)$adj.P.Val &lt; 0.005, ] par(mfrow = c(1, 3)) plot(hclust(dist(ALL_sign), method = &quot;complete&quot;)) plot(hclust(dist(ALL_sign), method = &quot;single&quot;)) plot(hclust(dist(ALL_sign), method = &quot;average&quot;)) 6.7 k-means clustering The k-means clustering algorithms aims at partitioning n observations into a fixed number of k clusters. The algorithm will find homogeneous clusters. In R, we use stats::kmeans(x, centers = 3, nstart = 10) where x is a numeric data matrix centers is the pre-defined number of clusters the k-means algorithm has a random component and can be repeated nstart times to improve the returned model Challenge: Run the k-means algorithm on the reduce hlm spatial proteomics dataset, save the results in a new variable cl, and explore its output when printed. Set the number of clusters to be the number of annotated sub-cellular niches. The actual results of the algorithms, i.e. the cluster membership can be accessed in the clusters element of the clustering result output. Use it to colour the inferred clusters to generate a figure like that shown below. cl &lt;- kmeans(MSnbase::exprs(hlm), length(getMarkerClasses(hlm)), nstart = 10) pairs(MSnbase::exprs(hlm)[, c(1, 3, 5, 9)], col = cl$cluster) How does k-means work Below, we use a manually generated dataset with 3 clusters. Initialisation: randomly assign class membership set.seed(123) x &lt;- data.frame(x = rnorm(100, 5, 4), y = rnorm(100, 5, 4)) init &lt;- sample(3, nrow(x), replace = TRUE) plot(x, col = init) Figure 6.4: k-means random intialisation Iteration: Calculate the centre of each subgroup as the average position of all observations is that subgroup. Each observation is then assigned to the group of its nearest centre. It’s also possible to stop the algorithm after a certain number of iterations, or once the centres move less than a certain distance. par(mfrow = c(1, 2)) plot(x, col = init) centres &lt;- sapply(1:3, function(i) colMeans(x[init == i, ], )) centres &lt;- t(centres) points(centres[, 1], centres[, 2], pch = 19, col = 1:3) tmp &lt;- dist(rbind(centres, x)) tmp &lt;- as.matrix(tmp)[, 1:3] ki &lt;- apply(tmp, 1, which.min) ki &lt;- ki[-(1:3)] plot(x, col = ki) points(centres[, 1], centres[, 2], pch = 19, col = 1:3) Figure 6.5: k-means iteration: calculate centers (left) and assign new cluster membership (right) Termination: Repeat iteration until no point changes its cluster membership. k-means convergence (credit Wikipedia) Model selection Due to the random initialisation, one can obtain different clustering results. When k-means is run multiple times, the best outcome, i.e. the one that generates the smallest total within cluster sum of squares (SS), is selected. The total within SS is calculated as: For each cluster results: for each observation, determine the squared euclidean distance from observation to centre of cluster sum all distances Note that this is a local minimum; there is no guarantee to obtain a global minimum. Below, we see that if we repeat k-means on our x data multiple times, setting the number of iterations to 1 or greater and check whether you repeatedly obtain the same results. cl1 &lt;- kmeans(x, centers = 3, nstart = 10) cl2 &lt;- kmeans(x, centers = 3, nstart = 10) table(cl1$cluster, cl2$cluster) ## ## 1 2 3 ## 1 0 29 0 ## 2 0 0 33 ## 3 38 0 0 cl3 &lt;- kmeans(x, centers = 3, nstart = 1) cl4 &lt;- kmeans(x, centers = 3, nstart = 1) table(cl3$cluster, cl4$cluster) ## ## 1 2 3 ## 1 0 37 1 ## 2 27 0 2 ## 3 0 0 33 How to determine the number of clusters Run k-means with k=1, k=2, …, k=n Record total within SS for each value of k. Choose k at the elbow position, as illustrated below. ks &lt;- 5:20 x &lt;- MSnbase::exprs(hlm) tot_within_ss &lt;- sapply(ks, function(k) { cl &lt;- kmeans(x, k, nstart = 10) cl$tot.withinss }) plot(ks, tot_within_ss, type = &quot;b&quot;, ylab = &quot;Total within squared distances&quot;, xlab = &quot;Values of k tested&quot;) There exists other metrics, other than the total within cluster sum of squares that can be applied, such as the gap statistic (see cluster::clusGap), or the Akaike (AIC) and Bayesian (BIC) information criteria. Challenge Let’s use what we have learned to cluster the 2337 proteins from the mulvey2015norm data in 20 clusters. Use k-means to cluster the mulvey2015norm data, setting centers = 20. Take care in repeating the clustering more than once. To plot the expression profiles for the 20 clusters, I suggest to use gplot2. Do do so, create a 2337 proteins by 18 sample dataframe (or tibble), appending the protein accession numbers (from the feature data - you can use the MSnbase::ms2df helper function.) and cluster numbers as 2 additional columns. Use gather to transform the data in a long format. Use ggplot2 to reproduce the figure below. Optional: use stat_summary to add a mean profile for each cluster of proteins. kmeans clustering on mulvey2015norm library(&quot;pRolocdata&quot;) data(mulvey2015norm) cl &lt;- kmeans(MSnbase::exprs(mulvey2015norm), centers = 16, nstart = 10, iter.max = 50) x &lt;- ms2df(mulvey2015norm, fcol = &quot;Accession&quot;) x[[&quot;cluster&quot;]] &lt;- cl$cluster tb &lt;- gather(x, key = sample, value = expression, -cluster, -Accession) %&gt;% as_tibble ## Check dimensions stopifnot(nrow(tb) == prod(dim(mulvey2015norm))) pd &lt;- pData(mulvey2015norm) tb$time &lt;- pd[tb[[&quot;sample&quot;]], &quot;times&quot;] tb$rep &lt;- pd[tb[[&quot;sample&quot;]], &quot;rep&quot;] ## Plotting kmp &lt;- ggplot(data = tb, aes(x = paste(time, rep), y = expression, group = Accession, colour = as.factor(cluster))) + geom_line() + facet_wrap(~ cluster) + theme(legend.position = &quot;none&quot;) + scale_x_discrete(&quot;Time course&quot;) kmp2 &lt;- kmp + stat_summary(aes(group = cluster), fun.y = mean, geom = &quot;line&quot;, colour = &quot;black&quot;) 6.8 Principal component analysis (PCA) Dimensionality reduction techniques are widely used and versatile techniques that can be used to: find structure in features pre-processing for other ML algorithms, and aid in visualisation. The basic principle of dimensionality reduction techniques is to transform the data into a new space that summarise properties of the whole data set along a reduced number of dimensions. These are then ideal candidates used to visualise the data along these reduced number of informative dimensions. How does it work Principal Component Analysis (PCA) is a technique that transforms the original n-dimensional data into a new n-dimensional space. These new dimensions are linear combinations of the original data, i.e. they are composed of proportions of the original variables. Along these new dimensions, called principal components, the data expresses most of its variability along the first PC, then second, … Principal components are orthogonal to each other, i.e. non-correlated. Figure 6.6: Original data (left). PC1 will maximise the variability while minimising the residuals (centre). PC2 is orthogonal to PC1 (right). In R, we can use the prcomp function. Let’s apply PCA on the mulvey2015norm samples. pca &lt;- prcomp(MSnbase::exprs(t(mulvey2015norm))) summary(pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 ## Standard deviation 12.803 5.8737 2.32468 1.92711 1.50167 1.31702 ## Proportion of Variance 0.748 0.1574 0.02466 0.01695 0.01029 0.00791 ## Cumulative Proportion 0.748 0.9054 0.93003 0.94698 0.95727 0.96518 ## PC7 PC8 PC9 PC10 PC11 PC12 ## Standard deviation 1.27894 1.09257 1.06266 0.96421 0.83033 0.80686 ## Proportion of Variance 0.00746 0.00545 0.00515 0.00424 0.00315 0.00297 ## Cumulative Proportion 0.97265 0.97809 0.98325 0.98749 0.99063 0.99360 ## PC13 PC14 PC15 PC16 PC17 PC18 ## Standard deviation 0.72439 0.64802 0.57176 0.2582 0.25217 3.468e-15 ## Proportion of Variance 0.00239 0.00192 0.00149 0.0003 0.00029 0.000e+00 ## Cumulative Proportion 0.99600 0.99791 0.99941 0.9997 1.00000 1.000e+00 A summary of the prcomp output shows that along PC1 along, we are able to retain over 75% of the total variance in the data. Visualisation A biplot features all original points re-mapped (rotated) along the first two PCs as well as the original features as vectors along the same PCs. biplot(pca) Feature vectors that are in the same direction in PC space are also correlated in the original data space. This can be seen below when running PCA on the protein (rather than the samples). biplot(prcomp(MSnbase::exprs(mulvey2015norm))) One important piece of information when using PCA is the proportion of variance explained along the PCs, in particular when dealing with high dimensional data, as PC1 and PC2 (that are generally used for visualisation), might only account for an insufficient proportion of variance to be relevant on their own. In the code chunk below, I extract the standard deviations from the PCA result to calculate the variances, then obtain the percentage of and cumulative variance along the PCs. var &lt;- pca$sdev^2 (pve &lt;- var/sum(var)) ## [1] 7.479554e-01 1.574190e-01 2.465846e-02 1.694543e-02 1.028939e-02 ## [6] 7.914534e-03 7.463453e-03 5.446791e-03 5.152624e-03 4.242092e-03 ## [11] 3.145870e-03 2.970534e-03 2.394341e-03 1.916061e-03 1.491659e-03 ## [16] 3.042167e-04 2.901557e-04 5.489151e-32 cumsum(pve) ## [1] 0.7479554 0.9053744 0.9300329 0.9469783 0.9572677 0.9651822 0.9726457 ## [8] 0.9780924 0.9832451 0.9874872 0.9906330 0.9936036 0.9959979 0.9979140 ## [15] 0.9994056 0.9997098 1.0000000 1.0000000 Challenge Repeat the PCA analysis above, reproducing the biplot and preparing a barplot of the percentage of variance explained by each PC. It is often useful to produce custom figures using the data coordinates in PCA space, which can be accessed as x in the prcomp object. Reproduce the PCA plots below, along PC1 and PC2 and PC3 and PC4 respectively. par(mfrow = c(1, 2)) plot(pca$x[, 1:2], col = mulvey2015norm$times) plot(pca$x[, 3:4], col = mulvey2015norm$times) Data pre-processing We haven’t looked at other prcomp parameters, other that the first one, x. There are two other ones that are or importance, in particular in the light of the section on pre-processing above, which are center and scale.. The former is set to TRUE by default, while the second one is set the FALSE. Example Repeat the analysis comparing the need for scaling on the mtcars dataset, but using PCA instead of hierarchical clustering. When comparing the two. par(mfrow = c(1, 2)) biplot(prcomp(mtcars, scale = FALSE), main = &quot;No scaling&quot;) ## 1 biplot(prcomp(mtcars, scale = TRUE), main = &quot;With scaling&quot;) ## 2 Without scaling, disp and hp are the features with the highest loadings along PC1 and 2 (all others are negligible), which are also those with the highest units of measurement. Scaling removes this effect. Final comments on PCA Real datasets often come with missing values. In R, these should be encoded using NA. Unfortunately, standard PCA cannot deal with missing values, and observations containing NA values will be dropped automatically. This is a viable solution only when the proportion of missing values is low. Alternatively, the NIPALS (non-linear iterative partial least squares) implementation does support missing values (see nipals::nipals). Finally, we should be careful when using categorical data in any of the unsupervised methods described above. Categories are generally represented as factors, which are encoded as integer levels, and might give the impression that a distance between levels is a relevant measure (which it is not, unless the factors are ordered). In such situations, categorical data can be dropped, or it is possible to encode categories as binary dummy variables. For example, if we have 3 categories, say A, B and C, we would create two dummy variables to encode the categories as: ## x y ## A 1 0 ## B 0 1 ## C 0 0 so that the distance between each category are approximately equal to 1. Challenge Produce the PCA plot for the ALL_bcrneg samples, and annotating the NEG and BCR/ABL samples on the plot. Do you think that the two first components offer enough resolution? pca &lt;- prcomp(t(MSnbase::exprs(ALL_bcrneg)), scale = TRUE, center = TRUE) plot(pca$x[, 1:2], col = ALL_bcrneg$mol.bio, cex = 2) legend(&quot;bottomright&quot;, legend = unique(ALL_bcrneg$mol.bio), pch = 1, col = c(&quot;black&quot;, &quot;red&quot;), bty = &quot;n&quot;) text(pca$x[, 1], pca$x[, 2], sampleNames(ALL_bcrneg), cex = 0.8) par(mfrow = c(1, 2)) barplot(pca$sdev^2/sum(pca$sdev^2), xlab=&quot;Principle component&quot;, ylab=&quot;% of variance&quot;) barplot(cumsum(pca$sdev^2/sum(pca$sdev^2) ), xlab=&quot;Principle component&quot;, ylab=&quot;Pumulative % of variance&quot;) ## Conclusion: the two first principle components are insufficient Repeat the kmeans clustering analysis on the hml data above and visualise the results on a PCA plot. cl &lt;- kmeans(MSnbase::exprs(hlm), length(getMarkerClasses(hlm)), nstart = 10) pca &lt;- prcomp(MSnbase::exprs(hlm), scale = TRUE, center = TRUE) plot(pca$x[, 1:2], col = cl$cluster) Produce a PCA with the proteins of the complete hyperLopit2015 experiment. Label the results using final.assignment feature variable (corresponding to the proteins assignment to they sub-cellular localisation) and scale to size of the points based on the assignment scores available in the svm.score feature variable (this might need to transform the scores to make differences visisble). x &lt;- prcomp(MSnbase::exprs(hyperLOPIT2015)) plot(x$x[, 1:2], pch = ifelse(fData(hyperLOPIT2015)$final.assignment == &quot;unknown&quot;, 1, 19), col = as.factor(fData(hyperLOPIT2015)$final.assignment), cex = exp(fData(hyperLOPIT2015)$svm.score) - 1) 6.9 t-Distributed Stochastic Neighbour Embedding t-Distributed Stochastic Neighbour Embedding (t-SNE) is a non-linear dimensionality reduction technique, i.e. that different regions of the data space will be subjected to different transformations. t-SNE will compress small distances, thus bringing close neighbours together, and will ignore large distances. It is particularly well suited for very high dimensional data. In R, we can use the Rtsne function from the Rtsne. As with PCA, the data can be scaled and centred prior the running t-SNE (see the pca_center and pca_scale arguments). The algorithm is stochastic, and will produce different results at each repetition. 6.9.1 Parameter tuning t-SNE has two important parameters that can substantially influence the clustering of the data Perplexity: balances global and local aspects of the data. Iterations: number of iterations before the clustering is stopped. It is important to adapt these for different data. The figure below shows a 5032 by 20 dataset that represent protein sub-cellular localisation. Effect of different perplexity and iterations when running t-SNE As a comparison, below are the same data with PCA (left) and t-SNE (right). PCA and t-SNE on hyperLOPIT 6.10 PCA loadings vs differential expression Let’s now compare the PCA loadings as calculated above and the p-values. As we can see below, large loadings may or may not correspond to small p-values. pca &lt;- prcomp(t(MSnbase::exprs(ALL_bcrneg)), scale = TRUE, center = TRUE) par(mfrow = c(1, 2)) plot(pca$x[,1:2], col = ALL_sign$mol.biol, pch = 19) legend(&quot;bottomleft&quot;, legend = unique(ALL_sign$mol.biol), col = 1:2, pch = 19, bty = &quot;n&quot;) smoothScatter(pca$rotation[, 1], fData(ALL_bcrneg)$adj.P.Val, xlab = &quot;PCA Loading&quot;, ylab = &quot;Adjusted p-values&quot;) Let’s now repeat the same camparison, focusing on differentially expressed genes. table(sign &lt;- fData(ALL_bcrneg)$adj.P.Val &lt; 0.05) ## ## FALSE TRUE ## 12442 183 ALL_sign &lt;- ALL_bcrneg[sign, ] pca_sign &lt;- prcomp(t(MSnbase::exprs(ALL_sign)), center = TRUE, scale = TRUE) Below, we see that there is better separation when we focus on differentially expressed genes and better consistency between p-values and loadings. However we can’t do this in practice! par(mfrow = c(1, 2)) plot(pca_sign$x[,1:2], col = ALL_sign$mol.biol, pch = 19) legend(&quot;bottomleft&quot;, legend = unique(ALL_sign$mol.biol), col = 1:2, pch = 19, bty = &quot;n&quot;) plot(pca_sign$rotation[, 1], fData(ALL_sign)$adj.P.Val, xlab = &quot;PCA Loading&quot;, ylab = &quot;Adjusted p-values&quot;) 6.11 Heatmaps heatmap(MSnbase::exprs(mulvey2015norm)) A heatmap is composed of two hierarchical clusters (one along the rows, one along the columns, leading to their re-ordering based on their similarity) and a intensity matrix. Each of these components is subject to parameters and options. As we have seen above, the distance used for clustering can have a substantial effect on the results, which is conformed below. heatmap(MSnbase::exprs(mulvey2015norm), distfun = cor.dist) Another important argument, scale controls whether rows, columns or none are scaled. Let’s re-use the toy data from the hierarchical clustering section below. heatmap(e, scale = &quot;none&quot;, main = &quot;No scaling&quot;) heatmap(e, scale = &quot;row&quot;, main = &quot;Scaling along rows&quot;) heatmap(e, scale = &quot;column&quot;, main = &quot;Scaling along columns&quot;) Based on the caveats above, it is essential to present and interpret heatmaps with great care. It is of course possible to use any type of data, not only expression data, to build a heatmap. There exists several packages that allow to produce heatmaps with various levels of sophistication, such as heatmap.2 from the gplots package, the Heatplus package, or the ComplexHeatmap packages, demonstrated below. library(&quot;ComplexHeatmap&quot;) x &lt;- MSnbase::exprs(mulvey2015norm) hcl &lt;- hclust(dist(x)) cl &lt;- cutree(hcl, k = 12) ha1 &lt;- HeatmapAnnotation(data.frame(time = factor(mulvey2015norm$time))) ha2 &lt;- HeatmapAnnotation(boxplot = anno_boxplot(x)) ha3 &lt;- rowAnnotation(data.frame(cluster = factor(cl))) Heatmap(x, top_annotation = ha1, bottom_annotation = ha2, column_names_gp = gpar(fontsize = 8), row_names_gp = gpar(fontsize = 3), bottom_annotation_height = unit(3, &quot;cm&quot;)) + ha3 Finally, the heatmaply can be used to generate interactive heatmaps. library(&quot;heatmaply&quot;) heatmaply(x[1:100, ]) Reference: Key M. A tutorial in displaying mass spectrometry-based proteomic data using heat maps. BMC Bioinformatics. 2012;13 Suppl 16:S10. doi: 10.1186/1471-2105-13-S16-S10. Epub 2012 Nov 5. Review. PMID: 23176119; PMCID: PMC3489527. "],
["using-r-and-bioconductor-for-ms-based-proteomics.html", "Chapter 7 Using R and Bioconductor for MS-based proteomics 7.1 Introduction 7.2 Reading MS data into R 7.3 Manipulating and visualising raw data 7.4 MS maps 7.5 Peptide spetrum matches 7.6 MS imaging 7.7 Missing values", " Chapter 7 Using R and Bioconductor for MS-based proteomics 7.1 Introduction R and Bioconductor offer a range of dedicated packages for the analysis of mass spectrometry-based proteomics. The advantage of such technology-infrastructure is that offer specific behaviour for these kind of data. The RforProteomics package provides a first introduction to what is available. It is also possible to navigate the biocViews hierarchy (these are tags that are used to describe individual package’s domain of action). The highest level defines 3 types of packages: Software: packages providing a specific functionality. AnnotationData: packages providing annotations, such as various ontologies, species annotations, microarray annotations, … ExperimentData: packages distributing experiments. The biocViews that are relevant here are Proteomcis, MassSpectrometry and MassSpectrometryData. The biocViews page is available here https://bioconductor.org/packages/release/BiocViews.html#___Software It is most easily accessed by clicking on the software packages link on the homepage, under About Bioconductor. See also this page for additional information. Here, we will focus on the MSnbase package, as it supports all types of MS-based proteomics data and files that one would generally encounter: Raw data Identification data Quantiative data The respective types of data come in the form of mzML files (mzXML and netCDF are also supported) mzIdentML files (abbreviated mzid) Arbitrary spreadsheets or mzTab files which can be read into R using readMSData readMzIdData readMSnSet2 or readMzTabData Once loaded in R, the data are stored as dedicated data structures MSnExp (an MS experiment) data.frame MSnSet (an MS expression set) ## ## ## data file.type function. class ## --------------- ------------ -------------- ----------- ## raw mzML readMSData MSnExp ## identification mzIdentML readMzIdData data.frame ## quantitation spreadsheet readMSnSet2 MSnSet ## quantitation mzTab readMzTabdata MSnSet To start, let’s load the MSnbase package library(&quot;MSnbase&quot;) We will make use of test data that is provided by the experiment package msdata, that we load below. We then extract the file names that will be used later on. library(&quot;msdata&quot;) rawf &lt;- msdata::proteomics(full.names = TRUE, pattern = &quot;20141210&quot;) basename(rawf) ## [1] &quot;TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML.gz&quot; idf &lt;- msdata::ident(full.names = TRUE) basename(idf) ## [1] &quot;TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzid&quot; 7.2 Reading MS data into R Raw data rw &lt;- readMSData(rawf, mode = &quot;onDisk&quot;) rw ## MSn experiment data (&quot;OnDiskMSnExp&quot;) ## Object size in memory: 2.96 Mb ## - - - Spectra data - - - ## MS level(s): 1 2 ## Number of spectra: 7534 ## MSn retention times: 0:0 - 60:2 minutes ## - - - Processing information - - - ## Data loaded [Tue May 1 20:58:58 2018] ## MSnbase version: 2.5.15 ## - - - Meta data - - - ## phenoData ## rowNames: ## TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML.gz ## varLabels: sampleNames ## varMetadata: labelDescription ## Loaded from: ## TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML.gz ## protocolData: none ## featureData ## featureNames: F1.S0001 F1.S0002 ... F1.S7534 (7534 total) ## fvarLabels: fileIdx spIdx ... spectrum (29 total) ## fvarMetadata: labelDescription ## experimentData: use &#39;experimentData(object)&#39; chr &lt;- chromatogram(rw) chr ## Chromatograms with 1 row and 1 column ## TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML.gz ## &lt;Chromatogram&gt; ## [1,] length: 1431 ## phenoData with 1 variables ## featureData with 1 variables Identification data id &lt;- readMzIdData(idf) id ## sequence ## 1 RQCRTDFLNYLR ## 2 ESVALADQVTCVDWRNRKATKK ## 3 KELLCLAMQIIR ## spectrumID chargeState rank ## 1 controllerType=0 controllerNumber=1 scan=2949 3 1 ## 2 controllerType=0 controllerNumber=1 scan=6534 2 1 ## 3 controllerType=0 controllerNumber=1 scan=5674 2 1 ## passThreshold experimentalMassToCharge calculatedMassToCharge modNum ## 1 TRUE 548.2856 547.9474 1 ## 2 TRUE 1288.1528 1288.1741 1 ## 3 TRUE 744.4109 744.4255 1 ## isDecoy post pre start end DatabaseAccess DBseqLength ## 1 FALSE V R 574 585 ECA2006 1295 ## 2 FALSE G R 69 90 ECA1676 110 ## 3 TRUE Q R 131 142 XXX_ECA2855 157 ## DatabaseSeq ## 1 ## 2 ## 3 ## DatabaseDescription ## 1 ECA2006 ATP-dependent helicase ## 2 ECA1676 putative growth inhibitory protein ## 3 ## acquisitionNum ## 1 2949 ## 2 6534 ## 3 5674 ## spectrumFile ## 1 TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML ## 2 TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML ## 3 TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML ## idFile ## 1 TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzid ## 2 TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzid ## 3 TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzid ## MS.GF.RawScore MS.GF.DeNovoScore MS.GF.SpecEValue MS.GF.EValue ## 1 10 101 4.617121e-08 1.321981e-01 ## 2 12 121 7.255875e-08 2.087481e-01 ## 3 8 74 9.341019e-08 2.674533e-01 ## MS.GF.QValue MS.GF.PepQValue modName modMass modLocation ## 1 0.52542370 0.54901963 Carbamidomethyl 57.02146 3 ## 2 0.61038960 0.62318840 Carbamidomethyl 57.02146 11 ## 3 0.62500000 0.63636360 Carbamidomethyl 57.02146 5 ## subOriginalResidue subReplacementResidue subLocation ## 1 &lt;NA&gt; &lt;NA&gt; NA ## 2 &lt;NA&gt; &lt;NA&gt; NA ## 3 &lt;NA&gt; &lt;NA&gt; NA ## [ reached getOption(&quot;max.print&quot;) -- omitted 5799 rows ] fvarLabels(rw) ## [1] &quot;fileIdx&quot; &quot;spIdx&quot; ## [3] &quot;smoothed&quot; &quot;seqNum&quot; ## [5] &quot;acquisitionNum&quot; &quot;msLevel&quot; ## [7] &quot;polarity&quot; &quot;originalPeaksCount&quot; ## [9] &quot;totIonCurrent&quot; &quot;retentionTime&quot; ## [11] &quot;basePeakMZ&quot; &quot;basePeakIntensity&quot; ## [13] &quot;collisionEnergy&quot; &quot;ionisationEnergy&quot; ## [15] &quot;lowMZ&quot; &quot;highMZ&quot; ## [17] &quot;precursorScanNum&quot; &quot;precursorMZ&quot; ## [19] &quot;precursorCharge&quot; &quot;precursorIntensity&quot; ## [21] &quot;mergedScan&quot; &quot;mergedResultScanNum&quot; ## [23] &quot;mergedResultStartScanNum&quot; &quot;mergedResultEndScanNum&quot; ## [25] &quot;injectionTime&quot; &quot;filterString&quot; ## [27] &quot;spectrumId&quot; &quot;centroided&quot; ## [29] &quot;spectrum&quot; rw &lt;- addIdentificationData(rw, idf) fvarLabels(rw) ## [1] &quot;fileIdx&quot; &quot;spIdx&quot; ## [3] &quot;smoothed&quot; &quot;seqNum&quot; ## [5] &quot;acquisitionNum&quot; &quot;msLevel&quot; ## [7] &quot;polarity&quot; &quot;originalPeaksCount&quot; ## [9] &quot;totIonCurrent&quot; &quot;retentionTime&quot; ## [11] &quot;basePeakMZ&quot; &quot;basePeakIntensity&quot; ## [13] &quot;collisionEnergy&quot; &quot;ionisationEnergy&quot; ## [15] &quot;lowMZ&quot; &quot;highMZ&quot; ## [17] &quot;precursorScanNum&quot; &quot;precursorMZ&quot; ## [19] &quot;precursorCharge&quot; &quot;precursorIntensity&quot; ## [21] &quot;mergedScan&quot; &quot;mergedResultScanNum&quot; ## [23] &quot;mergedResultStartScanNum&quot; &quot;mergedResultEndScanNum&quot; ## [25] &quot;injectionTime&quot; &quot;filterString&quot; ## [27] &quot;spectrumId&quot; &quot;centroided&quot; ## [29] &quot;spectrum&quot; &quot;acquisition.number&quot; ## [31] &quot;sequence&quot; &quot;chargeState&quot; ## [33] &quot;rank&quot; &quot;passThreshold&quot; ## [35] &quot;experimentalMassToCharge&quot; &quot;calculatedMassToCharge&quot; ## [37] &quot;modNum&quot; &quot;isDecoy&quot; ## [39] &quot;post&quot; &quot;pre&quot; ## [41] &quot;start&quot; &quot;end&quot; ## [43] &quot;DatabaseAccess&quot; &quot;DBseqLength&quot; ## [45] &quot;DatabaseSeq&quot; &quot;DatabaseDescription&quot; ## [47] &quot;idFile&quot; &quot;MS.GF.RawScore&quot; ## [49] &quot;MS.GF.DeNovoScore&quot; &quot;MS.GF.SpecEValue&quot; ## [51] &quot;MS.GF.EValue&quot; &quot;MS.GF.QValue&quot; ## [53] &quot;MS.GF.PepQValue&quot; &quot;modName&quot; ## [55] &quot;modMass&quot; &quot;modLocation&quot; ## [57] &quot;subOriginalResidue&quot; &quot;subReplacementResidue&quot; ## [59] &quot;subLocation&quot; &quot;nprot&quot; ## [61] &quot;npep.prot&quot; &quot;npsm.prot&quot; ## [63] &quot;npsm.pep&quot; Quantitative data library(&quot;pRolocdata&quot;) qtf &lt;- dir(system.file(&quot;extdata&quot;, package = &quot;pRolocdata&quot;), full.names = TRUE, pattern = &quot;Dunkley2006&quot;) basename(qtf) ## [1] &quot;Dunkley2006.csv.gz&quot; qt &lt;- readMSnSet2(qtf, ecol = 5:20) qt ## MSnSet (storageMode: lockedEnvironment) ## assayData: 689 features, 16 samples ## element names: exprs ## protocolData: none ## phenoData: none ## featureData ## featureNames: 1 2 ... 689 (689 total) ## fvarLabels: Protein.ID Loc.Predicted ... pd.markers (6 total) ## fvarMetadata: labelDescription ## experimentData: use &#39;experimentData(object)&#39; ## Annotation: ## - - - Processing information - - - ## MSnbase version: 2.6.0 7.3 Manipulating and visualising raw data plot(chr) rawf &lt;- proteomics(full.names = TRUE, pattern = &quot;MS3TMT11&quot;) basename(rawf) ## [1] &quot;MS3TMT11.mzML&quot; rw &lt;- readMSData(rawf, mode = &quot;onDisk&quot;) rw ## MSn experiment data (&quot;OnDiskMSnExp&quot;) ## Object size in memory: 0.5 Mb ## - - - Spectra data - - - ## MS level(s): 1 2 3 ## Number of spectra: 994 ## MSn retention times: 45:27 - 47:6 minutes ## - - - Processing information - - - ## Data loaded [Mon May 7 02:39:18 2018] ## MSnbase version: 2.6.0 ## - - - Meta data - - - ## phenoData ## rowNames: MS3TMT11.mzML ## varLabels: sampleNames ## varMetadata: labelDescription ## Loaded from: ## MS3TMT11.mzML ## protocolData: none ## featureData ## featureNames: F1.S001 F1.S002 ... F1.S994 (994 total) ## fvarLabels: fileIdx spIdx ... spectrum (29 total) ## fvarMetadata: labelDescription ## experimentData: use &#39;experimentData(object)&#39; table(centroided(rw), msLevel(rw)) ## ## 1 2 3 ## FALSE 30 0 0 ## TRUE 0 482 482 head(acquisitionNum(rw)) ## F1.S001 F1.S002 F1.S003 F1.S004 F1.S005 F1.S006 ## 21945 21946 21947 21948 21949 21950 i &lt;- 21945 rw2 &lt;- filterPrecursorScan(rw, 21945) rw2 ## MSn experiment data (&quot;OnDiskMSnExp&quot;) ## Object size in memory: 0.05 Mb ## - - - Spectra data - - - ## MS level(s): 1 2 3 ## Number of spectra: 35 ## MSn retention times: 45:27 - 45:30 minutes ## - - - Processing information - - - ## Data loaded [Mon May 7 02:39:18 2018] ## Filter: select parent/children scans for 21945 [Mon May 7 02:39:18 2018] ## MSnbase version: 2.6.0 ## - - - Meta data - - - ## phenoData ## rowNames: MS3TMT11.mzML ## varLabels: sampleNames ## varMetadata: labelDescription ## Loaded from: ## MS3TMT11.mzML ## protocolData: none ## featureData ## featureNames: F1.S001 F1.S002 ... F1.S035 (35 total) ## fvarLabels: fileIdx spIdx ... spectrum (29 total) ## fvarMetadata: labelDescription ## experimentData: use &#39;experimentData(object)&#39; rw2[[1]] ## Object of class &quot;Spectrum1&quot; ## Retention time: 45:27 ## MSn level: 1 ## Total ion count: 10768 ## Polarity: 1 plot(rw[[1]]) rw2[[3]] ## Object of class &quot;Spectrum2&quot; ## Precursor: 599.7978 ## Retention time: 45:27 ## Charge: 2 ## MSn level: 2 ## Peaks count: 630 ## Total ion count: 4263413 plot(rw2[[3]], full = TRUE) rw2[[4]] ## Object of class &quot;Spectrum2&quot; ## Precursor: 452.2501 ## Retention time: 45:27 ## Charge: 2 ## MSn level: 3 ## Peaks count: 283 ## Total ion count: 58042870 plot(rw2[[4]], reporters = TMT11) 7.4 MS maps rawf &lt;- msdata::proteomics(full.names = TRUE, pattern = &quot;20141210&quot;) basename(rawf) ## [1] &quot;TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML.gz&quot; rw &lt;- readMSData(rawf, mode = &quot;onDisk&quot;) ## indeices of MS1 with retention times 30 - 35 ms1 &lt;- which(msLevel(rw) == 1) rtsel &lt;- rtime(rw)[ms1] / 60 &gt; 30 &amp; rtime(rw)[ms1] / 60 &lt; 35 M &lt;- MSmap(rw, scans = ms1[rtsel], lowMz = 521, highMz = 523, resMz = .005) ## 1 plot(M, aspect = 1, allTicks = FALSE) plot3D(M) i &lt;- ms1[which(rtsel)][1] j &lt;- ms1[which(rtsel)][2] M2 &lt;- MSmap(rw, i:j, 100, 1000, 1) ## 1 plot3D(M2) It is then possible to build maps for successive retention times or M/Z ranges and generate animations (see [here])(https://lgatto.github.io/RforProteomics/articles/RProtVis.html#visualising-mass-spectrometry-data). for code) 7.5 Peptide spetrum matches data(itraqdata) itraqdata &lt;- pickPeaks(itraqdata) i &lt;- 14 s &lt;- as.character(fData(itraqdata)[i, &quot;PeptideSequence&quot;]) calculateFragments(s) ## mz ion type pos z seq ## 1 88.03931 b1 b 1 1 S ## 2 201.12337 b2 b 2 1 SI ## 3 258.14483 b3 b 3 1 SIG ## 4 405.21324 b4 b 4 1 SIGF ## 5 534.25583 b5 b 5 1 SIGFE ## 6 591.27729 b6 b 6 1 SIGFEG ## 7 706.30423 b7 b 7 1 SIGFEGD ## 8 793.33626 b8 b 8 1 SIGFEGDS ## 9 906.42032 b9 b 9 1 SIGFEGDSI ## 10 963.44178 b10 b 10 1 SIGFEGDSIG ## 11 175.11895 y1 y 1 1 R ## 12 232.14041 y2 y 2 1 GR ## 13 345.22447 y3 y 3 1 IGR ## 14 432.25650 y4 y 4 1 SIGR ## 15 547.28344 y5 y 5 1 DSIGR ## 16 604.30490 y6 y 6 1 GDSIGR ## [ reached getOption(&quot;max.print&quot;) -- omitted 16 rows ] plot(itraqdata[[i]], s, main = s) plot(itraqdata[[41]], itraqdata[[42]]) See also the protViz package for MS2 spectra annotation and visualisation. The MSGFgui package provides an interactive interface to navigate and investigate identification data using interactive plots, summary statistics and filtering. 7.6 MS imaging There are (at least) two packages, namely and Cardinal (web site) Cardinal imaging and MALDIquant MALDIquant imaging - try the shiny app that support MS imaging. 7.7 Missing values data(naset) image2(naset) image2(is.na(naset), col = c(&quot;black&quot;, &quot;white&quot;)) naplot(naset) ## PCA plots par(mfrow = c(1, 2)) plot2D(hyperLOPIT2015, main = &quot;Spatial proteomics - features&quot;) addLegend(hyperLOPIT2015, where = &quot;bottomleft&quot;, cex = .5) plot2D(t(mulvey2015norm), fcol = &quot;times&quot;, main = &quot;Time course - samples&quot;) addLegend(t(mulvey2015norm), where = &quot;bottomright&quot;, fcol = &quot;times&quot;) "],
["interactive-visualisation.html", "Chapter 8 Interactive visualisation 8.1 Interactivity graphs 8.2 Interactive apps Motivating examples", " Chapter 8 Interactive visualisation 8.1 Interactivity graphs This section is based on the on-line ggvis documentation The goal of ggvis is to make it easy to build interactive graphics for exploratory data analysis. ggvis has a similar underlying theory to ggplot2 (the grammar of graphics), but it’s expressed a little differently, and adds new features to make your plots interactive. ggvis also incorporates shiny’s reactive programming model (see later) and dplyr’s grammar of data transformation. library(&quot;ggvis&quot;) sml &lt;- sample(nrow(surveys_complete), 1e3) surveys_sml &lt;- surveys_complete[sml, ] p &lt;- ggvis(surveys_sml, x = ~weight, y = ~hindfoot_length) p %&gt;% layer_points() surveys_sml %&gt;% ggvis(x = ~weight, y = ~hindfoot_length, fill = ~species_id) %&gt;% layer_points() p %&gt;% layer_points(fill = ~species_id) p %&gt;% layer_points(shape = ~species_id) To set fixed plotting parameters, use :=. p %&gt;% layer_points(fill := &quot;red&quot;, stroke := &quot;black&quot;) p %&gt;% layer_points(size := 300, opacity := 0.4) p %&gt;% layer_points(shape := &quot;cross&quot;) Interactivity p %&gt;% layer_points( size := input_slider(10, 100), opacity := input_slider(0, 1)) p %&gt;% layer_points() %&gt;% add_tooltip(function(df) df$weight) input_slider() input_checkbox() input_checkboxgroup() input_numeric() input_radiobuttons() input_select() input_text() See the interactivity vignette for details. Layers Simple layers layer_points(), with properties x, y, shape, stroke, fill, strokeOpacity, fillOpacity, and opacity. layer_paths(), for paths and polygons (using the fill argument). layer_ribbons() for filled areas. layer_rects(), layer_text(). Compound layers, which which combine data transformations with one or more simple layers. layer_lines() which automatically orders by the x variable with arrange(). layer_histograms() and layer_freqpolys(), which first bin the data with compute_bin(). layer_smooths(), which fits and plots a smooth model to the data using compute_smooth(). See the layers vignette for details. Like for ggplot2’s geoms, we can overly multiple layers: p %&gt;% layer_points() %&gt;% layer_smooths(stroke := &quot;red&quot;) More components scales, to control the mapping between data and visual properties; see the properties and scales vignette. legends and axes to control the appearance of the guides produced by the scales. See the axes and legends vignette. Challenge Apply a PCA analysis on the mulvey2015norm data, and use ggvis to visualise that data along PC1 and PC2, controlling the point size using a slide bar. 8.2 Interactive apps Motivating examples In this section, we show how to build interactive shiny applications. shiny is widely used to explore and visualise biomolecular data. As motivating examples, we present here two such example for proteomics data. 8.2.1 Longitudinal proteomics QC Spatial proteomics The Biocpkg(&quot;pRoloc&quot;) package offers dedicated functionality to manipulate, annotate, analyse and visualise spatial proteomics data. Its companion package Biocpkg(&quot;pRolocGUI&quot;) offers support for interactive visualisation. You can start an app to visualise one of the published data as shown below library(&quot;pRoloc&quot;) library(&quot;pRolocdata&quot;) data(hyperLOPIT2015) library(&quot;pRolocGUI&quot;) pRolocVis(hyperLOPIT2015) This same data is also available as a standalone on-line application: Introduction A useful shiny cheet sheet is available here. This section is based on RStudio shiny tutorials. From the shiny package website: Shiny is an R package that makes it easy to build interactive web apps straight from R. When using shiny, one tends to aim for more complete, long-lasting applications, rather then simply and transient visualisations. A shiny application is composed of a ui (user interface) and a server that exchange information using a programming paradigm called reactive programming: changes performed by the user to the ui trigger a reaction by the server and the output is updated accordingly. In the ui: define the components of the user interface (such as page layout, page title, input options and outputs), i.e what the user will see and interact with. In the server: defines the computations in the R backend. The reactive programming is implemented through reactive functions, which are functions that are only called when their respective inputs are changed. An application is run with the shiny::runApp() function, which takes the directory containing the two files as input. Before looking at the details of such an architecture, let’s build a simple example from scratch, step by step. This app, shown below, uses the faithful data, describing the wainting time between eruptions and the duration of the reuption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. head(faithful) ## eruptions waiting ## 1 3.600 79 ## 2 1.800 54 ## 3 3.333 74 ## 4 2.283 62 ## 5 4.533 85 ## 6 2.883 55 It shows the distribution of waiting times along a histogram (produced by the hist function) and provides a slider to adjust the number of bins (the breaks argument to hist). The app can also be opened at https://lgatto.shinyapps.io/shiny-app1/ Creation of our fist shiny app Create a directory that will contain the app, such as for example &quot;shinyapp&quot;. In this directory, create the ui and server files, named ui.R and server.R. In the ui.R file, let’s defines a (fluid) page containing a title panel with a page title; a layout containing a sidebar and a main panel library(shiny) shinyUI(fluidPage( titlePanel(&quot;My Shiny App&quot;), sidebarLayout( sidebarPanel( ), mainPanel( ) ) )) In the server.R file, we define the shinyServer function that handles input and ouputs (none at this stage) and the R logic. library(shiny) shinyServer(function(input, output) { }) Let’s now add some items to the ui: a text input widget in the sidebar and a field to hold the text ouput. library(shiny) shinyUI(fluidPage( titlePanel(&quot;My Shiny App&quot;), sidebarLayout( sidebarPanel( textInput(&quot;textInput&quot;, &quot;Enter text here:&quot;) ), mainPanel( textOutput(&quot;textOutput&quot;) ) ) )) In the server.R file, we add in the shinyServer function some R code defining how to manipulate the user-provided text and render it using a shiny textOuput. library(shiny) shinyServer(function(input, output) { output$textOutput &lt;- renderText(paste(&quot;User-entered text: &quot;, input$textInput)) }) Let’s now add a plot in the main panel in ui.R and some code to draw a histogram in server.R: library(shiny) shinyUI(fluidPage( titlePanel(&quot;My Shiny App&quot;), sidebarLayout( sidebarPanel( textInput(&quot;textInput&quot;, &quot;Enter text here:&quot;) ), mainPanel( textOutput(&quot;textOutput&quot;), plotOutput(&quot;distPlot&quot;) ) ) )) library(shiny) shinyServer(function(input, output) { output$textOutput &lt;- renderText(paste(&quot;User-entered text: &quot;, input$textInput)) output$distPlot &lt;- renderPlot({ x &lt;- faithful[, 2] hist(x) }) }) We want to be able to control the number of breaks used to plot the histograms. We first add a sliderInput to the ui for the user to specify the number of bins, and then make use of that new input to parametrise the histogram. library(shiny) shinyUI(fluidPage( titlePanel(&quot;My Shiny App&quot;), sidebarLayout( sidebarPanel( textInput(&quot;textInput&quot;, &quot;Enter text here:&quot;), sliderInput(&quot;bins&quot;, &quot;Number of bins:&quot;, min = 1, max = 50, value = 30) ), mainPanel( textOutput(&quot;textOutput&quot;), plotOutput(&quot;distPlot&quot;) ) ) )) library(shiny) shinyServer(function(input, output) { output$textOutput &lt;- renderText(paste(&quot;User-entered text: &quot;, input$textInput)) output$distPlot &lt;- renderPlot({ x &lt;- faithful[, 2] bins &lt;- seq(min(x), max(x), length.out = input$bins + 1) hist(x, breaks = bins) }) }) The next addition is to add a menu for the user to choose a set of predefined colours (that would be a selectInput) in the ui.R file and use that new input to parametrise the colour of the histogramme in the server.R file. library(shiny) shinyUI(fluidPage( titlePanel(&quot;My Shiny App&quot;), sidebarLayout( sidebarPanel( textInput(&quot;textInput&quot;, &quot;Enter text here:&quot;), sliderInput(&quot;bins&quot;, &quot;Number of bins:&quot;, min = 1, max = 50, value = 30), selectInput(&quot;col&quot;, &quot;Select a colour:&quot;, choices = c(&quot;steelblue&quot;, &quot;darkgray&quot;, &quot;orange&quot;)) ), mainPanel( textOutput(&quot;textOutput&quot;), plotOutput(&quot;distPlot&quot;) ) ) )) library(shiny) shinyServer(function(input, output) { output$textOutput &lt;- renderText(paste(&quot;User-entered text: &quot;, input$textInput)) output$distPlot &lt;- renderPlot({ x &lt;- faithful[, 2] bins &lt;- seq(min(x), max(x), length.out = input$bins + 1) hist(x, breaks = bins, col = input$col) }) }) The last addition that we want is to visualise the actual data in the main panel. We add a dataTableOutput in ui.R and generate that table in server.R using a renderDataTable rendering function. library(shiny) ## Define UI for application that draws a histogram shinyUI(fluidPage( ## Application title titlePanel(&quot;My Shiny App&quot;), ## Sidebar with text, slide bar and menu selection inputs sidebarLayout( sidebarPanel( textInput(&quot;textInput&quot;, &quot;Enter text here:&quot;), sliderInput(&quot;bins&quot;, &quot;Number of bins:&quot;, min = 1, max = 50, value = 30), selectInput(&quot;col&quot;, &quot;Select a colour:&quot;, choices = c(&quot;steelblue&quot;, &quot;darkgray&quot;, &quot;orange&quot;)) ), ## Main panel showing user-entered text, a reactive plot and a ## dynamic table mainPanel( textOutput(&quot;textOutput&quot;), plotOutput(&quot;distPlot&quot;), dataTableOutput(&quot;dataTable&quot;) ) ) )) library(shiny) ## Define server logic shinyServer(function(input, output) { output$textOutput &lt;- renderText(paste(&quot;User-entered text: &quot;, input$textInput)) ## Expression that generates a histogram. The expression is ## wrapped in a call to renderPlot to indicate that: ## ## 1) It is &quot;reactive&quot; and therefore should be automatically ## re-executed when inputs change ## 2) Its output type is a plot output$distPlot &lt;- renderPlot({ x &lt;- faithful[, 2] ## Old Faithful Geyser data bins &lt;- seq(min(x), max(x), length.out = input$bins + 1) ## draw the histogram with the specified number of bins hist(x, breaks = bins, col = input$col, border = &#39;white&#39;) }) output$dataTable &lt;- renderDataTable(faithful) }) Challenge Write and your shinyapp applications, as described above. The shiny infrastructure The overview figure below is based and makes reference to lessons of the written tutorial. Another shiny app The ui uses the fluidPage UI and a sidebar layout. The sidebar panel contains a textInput with a caption, a selectInput to choose from the three possible datasets rock, pressure or cars, and a numericInput to define how many observations to show. The main panel use a textOutput to display the caption, a verbatimOutput to show the output of the summary function on the data chosen in the selectInput above, and a tableOutput to show the head of that same data. library(&quot;shiny&quot;) # Define UI for dataset viewer application shinyUI(fluidPage( # Application title titlePanel(&quot;Reactivity&quot;), # Sidebar with controls to provide a caption, select a dataset, # and specify the number of observations to view. Note that # changes made to the caption in the textInput control are # updated in the output area immediately as you type sidebarLayout( sidebarPanel( textInput(&quot;caption&quot;, &quot;Caption:&quot;, &quot;Data Summary&quot;), selectInput(&quot;dataset&quot;, &quot;Choose a dataset:&quot;, choices = c(&quot;rock&quot;, &quot;pressure&quot;, &quot;cars&quot;)), numericInput(&quot;obs&quot;, &quot;Number of observations to view:&quot;, 10) ), # Show the caption, a summary of the dataset and an HTML # table with the requested number of observations mainPanel( h3(textOutput(&quot;caption&quot;, container = span)), verbatimTextOutput(&quot;summary&quot;), tableOutput(&quot;view&quot;) ) ) )) The server defines a reactive expression that sets the appropriate data based on the selectInput above. It produces three outputs: the caption using renderText and the caption defined above; the appropriate summary using renderPrint and the reactive data; the table using renderTable to produce the head with the reactive data and number of observations defined by the numbericInput above. library(shiny) library(datasets) # Define server logic required to summarize and view the selected # dataset shinyServer(function(input, output) { # By declaring datasetInput as a reactive expression we ensure # that: # # 1) It is only called when the inputs it depends on changes # 2) The computation and result are shared by all the callers # (it only executes a single time) # datasetInput &lt;- reactive({ switch(input$dataset, &quot;rock&quot; = rock, &quot;pressure&quot; = pressure, &quot;cars&quot; = cars) }) # The output$caption is computed based on a reactive expression # that returns input$caption. When the user changes the # &quot;caption&quot; field: # # 1) This function is automatically called to recompute the # output # 2) The new caption is pushed back to the browser for # re-display # # Note that because the data-oriented reactive expressions # below don&#39;t depend on input$caption, those expressions are # NOT called when input$caption changes. output$caption &lt;- renderText({ input$caption }) # The output$summary depends on the datasetInput reactive # expression, so will be re-executed whenever datasetInput is # invalidated # (i.e. whenever the input$dataset changes) output$summary &lt;- renderPrint({ dataset &lt;- datasetInput() summary(dataset) }) # The output$view depends on both the databaseInput reactive # expression and input$obs, so will be re-executed whenever # input$dataset or input$obs is changed. output$view &lt;- renderTable({ head(datasetInput(), n = input$obs) }) }) Challenge Using the code above, implement and run the app. Single-file app Instead of defining the ui and server in their respective files, they can be combined into list to be passed directly to runApp: ui &lt;- fluidPage(...) server &lt;- function(input, output) { ... } app &lt;- list(ui = ui, server = server) runApp(app) Challenges Create an app to visualise the mulvey2015norm data where the user can select along which features to view the data. As above, where the visualisation is a PCA plot and the user chooses the PCs. There’s more to shiny Sharing shiny apps Share the code file(s) and runApp runUrl runGitHub runGist shinyapps Shiny server (in-house) More interactivity plotOutput(&quot;pca&quot;, hover = &quot;hover&quot;, click = &quot;click&quot;, dblclick = &quot;dblClick&quot;, brush = brushOpts( id = &quot;brush&quot;, resetOnNew = TRUE)) Example here. References shiny page shiny cheat sheet "],
["session-information.html", "Chapter 9 Session information", " Chapter 9 Session information The session information below documents the packages and versions used to generate this material. sessionInfo() ## R Under development (unstable) (2018-04-02 r74505) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 14.04.5 LTS ## ## Matrix products: default ## BLAS: /usr/lib/atlas-base/atlas/libblas.so.3.0 ## LAPACK: /usr/lib/lapack/liblapack.so.3.0 ## ## locale: ## [1] LC_CTYPE=en_GB.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_GB.UTF-8 LC_COLLATE=en_GB.UTF-8 ## [5] LC_MONETARY=en_GB.UTF-8 LC_MESSAGES=en_GB.UTF-8 ## [7] LC_PAPER=en_GB.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] grid stats4 parallel stats graphics grDevices utils ## [8] datasets methods base ## ## other attached packages: ## [1] bindrcpp_0.2.2.9000 forcats_0.3.0 stringr_1.3.0 ## [4] dplyr_0.7.4.9003 purrr_0.2.4 tibble_1.4.2 ## [7] tidyverse_1.2.1 knitr_1.20 patchwork_0.0.1 ## [10] ComplexHeatmap_1.18.0 RforProteomics_1.18.0 viridis_0.5.1 ## [13] viridisLite_0.3.0 RColorBrewer_1.1-2 shiny_1.0.5 ## [16] ggvis_0.4.3 UpSetR_1.3.3 Vennerable_3.1.0.9000 ## [19] magrittr_1.5 tidyr_0.8.0 readr_1.1.1 ## [22] MSstatsBioData_1.2.0 ALL_1.22.0 pRoloc_1.20.0 ## [25] MLInterfaces_1.60.0 cluster_2.0.7-1 annotate_1.58.0 ## [28] XML_3.98-1.11 AnnotationDbi_1.42.0 IRanges_2.14.1 ## [31] S4Vectors_0.18.1 pRolocdata_1.18.0 affy_1.58.0 ## [34] MSnbase_2.6.0 ProtGenerics_1.12.0 BiocParallel_1.14.0 ## [37] mzR_2.14.0 Rcpp_0.12.16 msdata_0.20.0 ## [40] ggplot2_2.2.1.9000 bioDist_1.52.0 KernSmooth_2.23-15 ## [43] Biobase_2.40.0 BiocGenerics_0.26.0 BiocStyle_2.8.0 ## [46] limma_3.36.0 ## ## loaded via a namespace (and not attached): ## [1] prabclus_2.2-6 ModelMetrics_1.1.0 ## [3] R.methodsS3_1.7.1 bit64_0.9-7 ## [5] R.utils_2.6.0 rpart_4.1-13 ## [7] hwriter_1.3.2 RCurl_1.95-4.10 ## [9] doParallel_1.0.11 preprocessCore_1.42.0 ## [11] RSQLite_2.1.0 proxy_0.4-22 ## [13] bit_1.1-12 xml2_1.2.0 ## [15] lubridate_1.7.4 httpuv_1.4.2 ## [17] assertthat_0.2.0 gower_0.1.2 ## [19] xfun_0.1 hms_0.4.2 ## [21] evaluate_0.10.1 promises_1.0.1 ## [23] BiocInstaller_1.30.0 DEoptimR_1.0-8 ## [25] progress_1.1.2 dendextend_1.8.0 ## [27] readxl_1.1.0 igraph_1.2.1 ## [29] DBI_1.0.0 htmlwidgets_1.2 ## [31] ddalpha_1.3.1.1 crosstalk_1.0.0 ## [33] rda_1.0.2-2 backports_1.1.2 ## [35] bookdown_0.7 trimcluster_0.1-2 ## [37] biomaRt_2.36.0 caret_6.0-79 ## [39] withr_2.1.2 sfsmisc_1.1-2 ## [41] robustbase_0.93-0 rpx_1.16.0 ## [43] prettyunits_1.0.2 mclust_5.4 ## [45] mnormt_1.5-5 lazyeval_0.2.1 ## [47] crayon_1.3.4 genefilter_1.62.0 ## [49] recipes_0.1.2 pkgconfig_2.0.1 ## [51] labeling_0.3 nlme_3.1-137 ## [53] nnet_7.3-12 bindr_0.1.1 ## [55] rlang_0.2.0.9001 diptest_0.75-7 ## [57] pls_2.6-0 affyio_1.50.0 ## [59] modelr_0.1.1 cellranger_1.1.0 ## [61] randomForest_4.6-14 rprojroot_1.3-2 ## [63] graph_1.58.0 Matrix_1.2-14 ## [65] base64enc_0.1-3 whisker_0.3-2 ## [67] GlobalOptions_0.0.13 rjson_0.2.15 ## [69] bitops_1.0-6 R.oo_1.22.0 ## [71] interactiveDisplay_1.18.0 blob_1.1.1 ## [73] shape_1.4.4 DRR_0.0.3 ## [75] scales_0.5.0.9000 lpSolve_5.6.13 ## [77] memoise_1.1.0 GSEABase_1.42.0 ## [79] plyr_1.8.4 hexbin_1.27.2 ## [81] gdata_2.18.0 zlibbioc_1.26.0 ## [83] threejs_0.3.1 compiler_3.6.0 ## [85] pcaMethods_1.72.0 dimRed_0.1.0 ## [87] cli_1.0.0 Category_2.46.0 ## [89] MASS_7.3-50 tidyselect_0.2.4 ## [91] vsn_3.48.0 stringi_1.2.2 ## [93] highr_0.6 yaml_2.1.19 ## [95] MALDIquant_1.17 biocViews_1.48.0 ## [97] tools_3.6.0 circlize_0.4.3 ## [99] rstudioapi_0.7 foreach_1.4.4 ## [ reached getOption(&quot;max.print&quot;) -- omitted 54 entries ] "]
]
