---
title: "Visualisation of biomolecular data"
author: "Laurent Gatto"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
description: "Visualisation of biomolecular data"
github-repo: lgatto/VisualisingBiomolecularData
---

```{r setup, echo=FALSE, warning=FALSE}
source("./src/setup.R")
```

# Introduction

This *Visualisation of biomolecular data* course was set up as part of
the 2018 edition of the May Institute *Computation and statistics for
mass spectrometry and proteomics* at Northeastern University, Boston
MA. It is aimed at people who are already familiar with the R language
and syntax, and who would like to get a hands-on introduction to
visualisation, with a focus on biomolecular data in general, and
proteomics in particular. This course is meant to be mostly hands-on,
with an intuitive understanding of the underlying techniques.

Let's use the famous
[*Anscombe's quartet*](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)
data as a motivating example. This data is composed of 4 pairs of
values, $(x_1, y_1)$ to $(x_4, y_4)$:

```{r anscombe, echo = FALSE, results='asis'}
library("knitr")
kable(anscombe)
```

Each of these $x$ and $y$ sets have the same variance, mean and
correlation:

```{r anscombetab, echo=FALSE}
tab <- matrix(NA, 5, 4)
colnames(tab) <- 1:4
rownames(tab) <- c("var(x)", "mean(x)",
				   "var(y)", "mean(y)",
				   "cor(x,y)")

for (i in 1:4)
	tab[, i] <- c(var(anscombe[, i]),
				  mean(anscombe[, i]),
				  var(anscombe[, i+4]),
				  mean(anscombe[, i+4]),
				  cor(anscombe[, i], anscombe[, i+4]))

```

```{r anstabdisplay, echo=FALSE}
kable(tab)
```

But...

While the *residuals* of the linear regression clearly indicate
fundamental differences in these data, the most simple and
straightforward approach is *visualisation* to highlight the
fundamental differences in the datasets.

```{r anscombefig, echo=FALSE}
ff <- y ~ x

mods <- setNames(as.list(1:4), paste0("lm", 1:4))

par(mfrow = c(2, 2), mar = c(4, 4, 1, 1))
for (i in 1:4) {
	ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
	plot(ff, data = anscombe, pch = 19, xlim = c(3, 19), ylim = c(3, 13))
	mods[[i]] <- lm(ff, data = anscombe)
	abline(mods[[i]])
}
```

See also another, more recent example:
[The Datasaurus Dozen dataset](https://www.autodeskresearch.com/publications/samestats).

![The Datasaurus Dozen dataset](https://d2f99xq7vri1nk.cloudfront.net/DinoSequentialSmaller.gif)




We will focus on producing visualisation that will enable
understanding important features of biomolecular data or the effect of
their transformation and analyses. With this in mind, the beauty of
the visualisations we will produce won't be assessed by how visually
attractive the figures are, but how then help us understand and assess
the data. In the process of data exploration and data analysis, we
want to be able to quickly generate and interpret figures, to progress
in our understanding of data. It is of course important to fine tune
the graphics to make them visually appealing and improve communication
with the audience, but we won't focus on these aspects here.

An important aspect of data visualisation is data manipulation,
transformation and the format of the data. These topics will be
introduced and documented throughout the course.

A short URL for this book is http://bit.ly/biomolvis

## Installation

A set of packages that are used, either directly or indirectly are
provided below. A complete session information with all packages used
to compile this document is available at the end.

The source code for this document is available on GitHub at
https://github.com/lgatto/VisualisingBiomolecularData


```{r pkgs, eval=FALSE}
library("ggplot2")
library("MSnbase")
library("limma")
library("pRolocdata")
library("pRoloc")
library("ALL")
library("MSstatsBioData")
library("readr")
library("tidyr")
library("plyr")
library("magrittr")
library("Vennerable") ## on GitHub at js229/Vennerable
library("UpSetR")
```

## Questions

For any questions or suggestions, please open an
[issue](https://github.com/lgatto/VisualisingBiomolecularData/issues). Please
do add the output of your session information and, of possible, a
reproducible example describing your question or suggestion.


## License

This material is licensed under the [Creative Commons
Attribution-ShareAlike 4.0
License](http://creativecommons.org/licenses/by-sa/4.0/). Some content
is inspired by other sources, see the *Credit* section in the
material.

## Useful references

Gatto L, Breckels LM, Naake T, Gibb S. *Visualization of proteomics
data using R and bioconductor*. Proteomics. 2015
Apr;15(8):1375-89. doi: [10.1002/pmic.201400392](https://doi.org/10.1002/pmic.201400392).
PMID: 25690415; PMCID: PMC4510819.

Key M. *A tutorial in displaying mass spectrometry-based proteomic
data using heat maps*. BMC Bioinformatics. 2012;13 Suppl
16:S10. doi: [10.1186/1471-2105-13-S16-S10](http://dx.doi.org/10.1186/1471-2105-13-S16-S10). Epub
2012 Nov 5. PMID: 23176119; PMCID: PMC3489527.

Gatto L, Christoforou A. *Using R and Bioconductor for proteomics data
analysis*. Biochim Biophys Acta. 2014 Jan;1844(1 Pt
A):42-51. doi: [10.1016/j.bbapap.2013.04.032](http://dx.doi.org/10.1016/j.bbapap.2013.04.032). Epub
2013 May 18. PMID: 23692960.

Conway JR, Lex A, Gehlenborg N. UpSetR: an R package for the
visualization of intersecting sets and their
properties. Bioinformatics. 2017 Sep 15;33(18):2938-2940. doi:
[10.1093/bioinformatics/btx364](https://doi.org/10.1093/bioinformatics/btx364). PMID:
28645171; PMCID: PMC5870712.

<!--chapter:end:index.Rmd-->

# Example datasets and data manipulation

We will used various datasets throughout the course. These data are
briefly described below, and we will explore them through various
visualisations later. This section also introduces some techniques to
manipulate data and transorm them into different formats.

## Raw MS data

Section *Using R and Bioconductor for MS-based proteomics* shows how
to visualise raw mass spectrometry data. The raw data that will be
using come from the `r Biocexptpkg("msdata")` `r Biocpkg("MSnbase")`
packages. These data will be introduced later.

## Mulvey 2015 time course

This data comes from

> Mulvey CM, Schröter C, Gatto L, Dikicioglu D, Fidaner IB,
> Christoforou A, Deery MJ, Cho LT, Niakan KK, Martinez-Arias A,
> Lilley KS. Dynamic Proteomic Profiling of Extra-Embryonic Endoderm
> Differentiation in Mouse Embryonic Stem Cells. Stem Cells. 2015
> Sep;33(9):2712-25. doi:
> [10.1002/stem.2067](https://doi.org/10.1002/stem.2067). Epub 2015
> Jun 23. PMID: 26059426.


```{r}
library("pRolocdata")
data(mulvey2015norm)
```

This `MSnSet`, available from the `r Biocexptpkg("pRolocdata")`
package, measured the expression profiles of `r nrow(mulvey2015norm)`
proteins along 6 time points in triplicate.

```{r}
exprs(mulvey2015norm)[1:5, 1:3]
pData(mulvey2015norm)
```

## The iPRG data


```{r irpg}
iprg <- read.csv("http://bit.ly/VisBiomedDataIprgCsv")
head(iprg)
dim(iprg)
table(iprg$Condition, iprg$TechReplicate)
```
Reference:

> Choi M, Eren-Dogu ZF, Colangelo C, Cottrell J, Hoopmann MR, Kapp EA,
> Kim S, Lam H, Neubert TA, Palmblad M, Phinney BS, Weintraub ST,
> MacLean B, Vitek O. *ABRF Proteome Informatics Research Group (iPRG)
> 2015 Study: Detection of Differentially Abundant Proteins in
> Label-Free Quantitative LC-MS/MS Experiments*.  J Proteome Res. 2017
> Feb 3;16(2):945-957. doi:
> [10.1021/acs.jproteome.6b00881](https://doi.org/10.1021/acs.jproteome.6b00881)
> Epub 2017 Jan 3. PMID: 27990823.

This data is in the so-called long format. In some applications, it is
more convenient to have the data in wide format, where rows contain
the protein expression data for all samples.

Let's start by simplifying the data to keep only the relevant columns:

```{r}
head(iprg2 <- iprg[, c(1, 3, 6)])
```

We can convert the `iPRG` into a wide format with `tidyr::spread`:

```{r}
library("tidyr")
iprg3 <- spread(iprg2, key = Run, value = Intensity)
head(iprg3)
nrow(iprg3)
```

```{r echo=FALSE}
stopifnot(identical(length(unique(iprg$Protein)), nrow(iprg3)))
```

Indeed, we started with

```{r}
length(unique(iprg$Protein))
```

unique proteins, which corresponds to the number of rows in the new
wide dataset.

The long format is ideal when using `ggplot2`, as we will see in a
later chapter. The wide format has also advantages. For example, it
becomes straighforward to verify if there are proteins that haven't
been quantified in some samples.

```{r}
(k <- which(is.na(iprg3), arr.ind = dim(iprg3)))
iprg3[unique(k[, "row"]), ]
```

The opposite operation to `spread` is `gather`, also from the `tidyr`
package:

```{r}
head(iprg4 <- gather(iprg3, key = Run, value = Intensity, -Protein))
```

The two lond datasets, `iprg2` and `iprg4` are different due to the
missing values shown above.

```{r}
nrow(iprg2)
nrow(iprg4)
nrow(na.omit(iprg4))
```

which can be accounted for by removing rows with missing values by
setting `na.rm = TRUE`.

```{r}
head(iprg5 <- gather(iprg3, key = Run, value = Intensity, -Protein, na.rm = TRUE))
```

## CRC training data

This dataset comes from the `r Biocexptpkg("MSstatsBioData")` package
and was generated as follows:

```{r crctrain, eval = FALSE}
library("MSstats")
library("MSstatsBioData")
data(SRM_crc_training)
Quant <- dataProcess(SRM_crc_training)
subjectQuant <- quantification(Quant)
```

It provides quantitative information for 72 proteins, including two
standard proteins, *AIAG-Bovine* and *FETUA-Bovine*. These proteins
were targeted for plasma samples with SRM with isotope labeled
reference peptides in order to identify candidate protein biomarker
for non-invasive detection of CRC. The training cohort included 100
subjects in control group and 100 subjects with CRC. Each sample for
subject was measured in a single injection without technical
replicate. The training cohort was analyzed with Skyline. The dataset
was already normalized as described in manuscript. User do not need
extra normalization. NAs should be considered as censored missing. Two
standard proteins can be removed for statistical analysis.

Clinical information where added manually thereafter.

To load this dataset:

```{r crcread}
crcdf <- read.csv("http://bit.ly/VisBiomedDataCrcCsv")
crcdf[1:10, 1:3]
```

This dataset is in the wide format. It contains the intensity of the
proteins in columns 1 to 72 for each of the 200 samples along the
rows. Generally, omics datasets contain the features (proteins,
transcripts, ...) along the rows and the samples along the columns.

In columns 73 to 79, we sample metadata.

```{r}
crcdf[1:10, 73:79]
```

A widely used data structure for omics data follows the convention
described in the figure below:

![An eSet-type of expression data container](./img/msnset.png)

This typical omics data structure, as defined by the `eSet` class in
the Bioconductor `Biobase` package, is represented below. It’s main
features are

-  An assay data slot containing the quantitative omics data
   (expression data), stored as a `matrix` and accessible with
   `exprs`. Features defined along the rows and samples along the
   columns.

- A sample metadata slot containing sample co-variates, stored as an
  annotated `data.frame` and accessible with `pData`. This data frame
  is stored with rows representing samples and sample covariate along
  the columns, and its rows match the expression data columns exactly.

- A feature metadata slot containing feature co-variates, stored as an
  annotated `data.frame` and accessible with `fData`. This dataframe’s
  rows match the expression data rows exactly.

The coordinated nature of the high throughput data guarantees that the
dimensions of the different slots will always match (i.e the columns
in the expression data and then rows in the sample metadata, as well
as the rows in the expression data and feature metadata) during data
manipulation. The metadata slots can grow additional co-variates
(columns) without affecting the other structures.

Below, we show how to transform the `crc` dataset into an `MSnSet`
(implementing the data structure above for quantitative proteomics
data) using the `readMSnSet2` function.

```{r crcmsnset}
library("MSnbase")
i <- 1:72 ## expression columns
e <- t(crcdf[, i]) ## expression data
colnames(e) <- 1:200
crc <- readMSnSet2(data.frame(e), e = 1:200)

pd <- crcdf[, -i]
rownames(pd) <- paste0("X", rownames(pd))
pData(crc) <- pd
crc
```

Or load it direcly:

```{r crcload}
download.file("http://bit.ly/VisBiomedDataCrcMSnSet", "./data/crc.rda")
load("./data/crc.rda")
crc
```

Reference:

> See Surinova, S. et al. (2015) *Prediction of colorectal cancer
> diagnosis based on circulating plasma proteins*. EMBO Mol. Med., 7,
> 1166–1178 for details.

## ALL data

```{r all}
library("ALL")
data(ALL)
ALL
```

From the documentation page:

> The *Acute Lymphoblastic Leukemia Data from the Ritz Laboratory*
> consist of microarrays from 128 different individuals with acute
> lymphoblastic leukemia (ALL). A number of additional covariates are
> available. The data have been normalized (using rma) and it is the
> jointly normalized data that are available here.

The `ALL` data is of class `ExpressionSet`, which implements the data
structure above for microarray expression data, and contains
normalised and summarised transcript intensities.

Below, we select will patients with B-cell lymphomas and BCR/ABL
abnormality and negative controls.

```{r}
table(ALL$BT)
table(ALL$mol.biol)
ALL_bcrneg <- ALL[, ALL$mol.biol %in% c("NEG", "BCR/ABL") & grepl("B", ALL$BT)]
ALL_bcrneg$mol.biol <- factor(ALL_bcrneg$mol.biol)
```

We then use the `r Biocpkg("limma")` package to

```{r limma}
library("limma")
design <- model.matrix(~0+ALL_bcrneg$mol.biol)
colnames(design) <- c("BCR.ABL", "NEG")

## Step1: linear model. lmFit is a wrapper around lm in R
fit1 <- lmFit(ALL_bcrneg, design)

## Step 2: fit contrasts: find genes that respond to estrogen
contrast.matrix <- makeContrasts(BCR.ABL-NEG, levels = design)
fit2 <- contrasts.fit(fit1, contrast.matrix)

## Step3: add empirical Bayes moderation
fit3 <- eBayes(fit2)

## Extract results and set them to the feature data
res <- topTable(fit3, n = Inf)
fData(ALL_bcrneg) <- res[featureNames(ALL_bcrneg), ]
```

```{r echo=FALSE}
if (!file.exists("./data/ALL_bcrneg.rda"))
	save(ALL_bcrneg, file = "./data/ALL_bcrneg.rda")
```

This annotated `ExpressionSet` can be reproduced as shown above or
downloaded and loaded using

```{r}
download.file("http://bit.ly/VisBiomedDataALL_bcrneg",  "./data/ALL_bcrneg.rda")
load("./data/ALL_bcrneg.rda")
```

Reference:

> Sabina Chiaretti, Xiaochun Li, Robert Gentleman, Antonella Vitale,
> Marco Vignetti, Franco Mandelli, Jerome Ritz, and Robin Foa *Gene
> expression profile of adult T-cell acute lymphocytic leukemia
> identifies distinct subsets of patients with different response to
> therapy and survival*. Blood, 1 April 2004, Vol. 103, No. 7.


## Using `dplyr` for data manipulation

> The following material is based on Data Carpentry's the [Data
> analisis and
> visualisation](http://www.datacarpentry.org/R-ecology-lesson/)
> lessons.


Bracket subsetting is handy, but it can be cumbersome and difficult to
read, especially for complicated operations. Enter
**`dplyr`**. **`dplyr`** is a package for making tabular data
manipulation easier. It pairs nicely with **`tidyr`** which enables
you to swiftly convert between different data formats for plotting and
analysis.

We will need the [`tidyverse`](https://www.tidyverse.org/)
package. This is an "umbrella-package" that installs several packages
useful for data analysis which work together well such as `dplyr`,
`ggplot2` (for visualisation), `tibble`, etc.

```{r}
library("tidyverse")
```
The `tidyverse` package tries to address 3 major problems with some of
base R functions:

1. The results from a base R function sometimes depends on the type of
   data.
2. Using R expressions in a non standard way, which can be confusing
   for new learners.
3. Hidden arguments, having default operations that new learners are
   not aware of.


To learn more about **`dplyr`** and **`tidyr`**, you may want to check
out this [handy data transformation with **`dplyr`**
cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf)
and this [one about
**`tidyr`**](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf).

Let's start by reading the data using `readr::read_csv` that will
produce a `tibble`.

```{r,results = 'hide', purl = FALSE}
library("readr")
iprg <- read_csv("http://bit.ly/VisBiomedDataIprgCsv")
```

Tibbles are data frames, but they tweak some of the old behaviors of
data frames. The data structure is very similar to a data frame. For
our purposes the only differences are that:

1. In addition to displaying the data type of each column under its
   name, it only prints the first few rows of data and only as many
   columns as fit on one screen.
2. Columns of class `character` are never converted into factors.

### Selecting columns and filtering rows

We're going to learn some of the most common **`dplyr`** functions:
`select()`, `filter()`, `mutate()`, `group_by()`, and
`summarize()`. To select columns of a data frame, use `select()`. The
first argument to this function is the data frame, and the subsequent
arguments are the columns to keep.

```{r, results = 'hide', purl = FALSE}
select(iprg, Protein, Run, Condition)
```
To choose rows based on a specific criteria, use `filter()`:

```{r, purl = FALSE}
filter(iprg, BioReplicate == 1)
```

```{r, purl = FALSE}
filter(iprg, Condition == 'Condition2')
```

### Pipes

But what if you wanted to select and filter at the same time? There
are three ways to do this: use intermediate steps, nested functions,
or pipes.

With intermediate steps, you essentially create a temporary data frame
and use that as input to the next function. This can clutter up your
workspace with lots of objects. You can also nest functions (i.e. one
function inside of another).  This is handy, but can be difficult to
read if too many functions are nested as things are evaluated from the
inside out.

The last option, pipes, are a fairly recent addition to R. Pipes let
you take the output of one function and send it directly to the next,
which is useful when you need to do many things to the same dataset.
Pipes in R look like `%>%` and are made available via the `magrittr`
package, installed automatically with **`dplyr`**. If you use RStudio,
you can type the pipe with <kbd>Ctrl</kbd>
+ <kbd>Shift</kbd> + <kbd>M</kbd> if you have a PC or <kbd>Cmd</kbd> +
<kbd>Shift</kbd> + <kbd>M</kbd> if you have a Mac.

```{r, purl = FALSE}
iprg %>%
  filter(Intensity > 1e8) %>%
  select(Protein, Condition, Intensity)
```

In the above, we use the pipe to send the `iprg` dataset first through
`filter()` to keep rows where `Intensity` is greater than 1e8, then
through `select()` to keep only the `Protein`, `Condition`, and
`Intensity` columns. Since `%>%` takes the object on its left and
passes it as the first argument to the function on its right, we don't
need to explicitly include it as an argument to the `filter()` and
`select()` functions anymore.

If we wanted to create a new object with this smaller version of the
data, we could do so by assigning it a new name:

```{r, purl = FALSE}
iprg_sml <- iprg %>%
	filter(Intensity > 1e8) %>%
	select(Protein, Condition, Intensity)

iprg_sml
```

Note that the final data frame is the leftmost part of this expression.

> **Challenge**
>
>  Using pipes, subset the `iprg` data to include Proteins with a log2
>  intensity greater than 20 and retain only the columns `Proteins`,
>  and `Condition`.


<details>
```{r, eval=FALSE, purl=FALSE}
## Answer
iprg %>%
	filter(Log2Intensity > 20) %>%
	select(Protein, Condition)
```
</details>


### Mutate

Frequently you'll want to create new columns based on the values in existing
columns, for example to do unit conversions, or find the ratio of values in two
columns. For this we'll use `mutate()`.

To create a new column of weight in kg:

```{r, purl = FALSE}
iprg %>%
  mutate(Log10Intensity = log10(Intensity))
```

You can also create a second new column based on the first new column
within the same call of `mutate()`:

```{r, purl = FALSE}
iprg %>%
	mutate(Log10Intensity = log10(Intensity),
		   Log10Intensity2 = Log10Intensity * 2)
```

If this runs off your screen and you just want to see the first few
rows, you can use a pipe to view the `head()` of the data. (Pipes work
with non-**`dplyr`** functions, too, as long as the **`dplyr`** or
`magrittr` package is loaded).

```{r, purl = FALSE}
iprg %>%
  mutate(Log10Intensity = log10(Intensity)) %>%
  head
```

Note that we don't include parentheses at the end of our call to `head()` above.
When piping into a function with no additional arguments, you can call the
function with or without parentheses (e.g. `head` or `head()`).

If you want to display more data, you can use the `print()` function
at the end of your chain with the argument `n` specifying the number
of rows to display:


```{r, purl = FALSE}
iprg %>%
	mutate(Log10Intensity = log10(Intensity),
			   Log10Intensity2 = Log10Intensity * 2) %>%
	print(n = 20)
```

Let's use a modified `iprg` data that contains missing values for the
next example. It can be loaded with

```{r}
download.file("http://bit.ly/VisBiomedDataIprgNA", "./data/iprgna.rda")
load("./data/iprgna.rda")
```


> **Challenge**
>
> Using the `iprgna` data repeat the creation of a new
> `Log10Intensisty` column.

<details>
```{r}
iprgna %>% mutate(Log10Intensity = log10(Intensity))
```
</details>

The first few rows of the output are full of `NA`s, so if we wanted to remove
those we could insert a `filter()` in the chain:

```{r, purl = FALSE}
iprgna %>%
	filter(!is.na(Intensity)) %>%
	mutate(Log10Intensity = log10(Intensity))
```

`is.na()` is a function that determines whether something is an
`NA`. The `!` symbol negates the result, so we're asking for
everything that *is not* an `NA`.

### Split-apply-combine data analysis and the summarize() function

Many data analysis tasks can be approached using the
*split-apply-combine* paradigm: split the data into groups, apply some
analysis to each group, and then combine the results. **`dplyr`**
makes this very easy through the use of the `group_by()` function.


#### The `summarize()` function

`group_by()` is often used together with `summarize()`, which
collapses each group into a single-row summary of that group.
`group_by()` takes as arguments the column names that contain the
**categorical** variables for which you want to calculate the summary
statistics. So to view the mean `weight` by sex:

```{r, purl = FALSE}
iprgna %>%
  group_by(Condition) %>%
  summarize(mean_Intensity = mean(Intensity))
```

Unfortunately, the `mean` of any vector that contains even a single
missing value is `NA`. We need to remove missing values before
calculating the mean, which is done easily with the `na.rm` argument.

```{r, purl = FALSE}
iprgna %>%
  group_by(Condition) %>%
  summarize(mean_Intensity = mean(Intensity, na.rm = TRUE))
```


You can also group by multiple columns:

```{r, purl = FALSE}
iprgna %>%
  group_by(TechReplicate, BioReplicate) %>%
  summarize(mean_Intensity = mean(Intensity, na.rm = TRUE))
```

#### Tallying

When working with data, it is also common to want to know the number of
observations found for each factor or combination of factors. For this, **`dplyr`**
provides `tally()`.

```{r, purl = FALSE}
iprgna %>%
  group_by(Condition) %>%
  tally
```

Here, `tally()` is the action applied to the groups created by `group_by()` and
counts the total number of records for each category.

> **Challenge**
>
> 1. How many proteins of each technical replicate are there?
>
> 2. Use `group_by()` and `summarize()` to find the mean, min, and max intensity
> for each condition.
>
> 3. What are the proteins with the highest intensity in each
> condition?


<details>
```{r}
## Answer 1
iprgna %>%
	group_by(TechReplicate) %>%
	tally

## Answer 2
iprgna %>%
	filter(!is.na(Intensity)) %>%
	group_by(Condition) %>%
	summarize(mean_int = mean(Intensity),
				  min_int = min(Intensity),
				  max_int = max(Intensity))

## Answer 3
iprgna %>%
	filter(!is.na(Intensity)) %>%
	group_by(Condition) %>%
	filter(Intensity == max(Intensity)) %>%
	arrange(Intensity)
```
</details>

<!--chapter:end:10-data.Rmd-->

# Exploring and visualising biomolecular data

This chapter provides an overview of a typical exploratory data
analysis, statistical analysis and their associated visualisations. We
will look into the respective figures and how to produce them in the
later chapters.

<!--chapter:end:20-eda.Rmd-->

# Plotting in R

## Base graphics

We won't go through all base graphics plotting functions one by one
here. We will encounter and learn several of these functions
throughout the course and, if necessary, discuss them when questions
arise.

## Plotting with `ggplot2`

Base graphics uses a *canvas model* a series of instructions that
sequentially fill the plotting canvas. While this model is very useful
to build plots bits by bits bottom up, which is useful in some cases,
it has some clear drawback:

* Layout choices have to be made without global overview over what may
  still be coming.
* Different functions for different plot types with different
  interfaces.
* No standard data input.
* Many routine tasks require a lot of boilerplate code.
* No concept of facets/lattices/viewports.
* Poor default colours.

The `ggplot2` package implements a grammar of graphics. Users describe
what and how to visualise data and the package then generates the
figure. The components of `ggplot2`'s of graphics are

1. A **tidy** dataset
2. A choice of geometric objects that servers as the visual
   representation of the data - for instance, points, lines,
   rectangles, contours.
3. A description of how the variables in the data are mapped to visual
   properties (aesthetics) or the geometric objects, and an associated
   scale (e.g. linear, logarithmic, polar)
4. A statistical summarisation rule
5. A coordinate system.
6. A facet specification, i.e. the use of several plots to look at the
   same data.



Fist of all, we need to load the `ggplot2` package and load the `iprg` data.

```{r}
library("ggplot2")
iprg <- read.csv("http://bit.ly/VisBiomedDataIprgCsv")
```

ggplot graphics are built step by step by adding new elements.

To build a ggplot we need to:

* bind the plot to a specific data frame using the `data` argument

```{r, eval=FALSE}
ggplot(data = iprg)
```

* define aesthetics (`aes`), by selecting the variables to be plotted
  and the variables to define the presentation such as plotting size,
  shape color, etc.

```{r, eval=FALSE}
ggplot(data = iprg, aes(x = Run, y = Log2Intensity))
```

* add `geoms` -- graphical representation of the data in the plot
	 (points, lines, bars). To add a geom to the plot use `+` operator

```{r}
ggplot(data = iprg, aes(x = Run, y = Log2Intensity)) +
  geom_boxplot()
```

See the [documentation page](http://ggplot2.tidyverse.org/reference/)
to explore the many available `geoms`.


The `+` in the `ggplot2` package is particularly useful because it
allows you to modify existing `ggplot` objects. This means you can
easily set up plot "templates" and conveniently explore different
types of plots, so the above plot can also be generated with code like
this:

```{r, eval=FALSE}
## Assign plot to a variable
ints_plot <- ggplot(data = iprg, aes(x = Run, y = Log2Intensity))

## Draw the plot
ints_plot + geom_boxplot()
```

Notes:

* Anything you put in the `ggplot()` function can be seen by any geom layers
  that you add (i.e., these are universal plot settings). This includes the x and
  y axis you set up in `aes()`.

* You can also specify aesthetics for a given geom independently of the
  aesthetics defined globally in the `ggplot()` function.

* The `+` sign used to add layers must be placed at the end of each
  line containing a layer. If, instead, the `+` sign is added in the
  line before the other layer, `ggplot2` will not add the new layer
  and will return an error message.


> **Challenge**
>
> * Repeat the plot above but displaying the raw intensities.
> * Log-10 transform the raw intensities on the flight when plotting.

<details>
```{r}
ggplot(data = iprg, aes(x = Run, y = Intensity)) + geom_boxplot()
ggplot(data = iprg, aes(x = Run, y = log10(Intensity))) + geom_boxplot()
```
</details>


## Customising plots

First, let's colour the boxplot based on the condition:

```{r}
ggplot(data = iprg,
	   aes(x = Run, y = Log2Intensity,
		   fill = Condition)) +
  geom_boxplot()
```

Now let's rename all axis labels and title, and rotate the x-axis
labels 90 degrees. We can add those specifications using the `labs`
and `theme` functions of the `ggplot2` package.

```{r}
ggplot(aes(x = Run, y = Log2Intensity, fill = Condition),
	   data = iprg) +
	geom_boxplot() +
	labs(title = 'Log2 transformed intensity distribution per MS run',
		 y = 'Log2(Intensity)',
		 x = 'MS run') +
	theme(axis.text.x = element_text(angle = 90))
```


And easily switch from a boxplot to a violin plot representation by
changing the `geom` type.

```{r}
ggplot(aes(x = Run, y = Log2Intensity, fill = Condition),
	   data = iprg) +
	geom_violin() +
	labs(title = 'Log2 transformed intensity distribution per Subject',
		 y = 'Log2(Intensity)',
		 x = 'MS run') +
	theme(axis.text.x = element_text(angle = 90))
```

Finally, we can also overlay multiple geoms by simply *adding* them
one after the other.

```{r}
p <- ggplot(aes(x = Run, y = Log2Intensity, fill = Condition),
			data = iprg)
p + geom_boxplot()
p + geom_boxplot() + geom_jitter() ## not very usefull
p + geom_jitter() + geom_boxplot()
p + geom_jitter(alpha = 0.1) + geom_boxplot()
```

> **Challenge**
>
> * Overlay a boxplot goem on top of a jitter geom for the raw or
>   log-10 transformed intensities.
> * Customise the plot as suggested above.

<details>
```{r}
## Note how the log10 transformation is applied to both geoms
ggplot(data = iprg, aes(x = Run, y = log10(Intensity))) +
	geom_jitter(alpha = 0.1) +
	geom_boxplot()
```
</details>

Finally, a very useful feature of `ggplot2` is **facetting**, that
defines how to subset the data into different *panels* (facets).

```{r}
names(iprg)
ggplot(data = iprg,
	   aes(x = TechReplicate, y = Log2Intensity,
		   fill = Condition)) +
	geom_boxplot() +
	facet_grid(~ Condition)
```

## Saving your figures

You can save plots to a number of different file formats. PDF is by
far the most common format because it's lightweight, cross-platform
and scales up well but jpegs, pngs and a number of other file formats
are also supported. Let's redo the last barplot but save it to the
file system this time.

Let's save the boxplot as pdf file.

```{r, eval=FALSE}
pdf()
p + geom_jitter(alpha = 0.1) + geom_boxplot()
dev.off()
```

The default file name is `Rplots.pdf`. We can customise that file name
specifying it by passing the file name, as a character, to the `pdf()`
function.

> **Challenge**
>
> Save a figure of your choice to a pdf file. Read the manual for the
> `png` function and save that same image to a png file.
>
> **Tip**: save your figures in a dedicated directory.


## References

- [`ggplot2` extensions - gallery](http://www.ggplot2-exts.org/gallery/)
- [`ggplot2` webpage](http://ggplot2.tidyverse.org/) and documentation
- *ggplot2: Elegant Graphics for Data Analysis* book (source of the
  book available for free [here](t
  https://github.com/hadley/ggplot2-book)).

<!--chapter:end:30-plotting.Rmd-->

# Tools and plots

## Transformations

### Expression data

Let's start with the comparison of two vectors of matching expression
intensities such as those from two samples in the `iprg3`
dataset. Let's extract the intensities of samples `r names(iprg3)[2]`
(second column) and `r names(iprg3)[3]` (third column) and produce a
scatter plot of one against the other.

```{r trans1}
x <- iprg3[[2]]
y <- iprg3[[3]]
plot(x, y)
```

Due to the distribution of the raw intensities, where most of the
intensities are low with very few high intensities (see density plots
below), the majority of points are squeezed close to the origin of the
scatter plot.

```{r denstrans1}
plot(density(na.omit(x)), col = "blue")
lines(density(na.omit(y)), col = "red")
```

This has negative effects as it (1) leads to overplotting in the low
intensity range and (2) gives too much confidence in the correlation
of the two vectors. A simple way to avoid this effect is to directly
log-tranform the data or set the graph axes to log scales:

```{r plotrans1log}
par(mfrow = c(1, 2))
plot(log10(x), log10(y))
plot(x, y, log = "xy")
```

We will see better visualisations to detect correlation between sample
replicates below.

It is possible to generalise to production of scatter plots to more
samples using the `pairs` function:

```{r pairs}
pairs(iprg3[2:6], log = "xy")
```

A lot of space is wasted by repeating the same sets of plots in the
upper right and lower left triangles of the matrix. See the `pairs`
documentation page.

A general technique to overcome overplotting is to set the alpha scale
(transparency), of to use `graphics::smoothScatter`:

```{r smoothscatter}
par(mfrow = c(1, 2))
plot(x, y, pch = 19, col = "#00000010", log = "xy")
smoothScatter(log10(x), log10(y))
```

### Fold changes

Log-transformation also comes handy when computing fold-changes. Below
we calculate the fold-changes and log2 fold-changes (omitting missing
values)

```{r fc}
fc <- na.omit(iprg3[[2]] / iprg3[[3]])
lfc <- log2(fc)
```

Below, we see how the log2 fold-changes become symmetrical around zero
(the absence of change), with positive values corresponding to
up-regulation and negative values to down-regulation.

```{r lfcplot}
plot(density(lfc), ylim = c(0, 5))
abline(v = median(lfc))
lines(density(fc), col = "red")
abline(v = median(fc), col = "red")
```

**Note**: when the data is already log-transformed, log fold-changes
are computed by subtracting values.

## Comparing samples and linear models

Let's return to the scatter plot example above and focus on three
replicates from consitions 1 and 4, remove missing values and
log-tranform the intensites.


```{r linmod1}
x <- log2(na.omit(iprg3[, c(2, 3, 11)]))
```

Below, we use the pairs function and print the pairwise correlations
in the upper right traingle.

```{r linmodpairs}
## put (absolute) correlations on the upper panels,
## with size proportional to the correlations.
## From ?pairs
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...) {
	usr <- par("usr"); on.exit(par(usr))
	par(usr = c(0, 1, 0, 1))
	r <- abs(cor(x, y))
	txt <- format(c(r, 0.123456789), digits = digits)[1]
	txt <- paste0(prefix, txt)
	if (missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
	text(0.5, 0.5, txt, cex = cex.cor * r)
}
pairs(x, lower.panel = panel.smooth, upper.panel = panel.cor)
```

It is often assumed that high correlation is a halmark of good
replication. Rather than focus on the correlation of the data, a
better measurement would be to look a the log2 fold-changes, i.e. the
distance between repeated measurements. The ideal way to visualise
this is on an MA-plot:


```{r ma, fig.width = 12}
par(mfrow = c(1, 2))
r1 <- x[[1]]
r2 <- x[[2]]
M <- r1 - r2
A <- (r1 + r2)/2
plot(A, M); grid()
library("affy")
affy::ma.plot(A, M)
```

See also this
[post](http://simplystatistics.org/2015/08/12/correlation-is-not-a-measure-of-reproducibility/)
on the *Simply Statistics* blog.

`abline(0, 1)` can be used to add a line with intercept 0 and
slop 1. It we want to add the line that models the data linearly, we
can calculate the parameters using the `lm` function:

```{r lmod}
lmod <- lm(r2 ~ r1)
summary(lmod)
```

which can be used to add the adequate line that reflects the (linear)
relationship between the two data

```{r lmodplot}
plot(r1, r2)
abline(lmod, col = "red")
```

As we have seen in the beginning of this section, it is essential not
to rely solely on the correlation value, but look at the data. This
also holds true for linear (or any) modelling, which can be done by
plotting the model:

```{r lmodplot2}
par(mfrow = c(2, 2))
plot(lmod)
```

* *Cook's distance* is a commonly used estimate of the influence of a
  data point when performing a least-squares regression analysis and
  can be used to highlight points that particularly influence the
  regression.

* *Leverage* quantifies the influence of a given observation on the
  regression due to its location in the space of the inputs.

See also `?influence.measures`.

> **Challenge**
>
> 1. Take any of the `iprg3` replicates, model and plot their linear
>    relationship.
> 2. The Anscombe quartet is available as `anscombe`. Load it, create
>    a linear model for one $(x_i, y_i)$ pair of your choice and
>    visualise/check the model.

<details>
```{r anscombechallenge}
x3 <- anscombe[, 3]
y3 <- anscombe[, 7]
lmod <- lm(y3 ~ x3)
summary(lmod)
par(mfrow = c(2, 2))
plot(lmod)
```
</details>

## Plots for statistical analyses

Let's use the `ALL_bclneg` dataset, that we already have analysed with
`r Biocpkg("limma")` in the *Data* chapter.

Whenever performing a statistical test, it is important to quality
check the distribution of non-adjusted p-values. Below, we see an
enrichment of small p-values, as opposed to a uniform distribution to
be expected under the null hypothesis of absence of changes between
groups.

```{r}
fvarLabels(ALL_bcrneg)
hist(fData(ALL_bcrneg)$P.Value)
```

The histograms below illustrate [other
distributions](http://varianceexplained.org/statistics/interpreting-pvalue-histogram/)
to beware of.

![](./img/plot_melted-1.png)

Another important visualisation for statistical results are the
*Volcano plots*, that show the relationship between the significance
of the individual tests (adjusted p-values) and their magnitude of the
effect (log2 fold-changes).


```{r}
lfc <- fData(ALL_bcrneg)$logFC
bh <- fData(ALL_bcrneg)$adj.P.Val
plot(lfc, -log10(bh),
	 main = "Volcano plot",
	 xlab = expression(log[2]~fold-change),
	 ylab = expression(-log[10]~adjusted~p-value))
grid()
```

The volcano plot can further be annotated using vertical and
horizontal lines depicting thresholds of interest or points can be
colour-coded based on their interest.

```{r}
lfc <- fData(ALL_bcrneg)$logFC
bh <- fData(ALL_bcrneg)$adj.P.Val
sign <- abs(lfc) > 1 & bh < 0.01
plot(lfc, -log10(bh),
	 main = "Volcano plot",
	 col = ifelse(sign, "red", "black"),
	 pch = ifelse(sign, 19, 1),
	 xlab = expression(log[2]~fold-change),
	 ylab = expression(-log[10]~adjusted~p-value))
grid()
abline(v = c(-1, 1), lty = "dotted")
abline(h = -log10(0.05), lty = "dotted")
```

It is also possible to identify and label individual points on the
plot using the `identify` function

```{r, eval = FALSE}
i <- identify(lfc, -log10(bh), featureNames(ALL_bcrneg))
```

```{r, echo=FALSE}
lfc <- fData(ALL_bcrneg)$logFC
bh <- fData(ALL_bcrneg)$adj.P.Val
sign <- abs(lfc) > 1 & bh < 0.01
plot(lfc, -log10(bh),
	 main = "Volcano plot",
	 col = ifelse(sign, "red", "black"),
	 pch = ifelse(sign, 19, 1),
	 xlab = expression(log[2]~fold-change),
	 ylab = expression(-log[10]~adjusted~p-value))
grid()
abline(v = c(-1, 1), lty = "dotted")
abline(h = -log10(0.05), lty = "dotted")
text(lfc[i], -log10(bh)[i], featureNames(ALL_bcrneg)[i], pos = 2)
```

## Visualising intersections

Venn and Euler diagrams are popular representation when comparing sets
and their intersection. Two useful R packages to generate such plots
are `r CRANpkg("venneuler")` and `r Githubpkg("js229/Vennerable")`.

We will use the `crc` feature names to generate a test data:

```{r}
set.seed(123)
x <- replicate(3, sample(featureNames(crc), 35), simplify = FALSE)
names(x) <- LETTERS[1:3]
(v <- Venn(x))
plot(v)
```

The `r CRANpkg("UpSetR")` visualises intersections of sets as a matrix
in which the rows represent the sets and the columns represent their
intersection sizes. For each set that is part of a given intersection,
a black filled circle is placed in the corresponding matrix cell. If a
set is not part of the intersection, a light gray circle is shown. A
vertical black line connects the topmost black circle with the
bottom most black circle in each column to emphasise the column-based
relationships. The size of the intersections is shown as a bar chart
placed on top of the matrix so that each column lines up with exactly
one bar. A second bar chart showing the size of the each set is shown
to the left of the matrix.

We will first make use of the `fromList` function to convert our list
to a `UpSetR` compatible input and then generate the figure:

```{r}
library("UpSetR")
x2 <- fromList(x)
upset(x2)
```

The following
[tweet](https://twitter.com/ngehlenborg/status/986354574989709312) by
the author of the package illustrates how Venn and upset diagrams
relate to each other.


```{r}
upset(x2, order.by = "freq")
upset(x2, order.by = "degree")
upset(x2, order.by = c("freq", "degree"))
upset(x2, order.by = c("degree", "freq"))

upset(x2, sets = c("A", "B"))
upset(x2, sets = c("B", "C", "A"), keep.order = TRUE)

upset(x2, group.by = "sets")

## Add set D with a single intersection
x3 <- x2
x3$D <- 0
x3[1, "D"] <- 1
head(x3)

upset(x3)
upset(x3, empty.intersections = "on")
```

Visualising intersections with `UpSetR` shines with more that 4 sets,
as Venn diagrams become practically useless.


> **Challenge**
>
> Generate a bigger dataset containing 10 sets. Try to generate Venn
> and upset diagrams as shown above.
>
> ```{r}
> set.seed(123)
> x <- replicate(10, sample(featureNames(crc), 35), simplify = FALSE)
> names(x) <- LETTERS[1:10]
> ```

When the number of sets become larger, the options above, as well as
`nsets`, the number of sets (default is 5) and `nintersects`, the
number of intersectios (default is 40) becomes useful.

## Unsupervised learning

In **unsupervised learning** (UML), no labels are provided, and the
learning algorithm focuses solely on detecting structure in unlabelled
input data. One generally differentiates between

- **Clustering**, where the goal is to find homogeneous subgroups
  within the data; the grouping is based on distance between
  observations.

- **Dimensionality reduction**, where the goal is to identify patterns in
  the features of the data. Dimensionality reduction is often used to
  facilitate visualisation of the data, as well as a pre-processing
  method before supervised learning.

UML presents specific challenges and benefits:

- there is no single goal in UML
- there is generally much more unlabelled data available than labelled
  data.

Unsupervised learning techniques are paramount for exploratory data
analysis and visualisation.

## k-means clustering

The k-means clustering algorithms aims at partitioning *n*
observations into a fixed number of *k* clusters. The algorithm will
find homogeneous clusters.

In R, we use

```{r, eval=FALSE}
stats::kmeans(x, centers = 3, nstart = 10)
```

where

- `x` is a numeric data matrix
- `centers` is the pre-defined number of clusters
- the k-means algorithm has a random component and can be repeated
  `nstart` times to improve the returned model

> Challenge:
>
> - To learn about k-means, let's use the `iris` dataset with the sepal and
>   petal length variables only (to facilitate visualisation). Create
>   such a data matrix and name it `x`

```{r solirisx, echo=FALSE}
i <- grep("Length", names(iris))
x <- iris[, i]
```

> - Run the k-means algorithm on the newly generated data `x`, save
>   the results in a new variable `cl`, and explore its output when
>   printed.

```{r solkmcl, echo=FALSE}
cl <- kmeans(x, 3, nstart = 10)
```

> - The actual results of the algorithms, i.e. the cluster membership
>   can be accessed in the `clusters` element of the clustering result
>   output. Use it to colour the inferred clusters to generate a figure
>   like that shown below.

```{r solkmplot, echo=FALSE, fig.cap = "k-means algorithm on sepal and petal lengths"}
plot(x, col = cl$cluster)
```

<details>
```{r soliris, eval=FALSE}
i <- grep("Length", names(iris))
x <- iris[, i]
cl <- kmeans(x, 3, nstart = 10)
plot(x, col = cl$cluster)
```
</details>

### How does k-means work {-}

**Initialisation**: randomly assign class membership


```{r kmworksinit, fig.cap="k-means random intialisation"}
set.seed(12)
init <- sample(3, nrow(x), replace = TRUE)
plot(x, col = init)
```

**Iteration**:

1. Calculate the centre of each subgroup as the average position of
   all observations is that subgroup.
2. Each observation is then assigned to the group of its nearest
   centre.

It's also possible to stop the algorithm after a certain number of
iterations, or once the centres move less than a certain distance.

```{r kmworksiter, fig.width = 12, fig.cap="k-means iteration: calculate centers (left) and assign new cluster membership (right)"}
par(mfrow = c(1, 2))
plot(x, col = init)
centres <- sapply(1:3, function(i) colMeans(x[init == i, ], ))
centres <- t(centres)
points(centres[, 1], centres[, 2], pch = 19, col = 1:3)

tmp <- dist(rbind(centres, x))
tmp <- as.matrix(tmp)[, 1:3]

ki <- apply(tmp, 1, which.min)
ki <- ki[-(1:3)]

plot(x, col = ki)
points(centres[, 1], centres[, 2], pch = 19, col = 1:3)
```

**Termination**: Repeat iteration until no point changes its cluster
membership.

![k-means convergence (credit Wikipedia)](https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif)

### Model selection  {-}

Due to the random initialisation, one can obtain different clustering
results. When k-means is run multiple times, the best outcome,
i.e. the one that generates the smallest *total within cluster sum of
squares (SS)*, is selected. The total within SS is calculated as:

For each cluster results:

- for each observation, determine the squared euclidean distance from
  observation to centre of cluster
- sum all distances

Note that this is a **local minimum**; there is no guarantee to obtain
a global minimum.

> Challenge:
>
> Repeat k-means on our `x` data multiple times, setting the number of
> iterations to 1 or greater and check whether you repeatedly obtain
> the same results. Try the same with random data of identical
> dimensions.

<details>
```{r selrep, fig.width = 12, fig.cap = "Different k-means results on the same (random) data"}
cl1 <- kmeans(x, centers = 3, nstart = 10)
cl2 <- kmeans(x, centers = 3, nstart = 10)
table(cl1$cluster, cl2$cluster)

cl1 <- kmeans(x, centers = 3, nstart = 1)
cl2 <- kmeans(x, centers = 3, nstart = 1)
table(cl1$cluster, cl2$cluster)

set.seed(42)
xr <- matrix(rnorm(prod(dim(x))), ncol = ncol(x))
cl1 <- kmeans(xr, centers = 3, nstart = 1)
cl2 <- kmeans(xr, centers = 3, nstart = 1)
table(cl1$cluster, cl2$cluster)
diffres <- cl1$cluster != cl2$cluster
par(mfrow = c(1, 2))
plot(xr, col = cl1$cluster, pch = ifelse(diffres, 19, 1))
plot(xr, col = cl2$cluster, pch = ifelse(diffres, 19, 1))
```
</details>

### How to determine the number of clusters  {-}

1. Run k-means with `k=1`, `k=2`, ..., `k=n`
2. Record total within SS for each value of k.
3. Choose k at the *elbow* position, as illustrated below.

```{r kmelbow, echo=FALSE, fig.cap = ""}
ks <- 1:5
tot_within_ss <- sapply(ks, function(k) {
	cl <- kmeans(x, k, nstart = 10)
	cl$tot.withinss
})
plot(ks, tot_within_ss, type = "b",
	 ylab = "Total within squared distances",
	 xlab = "Values of k tested")
```

> Challenge
>
> Calculate the total within sum of squares for k from 1 to 5 for our
> `x` test data, and reproduce the figure above.

<details>
```{r solkmelbow, eval = FALSE}
ks <- 1:5
tot_within_ss <- sapply(ks, function(k) {
	cl <- kmeans(x, k, nstart = 10)
	cl$tot.withinss
})
plot(ks, tot_within_ss, type = "b")
```
</details>

There exists other metrics, other than the total within cluster sum of
squares that can be applied, such as the gap statistic (see
`cluster::clusGap`), or the Akaike (AIC) and Bayesian (BIC)
information criteria.

### Challenge {-}

Let's use what we have learned to cluster the `r nrow(mulvey2015norm)`
proteins from the `mulvey2015norm` data in 20 clusters.

1. Use k-means to cluster the `mulvey2015norm` data, setting `centers
   = 20`. Take care in repeating the clustering more than once.

2. To plot the expression profiles for the 20 clusters, I suggest to
   use `gplot2`. Do do so, create a `r nrow(mulvey2015norm)` proteins
   by `r ncol(mulvey2015norm)` sample dataframe (or tibble), appending
   the protein accession numbers (from the feature data - you can use
   the `MSnbase::ms2df` helper function.) and cluster numbers as 2
   additional columns.

3. Use `gather` to transform the data in a long format.

4. Use `ggplot2` to reproduce the figure below. Optional: use
   `stat_summary` to add a mean profile for each cluster of proteins.

![kmeans clustering on `mulvey2015norm`](./img/kmeans-mulvey-2.png)

<details>
```{r mulveykmeans, eval=FALSE}
library("pRolocdata")
data(mulvey2015norm)

cl <- kmeans(MSnbase::exprs(mulvey2015norm),
			 centers = 16, nstart = 10, iter.max = 50)

x <- ms2df(mulvey2015norm, fcol = "Accession")
x[["cluster"]] <- cl$cluster
tb <- gather(x, key = sample, value = expression, -cluster, -Accession) %>%
	as_tibble

## Check dimensions
stopifnot(nrow(tb) == prod(dim(mulvey2015norm)))

pd <- pData(mulvey2015norm)
tb$time <- pd[tb[["sample"]], "times"]
tb$rep <- pd[tb[["sample"]], "rep"]

## Plotting
kmp <- ggplot(data = tb,
			  aes(x = paste(time, rep), y = expression,
				  group = Accession, colour = as.factor(cluster))) +
	geom_line() +
	facet_wrap(~ cluster) +
	theme(legend.position = "none") +
	scale_x_discrete("Time course")

kmp2 <- kmp +
	stat_summary(aes(group = cluster),
				 fun.y = mean, geom = "line",
				 colour = "black")
```
</details>

## Hierarchical clustering

### How does hierarchical clustering work {-}

**Initialisation**:  Starts by assigning each of the n points its own cluster

**Iteration**

1. Find the two nearest clusters, and join them together, leading to
   n-1 clusters
2. Continue the cluster merging process until all are grouped into a
   single cluster

**Termination:** All observations are grouped within a single cluster.

```{r hcldata, fig.width = 12, echo=FALSE, fig.cap = "Hierarchical clustering: initialisation (left) and colour-coded results after iteration (right)."}
set.seed(42)
xr <- data.frame(x = rnorm(5),
				 y = rnorm(5))
cls <- c("red", "blue", "orange", "blue", "orange")
cls <- scales::col2hcl(cls, alpha = 0.5)
par(mfrow = c(1, 2))
plot(xr, cex = 3)
text(xr$x, xr$y, 1:5)
plot(xr, cex = 3, col = cls, pch = 19)
text(xr$x, xr$y, 1:5)
```

The results of hierarchical clustering are typically visualised along
a **dendrogram**, where the distance between the clusters is
proportional to the branch lengths.


```{r hcldendro, echo=FALSE, fig.cap = "Visualisation of the hierarchical clustering results on a dendrogram"}
plot(hcr <- hclust(dist(xr)))
```

In R:

- Calculate the distance using `dist`, typically the Euclidean
  distance.
- Hierarchical clustering on this distance matrix using `hclust`

> Challenge
>
> Apply hierarchical clustering on the `iris` data and generate a
> dendrogram using the dedicated `plot` method.

<details>
```{r hclsol, fig.cap = ""}
d <- dist(iris[, 1:4])
hcl <- hclust(d)
hcl
plot(hcl)
```
</details>

### Defining clusters {-}

After producing the hierarchical clustering result, we need to *cut
the tree (dendrogram)* at a specific height to defined the
clusters. For example, on our test dataset above, we could decide to
cut it at a distance around 1.5, with would produce 2 clusters.

```{r cuthcl, echo=FALSE, fig.cap = "Cutting the dendrogram at height 1.5."}
plot(hcr)
abline(h = 1.5, col = "red")
```

In R we can us the `cutree` function to

- cut the tree at a specific height: `cutree(hcl, h = 1.5)`
- cut the tree to get a certain number of clusters: `cutree(hcl, k = 2)`

> Challenge
>
> - Cut the iris hierarchical clustering result at a height to obtain
>   3 clusters by setting `h`.
> - Cut the iris hierarchical clustering result at a height to obtain
>   3 clusters by setting directly `k`, and verify that both provide
>   the same results.


<details>
```{r cuthclsol}
plot(hcl)
abline(h = 3.9, col = "red")
cutree(hcl, k = 3)
cutree(hcl, h = 3.9)
identical(cutree(hcl, k = 3), cutree(hcl, h = 3.9))
```
</details>

### Challenge {-}

Using the same `mulvey2015norm` dataset, generate a hierarchical
cluster of samples (the dendrogram will have 18 leafs), as shown
below. The cut it to obtain early, late and fully developed groups.

```{r mulveyhcl, echo=FALSE}
d <- dist(t(MSnbase::exprs(mulvey2015norm)))
hcl <- hclust(d)
plot(hcl, main = "Mulvey et al. 2016")
abline(h = 14, col = "red")
cutree(hcl, k = 3)
```

## Pre-processing

Many of the machine learning methods that are regularly used are
sensitive to difference scales. This applies to unsupervised methods
as well as supervised methods, as we will see in the next chapter.

A typical way to pre-process the data prior to learning is to scale
the data, or apply principal component analysis (next section). Scaling
assures that all data columns have a mean of 0 and standard deviation of 1.

In R, scaling is done with the `scale` function.

<details>

**Example**: Using the `mtcars` data as an example, verify that the
variables are of different scales, then scale the data. To observe the
effect different scales, compare the hierarchical clusters obtained on
the original and scaled data.

```{r scalesol, fig.width=12, fig.cap=""}
colMeans(mtcars)
hcl1 <- hclust(dist(mtcars))
hcl2 <- hclust(dist(scale(mtcars)))
par(mfrow = c(1, 2))
plot(hcl1, main = "original data")
plot(hcl2, main = "scaled data")
```
</details>

## Principal component analysis (PCA)

**Dimensionality reduction** techniques are widely used and versatile
techniques that can be used to:

- find structure in features
- pre-processing for other ML algorithms, and
- aid in visualisation.

The basic principle of dimensionality reduction techniques is to
transform the data into a new space that summarise properties of the
whole data set along a reduced number of dimensions. These are then
ideal candidates used to visualise the data along these reduced number
of informative dimensions.

### How does it work {-}

Principal Component Analysis (PCA) is a technique that transforms the
original n-dimensional data into a new n-dimensional space.

- These new dimensions are linear combinations of the original data,
  i.e.  they are composed of proportions of the original variables.
- Along these new dimensions, called principal components, the data
  expresses most of its variability along the first PC, then second,
  ...
- Principal components are orthogonal to each other,
  i.e. non-correlated.


```{r pcaex, echo=FALSE, fig.width=12, fig.height=4, fig.cap="Original data (left). PC1 will maximise the variability while minimising the residuals (centre). PC2 is orthogonal to PC1 (right)."}
set.seed(1)
xy <- data.frame(x = (x <- rnorm(50, 2, 1)),
				 y = x + rnorm(50, 1, 0.5))
pca <- prcomp(xy)

z <- cbind(x = c(-1, 1), y = c(0, 0))
zhat <- z %*% t(pca$rotation[, 1:2])
zhat <- scale(zhat, center = colMeans(xy), scale = FALSE)
par(mfrow = c(1, 3))
plot(xy, main = "Orignal data (2 dimensions)")
plot(xy, main = "Orignal data with PC1")
abline(lm(y ~ x, data = data.frame(zhat - 10)), lty = "dashed")
grid()
plot(pca$x, main = "Data in PCA space")
grid()
```

In R, we can use the `prcomp` function.

Let's explore PCA on the `iris` data. While it contains only 4
variables, is already becomes difficult to visualise the 3 groups
along all these dimensions.

```{r irispairs, fig.cap=""}
pairs(iris[, -5], col = iris[, 5], pch = 19)
```

Let's use PCA to reduce the dimension.

```{r irispca}
irispca <- prcomp(iris[, -5])
summary(irispca)
```

A summary of the `prcomp` output shows that along PC1 along, we are
able to retain over 92% of the total variability in the data.

```{r histpc1, echo=FALSE, fig.cap="Iris data along PC1."}
## boxplot(irispca$x[, 1] ~ iris[, 5], ylab = "PC1")
hist(irispca$x[iris$Species == "setosa", 1],
	 xlim = range(irispca$x[, 1]), col = "#FF000030",
	 xlab = "PC1", main = "PC1 variance explained 92%")
rug(irispca$x[iris$Species == "setosa", 1], col = "red")
hist(irispca$x[iris$Species == "versicolor", 1], add = TRUE, col = "#00FF0030")
rug(irispca$x[iris$Species == "versicolor", 1], col = "green")
hist(irispca$x[iris$Species == "virginica", 1],  add = TRUE, col = "#0000FF30")
rug(irispca$x[iris$Species == "virginica", 1], col = "blue")
```

### Visualisation {-}

A **biplot** features all original points re-mapped (rotated) along the
first two PCs as well as the original features as vectors along the
same PCs. Feature vectors that are in the same direction in PC space
are also correlated in the original data space.

```{r irisbiplot, fig.cap=""}
biplot(irispca)
```

One important piece of information when using PCA is the proportion of
variance explained along the PCs, in particular when dealing with high
dimensional data, as PC1 and PC2 (that are generally used for
visualisation), might only account for an insufficient proportion of
variance to be relevant on their own.

In the code chunk below, I extract the standard deviations from the
PCA result to calculate the variances, then obtain the percentage of
and cumulative variance along the PCs.

```{r irispcavar}
var <- irispca$sdev^2
(pve <- var/sum(var))
cumsum(pve)
```

> Challenge
>
> - Repeat the PCA analysis on the iris dataset above, reproducing the
>   biplot and preparing a barplot of the percentage of variance
>   explained by each PC.
> - It is often useful to produce custom figures using the data
>   coordinates in PCA space, which can be accessed as `x` in the
>   `prcomp` object. Reproduce the PCA plots below, along PC1 and PC2
>   and PC3 and PC4 respectively.

```{r irispcax, echo=FALSE, fig.width=12, fig.cap=""}
par(mfrow = c(1, 2))
plot(irispca$x[, 1:2], col = iris$Species)
plot(irispca$x[, 3:4], col = iris$Species)
```

<details>
```{r irispcaxcol, eval=FALSE}
par(mfrow = c(1, 2))
plot(irispca$x[, 1:2], col = iris$Species)
plot(irispca$x[, 3:4], col = iris$Species)
```
</details>

### Data pre-processing {-}

We haven't looked at other `prcomp` parameters, other that the first
one, `x`. There are two other ones that are or importance, in
particular in the light of the section on pre-processing above, which
are `center` and `scale.`. The former is set to `TRUE` by default,
while the second one is set the `FALSE`.

<details>

**Example** Repeat the analysis comparing the need for scaling on the
`mtcars` dataset, but using PCA instead of hierarchical
clustering. When comparing the two.

```{r scalepcasol, fig.with=12, fig.cap=""}
par(mfrow = c(1, 2))
biplot(prcomp(mtcars, scale = FALSE), main = "No scaling")  ## 1
biplot(prcomp(mtcars, scale = TRUE), main = "With scaling") ## 2
```

Without scaling, `disp` and `hp` are the features with the highest
loadings along PC1 and 2 (all others are negligible), which are also
those with the highest units of measurement. Scaling removes this
effect.

</details>

### Final comments on PCA {-}

Real datasets often come with **missing values**. In R, these should
be encoded using `NA`. Unfortunately, standard PCA cannot deal with
missing values, and observations containing `NA` values will be
dropped automatically. This is a viable solution only when the
proportion of missing values is low. Alternatively, the *NIPALS*
(non-linear iterative partial least squares) implementation does
support missing values (see `nipals::nipals`).

Finally, we should be careful when using categorical data in any of
the unsupervised methods described above. Categories are generally
represented as factors, which are encoded as integer levels, and might
give the impression that a distance between levels is a relevant
measure (which it is not, unless the factors are ordered). In such
situations, categorical data can be dropped, or it is possible to
encode categories as binary **dummy variables**. For example, if we
have 3 categories, say `A`, `B` and `C`, we would create two dummy
variables to encode the categories as:

```{r dummvar, echo=FALSE}
dfr <- data.frame(x = c(1, 0, 0),
				  y = c(0, 1, 0))
rownames(dfr) <- LETTERS[1:3]
dfr
```

so that the distance between each category are approximately equal to 1.

### Challenge {-}

Produce the PCA plot for the `ALL_bcrneg` samples, and annotating the
`NEG` and `BCR/ABL` samples on the plot. Do you think that the two
first components offer enough resolution?

<details>

```{r pcachallenge0}
pca <- prcomp(t(MSnbase::exprs(ALL_bcrneg)), scale = TRUE, center = TRUE)
plot(pca$x[, 1:2], col = ALL_bcrneg$mol.bio, cex = 2)
legend("bottomright", legend = unique(ALL_bcrneg$mol.bio),
	   pch = 1, col = c("black", "red"), bty = "n")
text(pca$x[, 1], pca$x[, 2], sampleNames(ALL_bcrneg), cex = 0.8)

par(mfrow = c(1, 2))
barplot(pca$sdev^2/sum(pca$sdev^2),
		xlab="Principle component",
		ylab="% of variance")

barplot(cumsum(pca$sdev^2/sum(pca$sdev^2) ),
		xlab="Principle component",
		ylab="Pumulative % of variance")

## Conclusion: the two first principle components are insufficient
```
</details>

## t-Distributed Stochastic Neighbour Embedding

[t-Distributed Stochastic Neighbour Embedding](https://lvdmaaten.github.io/tsne/) (t-SNE)
is a *non-linear* dimensionality reduction technique, i.e. that
different regions of the data space will be subjected to different
transformations. t-SNE will compress small distances, thus bringing
close neighbours together, and will ignore large distances. It is
particularly well suited
for
[very high dimensional data](https://distill.pub/2016/misread-tsne/).

In R, we can use the `Rtsne` function from the `r CRANpkg("Rtsne")`.
Before, we however need to remove any duplicated entries in the
dataset.

```{r iristsne, fig.cap=""}
library("Rtsne")
uiris <- unique(iris[, 1:5])
iristsne <- Rtsne(uiris[, 1:4])
plot(iristsne$Y, col = uiris$Species)
```

As with PCA, the data can be scaled and centred prior the running
t-SNE (see the `pca_center` and `pca_scale` arguments). The algorithm
is stochastic, and will produce different results at each repetition.

### Parameter tuning

t-SNE has two important parameters that can substantially influence
the clustering of the data

- **Perplexity**: balances global and local aspects of the data.
- **Iterations**: number of iterations before the clustering is
  stopped.

It is important to adapt these for different data. The figure below
shows a 5032 by 20 dataset that represent protein sub-cellular
localisation.

![Effect of different perplexity and iterations when running t-SNE](https://raw.githubusercontent.com/lgatto/visualisation/master/figure/tsnesplots-1.png)

As a comparison, below are the same data with PCA (left) and t-SNE
(right).

![PCA and t-SNE on [hyperLOPIT](https://www.ncbi.nlm.nih.gov/pubmed/28471460)(https://raw.githubusercontent.com/lgatto/visualisation/master/figure/tsneex-1.png)

## PCA loadings vs differential expression

Let's now compare the PCA loadings as calculated above and the
p-values. As we can see below, large loadings may or may not
correspond to small p-values.

```{r pcaloadings}
pca <- prcomp(t(MSnbase::exprs(ALL_bcrneg)), scale = TRUE, center = TRUE)
plot(pca$rotation[, 1], fData(ALL_bcrneg)$adj.P.Val,
	 xlab = "PCA Loading", ylab = "Adjusted p-values")
```

Let's now repeat the same camparison, focusing on differentially
expressed genes.

```{r allsgn}
table(sign <- fData(ALL_bcrneg)$adj.P.Val < 0.05)
ALL_sign <- ALL_bcrneg[sign, ]
pca_sign <- prcomp(t(exprs(ALL_sign)), center = TRUE, scale = TRUE)
```

Below, we see that there is better separation when we focus on
differentially expressed genes and better consistency between p-values
and loadings. However we can't do this in practice!

```{r pcasign}
plot(pca_sign$x[,1:2], col = ALL_sign$mol.biol, pch = 19)
legend("bottomleft", legend = unique(ALL_sign$mol.biol),
	   col = 1:2, pch = 19, bty = "n")
plot(pca_sign$rotation[, 1], fData(ALL_sign)$adj.P.Val,
	 xlab = "PCA Loading", ylab = "Adjusted p-values")
```

## Heatmaps

See

> Key M. *A tutorial in displaying mass spectrometry-based proteomic
> data using heat maps*. BMC Bioinformatics. 2012;13 Suppl
> 16:S10. doi:
> [10.1186/1471-2105-13-S16-S10](https://doi.org/10.1186/1471-2105-13-S16-S10). Epub
> 2012 Nov 5. Review. PMID: 23176119; PMCID: PMC3489527.

<!--chapter:end:40-tools.Rmd-->

# Using R and Bioconductor for MS-based proteomics

To be based on second part of [`RforProteomics` visualisation
vignette](https://lgatto.github.io/RforProteomics/articles/RProtVis.html)
and the [`MSnbase`
vignettes](https://lgatto.github.io/MSnbase/articles/index.html).

<!--chapter:end:50-rbiocms.Rmd-->

# Interactive visualisation


Based on this
[material](https://lgatto.github.io/2017_11_09_Rcourse_Jena/interactive-visualisation.html)
and the `RforProteomics::shinyMA()` app.

<!--chapter:end:60-interactive.Rmd-->

# Session information

The session information below documents the packages and versions used
to generate this material.

```{r si}
sessionInfo()
```

<!--chapter:end:99-si.Rmd-->

